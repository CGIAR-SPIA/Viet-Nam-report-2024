
R version 4.3.1 (2023-06-16 ucrt) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Workspace loaded from ~/.RData]

> library(readstata13)
Warning message:
package ‘readstata13’ was built under R version 4.3.2 
> library(readxl)
Warning message:
package ‘readxl’ was built under R version 4.3.2 
> library(foreign)
> library(ggplot2)
Warning message:
package ‘ggplot2’ was built under R version 4.3.2 
> library(forcats)
Warning message:
package ‘forcats’ was built under R version 4.3.2 
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

Warning message:
package ‘dplyr’ was built under R version 4.3.2 
> library(haven)
Warning message:
package ‘haven’ was built under R version 4.3.2 
> library(tidyr)
Warning message:
package ‘tidyr’ was built under R version 4.3.2 
> library(rlang)
Warning message:
package ‘rlang’ was built under R version 4.3.2 
> library(maditr)

Use magrittr pipe '%>%' to chain several operations:
             mtcars %>%
                 let(mpg_hp = mpg/hp) %>%
                 take(mean(mpg_hp), by = am)
        


Attaching package: ‘maditr’

The following object is masked from ‘package:rlang’:

    :=

The following objects are masked from ‘package:dplyr’:

    between, coalesce, first, last

Warning message:
package ‘maditr’ was built under R version 4.3.3 
> library(stringr)
Warning message:
package ‘stringr’ was built under R version 4.3.2 
> library(networkD3)
Warning message:
package ‘networkD3’ was built under R version 4.3.3 
> library(openxlsx)
Warning message:
package ‘openxlsx’ was built under R version 4.3.2 
> library(scales)
Warning message:
package ‘scales’ was built under R version 4.3.2 
> library(jtools)
Warning message:
package ‘jtools’ was built under R version 4.3.3 
> library(officer)

Attaching package: ‘officer’

The following object is masked from ‘package:readxl’:

    read_xlsx

Warning message:
package ‘officer’ was built under R version 4.3.2 
> library(httr)
Warning message:
package ‘httr’ was built under R version 4.3.2 
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> # Rice DNA fingerprinting  ----
> curl_function("data/processed/Rice.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/Rice.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 119588 bytes (116 KB)
downloaded 116 KB

> dat <- read.csv ("data/processed/Rice.vars.VH24.csv") 
> 
> curl_function("data/processed/Rice_Years.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/Rice_Years.csv'
Content type 'text/plain; charset=utf-8' length 5352 bytes
downloaded 5352 bytes

> Years <- read.csv ('data/processed/Rice_Years.csv') 
> dat <- merge (dat, Years, by.x='Correct_name.DNA2', by.y = 'Name', all.x=TRUE) 
> # Figure 13. Evolution of rice production and exports in Viet Nam, from 2003 to 2023, in thousand tonnes ----
> data <- data.frame(
+   Year = c(2003:2023),
+   Production = c(34569, 36149, 35833, 35850, 35943, 38730, 38950, 40006, 42399, 43738, 44039, 44975, 45091, 43109, 42739, 44046, 43495, 42765, 43853, 42661, 43498),
+   Exports = c(3810, 4063, 5255, 4642, 4580, 4745, 5969, 6893, 7116, 8017, 6587, 6331, 6582, 6582, 5819, 6107, 6371, 6249, 6242, 7105, 8132)
+ )
> 
> data_melted <- melt(data, id.vars="Year", variable.name="Category", value.name="Value")
> 
> ggplot(data=data_melted, aes(x=Year, y=Value, fill=Category)) +
+   geom_area(position = "identity", alpha=0.7) +
+   scale_fill_manual(values=c("Production"="#6699CC", "Exports"="#336699")) +
+   scale_x_continuous(limits=c(2003, 2023), breaks=seq(2003, 2023, by=2)) +
+   scale_y_continuous(labels = comma) +  # Add 1,000 separator
+   labs(x="Year", y="Tonnes") +
+   theme_minimal() +
+   theme(legend.title=element_blank(),
+         plot.title=element_blank(),
+         panel.grid.major = element_blank())  
> # Table 15. Distribution of rice by variety planted during the 2022/23 agricultural season ----
> 
> freq <- table(dat$Correct_name.DNA2)
> sorted_freq <- sort(freq, decreasing = TRUE)
> sorted_dat <- data.frame(Correct_name = names(sorted_freq), Frequency = sorted_freq)
> 
> # Adding Age var to table
> merged_dat <- merge(sorted_dat, Years [,c (1,2,11,6)], by.x = "Correct_name", by.y= "Name", all.x = TRUE)
> colnames(merged_dat)[2] <- "Frequency"
> merged_dat <- merged_dat[order(-merged_dat$Frequency.Freq), ]
> 
> # Clean data
> merged_dat$pctn <- round ((merged_dat$Frequency.Freq / sum(merged_dat$Frequency.Freq)) * 100, 1)
> merged_dat$Year <- (2022 - merged_dat$Year)
> merged_dat <- merged_dat [, c(2,3,7,4,5,6)]
> names(merged_dat)[1] <- 'Variety'; names(merged_dat)[2] <- 'Number of rice samples'; names(merged_dat)[3] <- '% rice samples'; names(merged_dat)[4] <- 'Age (years)'; names(merged_dat)[5] <- 'Pedigree'; names(merged_dat)[6] <- 'Origin'
> 
> #Note: These are unweighted figures. Weighted adoption statistics are here
> #https://www.dropbox.com/scl/fi/t5gonv515g5i25qxu17h7/Rice_adoption_byvar.html?rlkey=mmxr6akxh0c203x72qfrig3vf&dl=0
> 
> write.xlsx (merged_dat, 'Output/Tab15.Rice.xlsx') 
> # Figure 15. Maps showing the percentage adoption of (a) improved varietal adoption, (b) IRRI-related varietal adoption and c) average age of improved varieties in Viet Nam ----
> # Generated in "Rice_Maps_temp.RmD"
> # Figure 17. Map showing the intensity of adoption of the saltol gene by province in Vietnam  ----
> # See Maps_all.R
> # Table 16. Results of the OLS model estimating the relationship between the presence of alleles associated with the Saltol gene on farmer’s plot and salinity risk levels in the MRD ----
> curl_function ("data/processed/VH22_CSMAPS_all.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_CSMAPS_all.csv'
Content type 'text/plain; charset=utf-8' length 58078 bytes (56 KB)
downloaded 56 KB

> VH22_GPS <- read.csv('data/processed/VH22_CSMAPS_all.csv')
> 
> table (VH22_GPS$Salinity_E)

  0   1   2   3   4 
129  16  15  25   2 
> 
> VH22_GPS$Sal_Medium <- ifelse (VH22_GPS$Salinity_E == 2, TRUE, FALSE)
> VH22_GPS$Sal_High <- ifelse (VH22_GPS$Salinity_E %in% c(3,4), TRUE, FALSE)
> VH22_GPS$Sal_Med_High <- ifelse (VH22_GPS$Salinity_E %in% c(2,3,4), TRUE, FALSE)
> 
> VH22_GPS <- VH22_GPS %>%
+   mutate(
+     Sub1 = if_else(qSub1 == "[+p]", 1,
+                    if_else(qSub1 == "?" | qSub1 == "[--]", 0, NA_real_))
+     ,
+     Saltol = case_when(
+       Saltol == "[+p]-Aro" ~ 1,
+       Saltol == "[+p]-Aus" ~ 1,
+       Saltol == "?" ~ NA,
+       Saltol == "[--]" ~ 0,
+       TRUE ~ NA_real_
+     )
+   )
> 
> # n=156 Obs, rest are NA on Saltol
> summary(lm(Saltol ~ Sal_Medium + Sal_High, data = VH22_GPS))

Call:
lm(formula = Saltol ~ Sal_Medium + Sal_High, data = VH22_GPS)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.6522 -0.4836  0.3478  0.5164  0.5164 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.48361    0.04536  10.661   <2e-16 ***
Sal_MediumTRUE  0.06185    0.15774   0.392    0.696    
Sal_HighTRUE    0.16857    0.11390   1.480    0.141    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5011 on 153 degrees of freedom
  (31 observations deleted due to missingness)
Multiple R-squared:  0.01443,	Adjusted R-squared:  0.001549 
F-statistic:  1.12 on 2 and 153 DF,  p-value: 0.3289

> summ(lm(Sub1 ~ Sal_Medium + Sal_High, data = VH22_GPS), robust = 'HC1', digits = 3) 
MODEL INFO:
Observations: 187
Dependent Variable: Sub1
Type: OLS linear regression 

MODEL FIT:
F(2,184) = 6.402, p = 0.002
R² = 0.065
Adj. R² = 0.055 

Standard errors: Robust, type = HC1
------------------------------------------------------
                         Est.    S.E.   t val.       p
-------------------- -------- ------- -------- -------
(Intercept)             0.007   0.007    0.995   0.321
Sal_MediumTRUE         -0.007   0.007   -0.995   0.321
Sal_HighTRUE            0.104   0.061    1.698   0.091
------------------------------------------------------
> summary(lm(Saltol ~ Sal_Med_High, data = VH22_GPS))

Call:
lm(formula = Saltol ~ Sal_Med_High, data = VH22_GPS)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.6177 -0.4836  0.3824  0.5164  0.5164 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.48361    0.04527  10.684   <2e-16 ***
Sal_Med_HighTRUE  0.13404    0.09696   1.382    0.169    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5 on 154 degrees of freedom
  (31 observations deleted due to missingness)
Multiple R-squared:  0.01226,	Adjusted R-squared:  0.005844 
F-statistic: 1.911 on 1 and 154 DF,  p-value: 0.1688

> 
> VH22_GPS$Sal_High <- ifelse (VH22_GPS$Salinity_E == 3, TRUE, FALSE)
> summ(lm(Saltol ~ Sal_Medium + Sal_High, data = VH22_GPS), robust = 'HC1', digits = 3) 
MODEL INFO:
Observations: 156 (31 missing obs. deleted)
Dependent Variable: Saltol
Type: OLS linear regression 

MODEL FIT:
F(2,153) = 1.552, p = 0.215
R² = 0.020
Adj. R² = 0.007 

Standard errors: Robust, type = HC1
-----------------------------------------------------
                        Est.    S.E.   t val.       p
-------------------- ------- ------- -------- -------
(Intercept)            0.480   0.045   10.546   0.000
Sal_MediumTRUE         0.066   0.158    0.416   0.678
Sal_HighTRUE           0.202   0.110    1.836   0.068
-----------------------------------------------------
> summary_table <- summ(lm(Saltol ~ Sal_Medium + Sal_High, data = VH22_GPS), robust = 'HC1', digits = 3)
> 
> 
> 
> summary_table <- summ(lm(Saltol ~ Sal_Med_High, data = VH22_GPS), robust = 'HC1', digits = 3)
> 
> export_summs(summary_table, robust = 'HC1', digits = 3, 
+              to.file = "docx", file.name = "Output/Tab16.OLS_Saltol.docx")
    ─────────────────────────────────────────────────────────────────────────────────────────────────
                                                                         Model 1                     
                                                    ─────────────────────────────────────────────────
      (Intercept)                                                                         0.484 ***  
                                                                                         (0.046)     
      Sal_Med_HighTRUE                                                                    0.134      
                                                                                         (0.095)     
                                                    ─────────────────────────────────────────────────
      N                                                                                 156          
      R2                                                                                  0.012      
    ─────────────────────────────────────────────────────────────────────────────────────────────────
      Standard errors are heteroskedasticity robust.  *** p < 0.001; ** p < 0.01; * p < 0.05.        

Column names: names, Model 1
> 
> table (VH22_GPS$Salinity_E, VH22_GPS$Saltol)
   
     0  1
  0 57 52
  1  6  7
  2  5  6
  3  7 15
  4  1  0
> table (VH22_GPS$Salinity_E, VH22_GPS$Sub1)
   
      0   1
  0 128   1
  1  16   0
  2  15   0
  3  22   3
  4   2   0
> table (VH22_GPS$Salinity_E, VH22_GPS$IRRI_Pare0)
   
    Imported from IRRI Genebank IRRI-related (P) IRRI-related (P2) IRRI-related line Not IRRI-related
  0                           2               47                29                10                0
  1                           0                4                 1                 0                2
  2                           0                4                 2                 0                0
  3                           0                9                 1                 2                0
  4                           0                0                 0                 1                0
   
    Parents pedigree unknown
  0                        1
  1                        0
  2                        0
  3                        0
  4                        0
> 
> # New version
> 
> # a) include 3 dummies — for low risk, medium risk and high risk, 
> 
> VH22_GPS$Sal_Low <- ifelse (VH22_GPS$Salinity_E == 1, TRUE, FALSE)
> VH22_GPS$Sal_Medium <- ifelse (VH22_GPS$Salinity_E == 2, TRUE, FALSE)
> VH22_GPS$Sal_High <- ifelse (VH22_GPS$Salinity_E == 3, TRUE, FALSE)
> 
> summary(lm(Saltol ~ Sal_Low + Sal_Medium + Sal_High, data = VH22_GPS))

Call:
lm(formula = Saltol ~ Sal_Low + Sal_Medium + Sal_High, data = VH22_GPS)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.6818 -0.4727  0.3182  0.5273  0.5273 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     0.47273    0.04777   9.897   <2e-16 ***
Sal_LowTRUE     0.06573    0.14693   0.447   0.6552    
Sal_MediumTRUE  0.07273    0.15842   0.459   0.6468    
Sal_HighTRUE    0.20909    0.11700   1.787   0.0759 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.501 on 152 degrees of freedom
  (31 observations deleted due to missingness)
Multiple R-squared:  0.02118,	Adjusted R-squared:  0.001859 
F-statistic: 1.096 on 3 and 152 DF,  p-value: 0.3526

> 
> summary_table <- summ(lm(Saltol ~ Sal_Medium + Sal_Low + Sal_Med_High, data = VH22_GPS), robust = 'HC1', digits = 3)
> export_summs(summary_table, robust = 'HC1', digits = 3, 
+              to.file = "docx", file.name = "Output/Tab16.OLS_Saltol.docx")
    ─────────────────────────────────────────────────────────────────────────────────────────────────
                                                                         Model 1                     
                                                    ─────────────────────────────────────────────────
      (Intercept)                                                                         0.477 ***  
                                                                                         (0.048)     
      Sal_MediumTRUE                                                                     -0.107      
                                                                                         (0.182)     
      Sal_LowTRUE                                                                         0.061      
                                                                                         (0.148)     
      Sal_Med_HighTRUE                                                                    0.175      
                                                                                         (0.112)     
                                                    ─────────────────────────────────────────────────
      N                                                                                 156          
      R2                                                                                  0.016      
    ─────────────────────────────────────────────────────────────────────────────────────────────────
      Standard errors are heteroskedasticity robust.  *** p < 0.001; ** p < 0.01; * p < 0.05.        

Column names: names, Model 1
> 
> # b) 2 dummies — "low risk" and combine "medium risk and high risk", and possibly 
> 
> VH22_GPS$Sal_Med_High <- ifelse (VH22_GPS$Salinity_E %in% c(2,3), TRUE, FALSE)
> 
> summary(lm(Saltol ~ Sal_Low + Sal_Med_High, data = VH22_GPS))

Call:
lm(formula = Saltol ~ Sal_Low + Sal_Med_High, data = VH22_GPS)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.6364 -0.4727  0.3636  0.5273  0.5273 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       0.47273    0.04770   9.911   <2e-16 ***
Sal_LowTRUE       0.06573    0.14671   0.448    0.655    
Sal_Med_HighTRUE  0.16364    0.09929   1.648    0.101    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5002 on 153 degrees of freedom
  (31 observations deleted due to missingness)
Multiple R-squared:  0.01768,	Adjusted R-squared:  0.004839 
F-statistic: 1.377 on 2 and 153 DF,  p-value: 0.2555

> 
> # c) other combinations. 
> 
> VH22_GPS$Sal_Low_Med <- ifelse (VH22_GPS$Salinity_E %in% c(2,1), TRUE, FALSE)
> summary(lm(Saltol ~ Sal_Low_Med + Sal_High, data = VH22_GPS))

Call:
lm(formula = Saltol ~ Sal_Low_Med + Sal_High, data = VH22_GPS)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.6818 -0.4727  0.3182  0.5273  0.5273 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)      0.47273    0.04761   9.929   <2e-16 ***
Sal_Low_MedTRUE  0.06894    0.11250   0.613    0.541    
Sal_HighTRUE     0.20909    0.11662   1.793    0.075 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4993 on 153 degrees of freedom
  (31 observations deleted due to missingness)
Multiple R-squared:  0.02117,	Adjusted R-squared:  0.008376 
F-statistic: 1.655 on 2 and 153 DF,  p-value: 0.1946

> # Table 13. Overview of cassava varietal adoption in Vietnam ----
> #Note: These are unweighted figures 
> 
> curl_function ("data/raw/VHLSS_2023_Household/Combined_modules/Cassava.dat.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Cassava.dat.csv'
Content type 'text/plain; charset=utf-8' length 163077 bytes (159 KB)
downloaded 159 KB

> Cassava.dat <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Cassava.dat.csv")
> 
> # Weights
> #duplicated_rows <- weight2023[duplicated(weight2023$xa), ] # 21 communes have 2 EAs
> # = At the EA level
> 
> # Import DNA data
> 
> curl_function ("data/raw/Genetics/Cassava/Cassava_assigns_758.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Genetics/Cassava/Cassava_assigns_758.csv'
Content type 'text/plain; charset=utf-8' length 198494 bytes (193 KB)
downloaded 193 KB

> Cass.DNA <- read.csv ("data/raw/Genetics/Cassava/Cassava_assigns_758.csv")
> Cass.DNA <- Cass.DNA [, c(1,5,7,8)]
> Cass.DNA$ID <- sub("_.*", "", Cass.DNA$Sample_chip)
> Cass.DNA <- Cass.DNA [, c(5,2,3,4)]; names (Cass.DNA)[2] <- 'Genotype'; names (Cass.DNA)[3] <- 'CMD'; names (Cass.DNA)[4] <- 'DMC'
> 
> table (Cass.DNA$Genotype)

                                                  1                        38                  ba trăng 
                        8                         1                         1                         2 
                 Ba Trăng                    cao 94                   Cao San                   cao sản 
                        5                         4                         1                        17 
                  Cao sản                   cao sẳn        cao sản thân trắng                địa phương 
                        1                         1                        11                         7 
          địa phương/HL23          giống địa phương      giống sắn địa phương                    HL-S11 
                        1                        23                         4                         4 
                      HN2                       k94               khoai mì kè                    km 140 
                        1                         2                         7                         1 
                    km 94                     km140                     KM140                     KM297 
                        1                         3                        52                         7 
                    KM419                     KM505                 KM57_VNM8             KM60/R60/TAI8 
                       14                         5                        66                         2 
                     km94                      KM94                 KM94/KU50                      km95 
                        2                         1                       170                         1 
                     km98                    KM98-1      KM98-5/BRA1305/ARG49              KM98.5/140.8 
                        1                         4                        68                         1 
           KM98/CR63/TAI9                    lá tre                mỳ cao sản                    Pirun2 
                       44                         1                         4                         1 
                  R1/TAI1                       sắn                   Sắn cao               Sắn cao sản 
                        1                         2                         1                         3 
         sắn cao sản xanh    sắn cao sản/sắn lá tre                 sắn chuối                 Sắn Chuối 
                        1                        27                         1                         2 
           sắn địa phương                    sắn đỏ         Sắn đỏ ( Sắn tre)         sắn đỏ địa phương 
                        6                         8                        13                         1 
             sắn đồng nai                  sắn KM94                sắn lá tre     sắn lá tre/địa phương 
                        2                         1                        21                        16 
                  sắn nếp  sắn nếp/giống địa phương               sắn thân đỏ                săn thương 
                       20                         7                         1                         2 
sắn thường để ăn củ và lá sắn thường/sắn đỏ phú thọ                 San trang                 sắn trắng 
                        1                         9                        27                         3 
        sắn trắng/cao sản                   Sắn tre                  sắn xanh         sắn xanh hòa bình 
                        9                         1                         6                         1 
                      SC9                  Tai xanh                  tăng sản                    tây mo 
                        3                         2                         2                         1 
                 tây ninh                   Unknown                      VNM1 
                        1                         5                         1 
> number_of_unique_values <- length(unique(Cass.DNA$Genotype)) # 75 
> 
> Cass.DNA <- Cass.DNA %>%
+   mutate(Genotype_rec = recode(Genotype,
+                                "1" = "Unknown",
+                                "Ba Trăng" = "Ba Trăng",
+                                "ba trăng" = "Ba Trăng",
+                                "cao 94" = "Other landraces",
+                                "Cao San" = "Cao San",
+                                "cao sản" = "Cao San",
+                                "Cao sản" = "Cao San",
+                                "cao sẳn" = "Cao San",
+                                "cao sản thân trắng" = "Cao San",
+                                "địa phương" = "Dịa phương",
+                                "địa phương/HL23" = "Dịa phương",
+                                "giống địa phương" = "Dịa phương",
+                                "giống sắn địa phương" = "Dịa phương",
+                                "HL-S11" = "",
+                                "khoai mì kè" = "Other landraces",
+                                "KM140" = "",
+                                "km 140" = "KM140",
+                                "km140" = "KM140",
+                                "KM297" = "",
+                                "KM419" = "",
+                                "KM505" = "KM7", # Similar variety with different names (Cong et al. 2016)
+                                "KM57_VNM8" = "KM57",
+                                "KM60/R60/TAI8" = "KM60",
+                                "km94" = "KM94",
+                                "KM94" = "KM94",
+                                "k94" = "KM94",
+                                "k94 " = "KM94",
+                                "sắn KM94" = "KM94",
+                                "KM94/KU50" = "KM94",
+                                "km95" = "KM95",
+                                "km 94" = "KM94",
+                                "KM98.5/140.8" = "KM98-5",
+                                "KM98/CR63/TAI9" = "KM98-5", # From Adriana's report
+                                "km98" = "KM98-5",
+                                "KM98-1" = "",
+                                "KM98-5/BRA1305/ARG49" = "KM98-5",
+                                "Pirun2" = "",
+                                "R1/TAI1" = "",
+                                "sắn" = "Cao San",
+                                "Sắn cao" = "Cao San",
+                                "Sắn cao sản" = "Cao San",
+                                "sắn cao sản xanh" = "Cao San",
+                                "mỳ cao sản" = "Cao San",
+                                "sắn cao sản/sắn lá tre" = "Cao San",
+                                "sắn chuối" = "Other landraces",
+                                "Sắn chuối" = "Other landraces",
+                                "sắn địa phương" = "Dịa phương",
+                                "sắn đỏ" = "Other landraces",
+                                "Sắn đỏ ( Sắn tre)" = "Sắn tre",
+                                "sắn đỏ địa phương" = "Dịa phương",
+                                "sắn lá tre" = "Sắn tre",
+                                "sắn lá tre/địa phương" = "Dịa phương",
+                                "sắn nếp" = "Other landraces",
+                                "sắn nếp/giống địa phương" = "Dịa phương",
+                                "sắn thân đỏ" = "Other landraces",
+                                "sắn thường/sắn đỏ phú thọ" = "Other landraces",
+                                "sắn thường để ăn củ và lá" = "Other landraces",
+                                "săn thương" = "Other landraces",
+                                "San trang" = "Other landraces",
+                                "sắn trắng" = "Other landraces",
+                                "sắn trắng/cao sản" = "Cao San",
+                                "Sắn tre" = "Sắn tre",
+                                "sắn xanh" = "Other landraces",
+                                "SC9" = "SC9",
+                                "sắn đồng nai" = "Other landraces",
+                                "tăng sản" = "Other landraces",
+                                "tây ninh" = "Other landraces",
+                                "Tai xanh" = "Other landraces",
+                                "tây mo" = "Other landraces",
+                                "Sắn Chuối" = "Other landraces",
+                                "38" = "Other landraces",
+                                "lá tre" = "Other landraces",
+                                "sắn xanh hòa bình" = "Other landraces",
+                                "Unknown" = "Unknown",
+                                "VNM1" = "VNM1",
+                                .default = Genotype)) 
> 
> number_of_unique_values <- length(unique(Cass.DNA$Genotype_rec)) # 20 recoded as 'Other landraces'
> 
> Cass.DNA$Genotype_rec <- ifelse (Cass.DNA$Genotype_rec == "", Cass.DNA$Genotype, Cass.DNA$Genotype_rec)
> Cass.DNA$Genotype_rec <- ifelse (Cass.DNA$Genotype == "", "Unknown", Cass.DNA$Genotype_rec)
> 
> curl_function("data/raw/Genetics/Cassava/Cassava_Years.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Genetics/Cassava/Cassava_Years.csv'
Content type 'text/plain; charset=utf-8' length 1395 bytes
downloaded 1395 bytes

> Years <- read.csv ("data/raw/Genetics/Cassava/Cassava_Years.csv") 
> Cass.DNA <- merge (Cass.DNA, Years, by.x='Genotype_rec', by.y = 'Name', all.x=TRUE)  
> 
> genotype_df <- as.data.frame(table (Cass.DNA$Genotype_rec))
> colnames(genotype_df) <- c("Genotype", "Frequency")
> ordered_genotype_df <- genotype_df[order(-genotype_df$Frequency), ]
> genotype_df$pctn <- round ((genotype_df$Frequency / sum(genotype_df$Frequency)) * 100, 1)
> 
> # Adding Age var to table
> genotype_df <- merge(genotype_df, Years [,c (1,2,5)], by.x = "Genotype", by.y= "Name", all.x = TRUE) # Add INSTITUIONAL SOURCE LATER
> genotype_df$Year <- (2023 - genotype_df$Year)
> genotype_df$CIAT.related <- ifelse (is.na(genotype_df$CIAT.related), 0, genotype_df$CIAT.related)
> genotype_df <- genotype_df[order(-genotype_df$Frequency), ]
> names(genotype_df)[1] <- 'Cultivar'; names(genotype_df)[2] <- 'Number of samples'; names(genotype_df)[3] <- '% samples'; names(genotype_df)[4] <- 'Age (years)'; names(genotype_df)[5] <- 'CGIAR-related' #; names(genotype_df)[6] <- 'Origin'
> genotype_df$`CGIAR-related` <- ifelse (genotype_df$`CGIAR-related` == '0', 'No', genotype_df$`CGIAR-related`)
> 
> # 24. CMD-resistant cassava varieties
> # Marker CMD2, snpME00021 - homozygous favorable allele (TT) or heterozygous (TG)
> Cass.DNA$CMD <- ifelse (Cass.DNA$CMD %in% c('T:T','T:G'), TRUE, FALSE)
> 
> # 23. High-starch improved cassava varieties
> # Marker DM (Dry matter content), snpME00027 - homozygous favorable allele (CC). How about heterozygous (TC) samples? CHECK 
> Cass.DNA$DMC <- ifelse (Cass.DNA$DMC == 'C:C', TRUE, FALSE) # 491/611
> Cass.DNA$Year <- 2023 - Cass.DNA$Year
> 
> # QTL percent per cultivar
> summary_table <- Cass.DNA %>%
+   group_by(Genotype_rec) %>%
+   summarise(
+     DMC_QTL = round((sum(DMC == TRUE, na.rm = TRUE) / n()) * 100, 0),
+     CMD_QTL = round((sum(CMD == TRUE, na.rm = TRUE) / n()) * 100, 0)
+   )
> 
> # Add to overview table
> genotype_df <- merge (genotype_df, summary_table, by.x='Cultivar', by.y='Genotype_rec', all.x=TRUE)
> names(genotype_df)[6] <- 'Dry matter content QTL (in %)'; names(genotype_df)[7] <- 'CMD-resistant QTL (in %)'
> genotype_df <- genotype_df[order(-genotype_df$`Number of samples`), ]
> 
> write.xlsx (genotype_df, 'Output/Tab13.Cassava.xlsx') 
> rm (list = ls()) #start clean
> library (tidyverse)
── Attaching core tidyverse packages ────────────────────────────────────────────────── tidyverse 2.0.0 ──
✔ lubridate 1.9.3     ✔ readr     2.1.4
✔ purrr     1.0.2     ✔ tibble    3.2.1
── Conflicts ──────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
✖ purrr::%@%()         masks rlang::%@%()
✖ maditr:::=()         masks rlang:::=()
✖ maditr::between()    masks dplyr::between()
✖ maditr::coalesce()   masks dplyr::coalesce()
✖ readr::col_factor()  masks scales::col_factor()
✖ readr::cols()        masks maditr::cols()
✖ purrr::discard()     masks scales::discard()
✖ dplyr::filter()      masks stats::filter()
✖ maditr::first()      masks dplyr::first()
✖ purrr::flatten()     masks rlang::flatten()
✖ purrr::flatten_chr() masks rlang::flatten_chr()
✖ purrr::flatten_dbl() masks rlang::flatten_dbl()
✖ purrr::flatten_int() masks rlang::flatten_int()
✖ purrr::flatten_lgl() masks rlang::flatten_lgl()
✖ purrr::flatten_raw() masks rlang::flatten_raw()
✖ purrr::invoke()      masks rlang::invoke()
✖ dplyr::lag()         masks stats::lag()
✖ maditr::last()       masks dplyr::last()
✖ officer::read_xlsx() masks readxl::read_xlsx()
✖ purrr::splice()      masks rlang::splice()
✖ purrr::transpose()   masks maditr::transpose()
ℹ Use the conflicted package to force all conflicts to become errors
Warning messages:
1: package ‘tidyverse’ was built under R version 4.3.2 
2: package ‘tibble’ was built under R version 4.3.2 
3: package ‘readr’ was built under R version 4.3.2 
4: package ‘purrr’ was built under R version 4.3.2 
5: package ‘lubridate’ was built under R version 4.3.2 
> library (cowplot)

Attaching package: ‘cowplot’

The following object is masked from ‘package:lubridate’:

    stamp

Warning message:
package ‘cowplot’ was built under R version 4.3.3 
> library (gridExtra)

Attaching package: ‘gridExtra’

The following object is masked from ‘package:dplyr’:

    combine

Warning message:
package ‘gridExtra’ was built under R version 4.3.2 
> library (httr)
> library (readxl)
> library (e1071)
Warning message:
package ‘e1071’ was built under R version 4.3.2 
> library (estimatr)
Warning message:
package ‘estimatr’ was built under R version 4.3.3 
> library (jtools)
> library (haven)
> library (summarytools)

Attaching package: ‘summarytools’

The following object is masked from ‘package:tibble’:

    view

Warning message:
package ‘summarytools’ was built under R version 4.3.3 
> 
> # Function to curl data from GitHub----
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> # Figure 21. Number of Vietnamese provinces referencing the strengh of El-Niño in yearly agricultural plans, by region (2021-2024) ----
> 
> el_nino_data <- data.frame(
+   Year = c(2021, 2021, 2021, 2022, 2022, 2022, 2023, 2023, 2023, 2024, 2024, 2024),
+   Region = c("MRD", "NM-RRD", "SCC-CH", "MRD", "NM-RRD", "SCC-CH", "MRD", "NM-RRD", "SCC-CH", "MRD", "NM-RRD", "SCC-CH"),
+   Weak_El_Nino = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
+   Moderate_El_Nino = c(1, 0, 0, 1, 0, 1, 4, 1, 1, 2, 3, 1),
+   Strong_El_Nino = c(0, 0, 0, 1, 0, 0, 3, 0, 0, 3, 2, 0)
+ )
> 
> # Reshape data to long format
> el_nino_data_long <- el_nino_data %>%
+   pivot_longer(cols = c(Weak_El_Nino, Moderate_El_Nino, Strong_El_Nino),
+                names_to = "El_Nino_Type",
+                values_to = "Count")
> 
> # Rename and reorder the El Nino Type levels
> el_nino_data_long$El_Nino_Type <- factor(el_nino_data_long$El_Nino_Type, 
+                                          levels = c("Weak_El_Nino", "Moderate_El_Nino", "Strong_El_Nino"),
+                                          labels = c("Weak El Niño", "Moderate El Niño", "Strong El Niño"))
> 
> # Define a color palette for El Nino types
> el_nino_color_palette <- scale_fill_manual(values = c("Weak El Niño" = "#6baed6",    # Blue for Weak El Nino
+                                                       "Moderate El Niño" = "#fdae61", # Orange for Moderate El Nino
+                                                       "Strong El Niño" = "#f46d43"))  # Pale red for Strong El Nino
> 
> # Create plot for MRD
> plot_mrd <- ggplot(el_nino_data_long[el_nino_data_long$Region == "MRD", ], aes(x = as.factor(Year), y = Count, fill = El_Nino_Type)) +
+   geom_bar(stat = "identity", position = "dodge") +
+   el_nino_color_palette +
+   ylim(0, 5) +  # Set y-axis scale to be between 0 and 8
+   labs(title = "MRD provinces (n=11)") +
+   theme_minimal() +
+   theme(legend.position = "none", 
+         axis.title.x = element_blank(),
+         axis.title.y = element_blank(),
+         plot.title = element_text(hjust = 0.5))  # Center the title
> 
> # Create plot for RRD
> plot_rrd <- ggplot(el_nino_data_long[el_nino_data_long$Region == "NM-RRD", ], aes(x = as.factor(Year), y = Count, fill = El_Nino_Type)) +
+   geom_bar(stat = "identity", position = "dodge") +
+   el_nino_color_palette +
+   ylim(0, 5) +  # Set y-axis scale to be between 0 and 8
+   labs(title = "NM-RRD provinces (n=10)") +
+   theme_minimal() +
+   theme(legend.position = "none", 
+         axis.title.x = element_blank(),
+         axis.title.y = element_blank(),
+         plot.title = element_text(hjust = 0.5))  # Center the title
> 
> # Create plot for NCC-CH
> plot_nccch <- ggplot(el_nino_data_long[el_nino_data_long$Region == "SCC-CH", ], aes(x = as.factor(Year), y = Count, fill = El_Nino_Type)) +
+   geom_bar(stat = "identity", position = "dodge") +
+   el_nino_color_palette +
+   ylim(0, 5) +  # Set y-axis scale to be between 0 and 8
+   labs(title = "SCC-CH provinces (n=8)") +
+   theme_minimal() +
+   theme(legend.position = "none", 
+         axis.title.x = element_blank(),
+         axis.title.y = element_blank(),
+         plot.title = element_text(hjust = 0.5))  # Center the title
> 
> # Extract the legend and rename 'El_Nino_Type' to 'El Niño Category'
> legend_el_nino <- get_legend(
+   ggplot(el_nino_data_long, aes(x = as.factor(Year), y = Count, fill = El_Nino_Type)) +
+     geom_bar(stat = "identity", position = "dodge") +
+     el_nino_color_palette +
+     labs(fill = " ") +  # Rename legend title to "El Niño Category"
+     theme_minimal() +
+     theme(legend.position = "bottom")
+ )
> 
> # Arrange the three plots horizontally with the legend centered at the bottom
> grid.arrange(
+   arrangeGrob(plot_mrd, plot_rrd, plot_nccch, ncol = 3),
+   legend_el_nino,
+   nrow = 2,
+   heights = c(8, 1)  # Adjust the height ratio to give more space to the plots
+ )
> # Figure 22. Number of Vietnamese provinces referencing forecasted reservoir water levels in yearly agricultural plans, by region (2021-2024) ----
> 
> data <- data.frame(
+   Year = c(2021, 2022, 2023, 2024, 2021, 2022, 2023, 2024, 2021, 2022, 2023, 2024),
+   Region = c("RRD", "RRD", "RRD", "RRD", "SCC-CH", "SCC-CH", "SCC-CH", "SCC-CH", "NC", "NC", "NC", "NC"),
+   Normal = c(0, 0, 1, 1, 2, 3, 3, 1,2,1,0,0),
+   Insufficient = c(1, 5, 4, 7, 6, 4, 4, 0,2,1,1,0),
+   Extreme_Insufficient = c(0, 0, 1, 0, 1, 0, 0, 0,0,0,0,0),
+   Total = c(8, 9, 10, 3, 14, 10, 10, 1,1,1,1,1)
+ )
> 
> # Reshape data to long format
> data_long <- data %>%
+   pivot_longer(cols = c(Normal, Insufficient, Extreme_Insufficient),
+                names_to = "Status",
+                values_to = "CoInt")
> 
> # Rename and reorder the Status levels
> data_long$Status <- factor(data_long$Status, 
+                            levels = c("Normal", "Insufficient", "Extreme_Insufficient"),
+                            labels = c("Normal", "Insufficient", "Extremely Insufficient"))
> 
> # Define new color palette
> color_palette <- scale_fill_manual(values = c("Normal" = "#6baed6",    # Blue for Normal
+                                               "Insufficient" = "#fdae61", # Orange for Insufficient
+                                               "Extremely Insufficient" = "#f46d43")) # Pale red for Extremely Insufficient
> 
> # Create RRD plot without axis labels and without legend, y-axis scaled to 8
> plot_rrd <- ggplot(data_long[data_long$Region == "RRD", ], aes(x = as.factor(Year), y = CoInt, fill = Status)) +
+   geom_bar(stat = "identity", position = "dodge") +
+   color_palette +
+   ylim(0, 8) +  # Set y-axis scale to be between 0 and 8
+   labs(title = "RRD-NMMA provinces (n=10)") +
+   theme_minimal() +
+   theme(legend.position = "none", 
+         axis.title.x = element_blank(),
+         axis.title.y = element_blank(),
+         plot.title = element_text(hjust = 0.5))  # Center the title
> 
> # Create SCC-CH plot without axis labels and without legend, y-axis scaled to 8
> plot_sccch <- ggplot(data_long[data_long$Region == "SCC-CH", ], aes(x = as.factor(Year), y = CoInt, fill = Status)) +
+   geom_bar(stat = "identity", position = "dodge") +
+   color_palette +
+   ylim(0, 8) +  # Set y-axis scale to be between 0 and 8
+   labs(title = "SCC-CH provinces (n=8)") +
+   theme_minimal() +
+   theme(legend.position = "none", 
+         axis.title.x = element_blank(),
+         axis.title.y = element_blank(),
+         plot.title = element_text(hjust = 0.5))  # Center the title
> 
> plot_nc <- ggplot(data_long[data_long$Region == "NC", ], aes(x = as.factor(Year), y = CoInt, fill = Status)) +
+   geom_bar(stat = "identity", position = "dodge") +
+   color_palette +
+   ylim(0, 8) +  # Set y-axis scale to be between 0 and 8
+   labs(title = "NC provinces (n=6)") +
+   theme_minimal() +
+   theme(legend.position = "none", 
+         axis.title.x = element_blank(),
+         axis.title.y = element_blank(),
+         plot.title = element_text(hjust = 0.5))  # Center the title
> 
> 
> # Extract the legend and rename 'Status' to 'Risk level'
> legend <- get_legend(
+   ggplot(data_long, aes(x = as.factor(Year), y = CoInt, fill = Status)) +
+     geom_bar(stat = "identity", position = "dodge") +
+     color_palette +
+     labs(fill = "Water reservoirs capacity") +  # Rename legend title to "Risk level"
+     theme_minimal() +
+     theme(legend.position = "bottom")
+ )
> 
> # Arrange the plots horizontally with the legend centered at the bottom
> grid.arrange(
+   arrangeGrob(plot_rrd, plot_sccch, plot_nc, ncol = 3),
+   legend,
+   nrow = 2,
+   heights = c(8, 1)  # Adjust the height ratio to give more space to the plots
+ )
> # Saved 650 * 464
> # Figure 23. Heatmap showing the the occurrences of recommending no rice cultivation or shift to another crop in a given season from 2021 to 2024 in agricultural plannings ----
> 
> # Example data frame
> data <- data.frame(
+   Province = c("Dong Thap", "Bac Lieu", "Ben Tre", "Ca Mau", "Can Tho", "An Giang", "Hau Giang", "Kien Giang", "Long An", "Soc Trang", "Tra Vinh"),
+   WS_2021_2022 = c(0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0),
+   WS_2022_2023 = c(0, 1, 1, 1, 0, 0, 0, 0, 2, 1, 0),
+   WS_2023_2024 = c(0, 1, 0, 1, 0, 0, 0, 0, 2, 1, 0),
+   AW_2021 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
+   AW_2022 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
+   AW_2023 = c(0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0)
+ )
> 
> # Convert to long format and order provinces alphabetically
> data_long <- data %>%
+   pivot_longer(cols = -Province,
+                names_to = "Season",
+                values_to = "Count") %>%
+   mutate(Province = factor(Province, levels = rev(sort(unique(Province)))),
+          Season = factor(Season, levels = c("WS_2021_2022", "WS_2022_2023", "WS_2023_2024", "AW_2021", "AW_2022", "AW_2023"),
+                          labels = c("WS 2021-2022", "WS 2022-2023", "WS 2023-2024",
+                                     "AW 2021", "AW 2022", "AW 2023")))
> 
> # Plot heatmap with custom color scale and ordered provinces
> ggplot(data_long, aes(x = Season, y = Province, fill = Count)) +
+   geom_tile() +
+   scale_fill_gradient(name = "Count", low = "white", high = "deepskyblue3", breaks = c(0, 1, 2), labels = scales::label_comma(accuracy = 1)) +
+   labs(title = " ",
+        x = "Cropping Seasons",
+        y = "Provinces") +
+   theme_minimal() +
+   theme(axis.text.x = element_text(angle = 45, hjust = 1),
+         axis.text.y = element_text(size = 10),
+         legend.title = element_text(margin = margin(b = 10))) + # Adds space below the legend title
+   guides(fill = guide_colorbar(reverse = TRUE))
> # Table 18. Frequency and percentage of respondents who declared having used CS-MAPs in the last six cropping seasons by region ----
> curl_function ("data/processed/CS-MAPS%20Phase1_WIDE.xlsx")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/CS-MAPS%20Phase1_WIDE.xlsx'
Content type 'application/octet-stream' length 75247 bytes (73 KB)
downloaded 73 KB

> CS <- read_excel("data/processed/CS-MAPS%20Phase1_WIDE.xlsx")
> names(CS)
  [1] "SubmissionDate"           "start"                    "end"                     
  [4] "today"                    "deviceid"                 "Enumerator_ID"           
  [7] "Repondent_ID"             "region"                   "hhidprovince"            
 [10] "S1_Q3"                    "S1_Q1"                    "S1_Q2"                   
 [13] "S1_Q4"                    "S1_Q4b"                   "sowing"                  
 [16] "S1_Q5"                    "S1_Q6"                    "cal"                     
 [19] "S1_Q7"                    "S1_contact1"              "S1_contact2"             
 [22] "Date_release"             "S2a_Q1"                   "S2a_Q1o"                 
 [25] "S2a_Q4"                   "table_list_S2a_Q2"        "S2a_Q2_Drought"          
 [28] "S2a_Q2_Salt"              "S2a_Q2_Flood"             "table_list_S2a_Q3"       
 [31] "S2a_Q3_Drought"           "S2a_Q3_Salt"              "S2a_Q3_Flood"            
 [34] "S2a_Q5"                   "S2a_Q5a"                  "S2a_Q5b"                 
 [37] "Date_release2"            "S2a_Q7"                   "table_list_S2a_Q8"       
 [40] "S2a_Q8a"                  "S2a_Q8b"                  "S2a_Q8c"                 
 [43] "S2a_Q8d"                  "S2a_Q8e"                  "S2a_Q8f"                 
 [46] "S2a_Q8g"                  "S2a_Q9a"                  "S2a_Q9"                  
 [49] "table_list_S2a_Q10"       "S2a_Q10a"                 "S2a_Q10b"                
 [52] "S2a_Q10c"                 "S2a_Q10d"                 "S2a_Q10e"                
 [55] "S2a_Q10f"                 "S2a_Q10g"                 "S2a_Q10a2"               
 [58] "S2a_Q10b2"                "S2a_Q10c2"                "S2a_Q10d2"               
 [61] "S2a_Q10e2"                "S2a_Q10f2"                "S2a_Q10g2"               
 [64] "table_list_S2a_Q100"      "S2a_Q100b"                "S2a_Q100c"               
 [67] "S2a_Q100d"                "table_list_S2a_Q11"       "S2a_Q11b"                
 [70] "S2a_Q11c"                 "S2a_Q11d"                 "S2a_Q11e"                
 [73] "S2a_Q11f"                 "S2a_Q11g"                 "S2a_Q12"                 
 [76] "S2a_Q13"                  "S2b_Q0"                   "S2b_Q0o"                 
 [79] "S2b_Q3"                   "S2b_Q1"                   "S2a_Q1b"                 
 [82] "S2a_Q1c"                  "S2a_Q1d"                  "S2b_Q2"                  
 [85] "S2b_Q2b"                  "table_list_S2a_Q200"      "S2a_Q200b"               
 [88] "S2a_Q200c"                "S2a_Q200d"                "table_list_S2b_Q4"       
 [91] "S2b_Q4b"                  "S2b_Q4c"                  "S2b_Q4d"                 
 [94] "S2b_Q4e"                  "S2b_Q4f"                  "S2b_Q4g"                 
 [97] "table_list_S3_Q1"         "S3_Q1a"                   "S3_Q1b"                  
[100] "S3_Q1c"                   "S3_Q1d"                   "S3_Q1e"                  
[103] "S3_Q1f"                   "S3_Q2"                    "S3_Q2b"                  
[106] "S3_Q3"                    "S3_Q4"                    "S1_contact3"             
[109] "End_Comments_Interviewee" "End_Comments_Enum"        "__version__"             
[112] "instanceID"               "formdef_version"          "KEY"                     
> 
> CS <- CS %>%
+   mutate(Region_CSMaps = case_when(
+     hhidprovince %in% c("Thanh Hoa", "Nghe An", "Ha Tinh", "Quang Binh", "Quang Tri", "Thua Thien Hue") ~ "NC",
+     hhidprovince %in% c("Da Nang", "Quang Ngai", "Quang Nam", "Binh Dinh", "Khanh Hoa", "Phu Yen", "Binh Thuan", "Ninh Thuan", "Dak Lak", "Gia Lai") ~ "SCC-CH",
+     hhidprovince %in% c("Ninh Binh", "Nam Dinh", "Thai Binh", "Vinh Phuc", "Hai Duong", "Hung Yen", "Bac Ninh", "Ha Nam", "Ha Noi", "Hai Phong", "Bac Giang", "Phu Tho") ~ "NM-RRD",
+     hhidprovince %in% c("Can Tho", "An Giang", "Tien Giang", "Dong Thap", "Long An", "Vinh Long", "Ben Tre", "Tra Vinh", "Soc Trang", "Hau Giang", "Bac Lieu", "Ca Mau", "Kien Giang") ~ "MRD",
+     TRUE ~ NA_character_ # Optional: Assign NA for unmatched provinces
+   ))
> 
> table (CS$hhidprovince, CS$Region_CSMaps)
                
                 MRD NC NM-RRD SCC-CH
  An Giang         2  0      0      0
  Bac Giang        0  0      2      0
  Bac Lieu         2  0      0      0
  Bac Ninh         0  0      2      0
  Ben Tre          2  0      0      0
  Binh Dinh        0  0      0      1
  Binh Thuan       0  0      0      1
  Ca Mau           1  0      0      0
  Can Tho          2  0      0      0
  Da Nang          0  0      0      1
  Dak Lak          0  0      0      2
  Dong Thap        2  0      0      0
  Gia Lai          0  0      0      1
  Ha Nam           0  0      2      0
  Ha Noi           0  0      2      0
  Ha Tinh          0  2      0      0
  Hai Duong        0  0      2      0
  Hai Phong        0  0      2      0
  Hau Giang        3  0      0      0
  Hung Yen         0  0      2      0
  Khanh Hoa        0  0      0      2
  Kien Giang       2  0      0      0
  Long An          2  0      0      0
  Nam Dinh         0  0      2      0
  Nghe An          0  2      0      0
  Ninh Binh        0  0      1      0
  Ninh Thuan       0  0      0      2
  Phu Tho          0  0      2      0
  Phu Yen          0  0      0      1
  Quang Binh       0  2      0      0
  Quang Nam        0  0      0      1
  Quang Ngai       0  0      0      2
  Quang Tri        0  2      0      0
  Soc Trang        2  0      0      0
  Thai Binh        0  0      2      0
  Thanh Hoa        0  2      0      0
  Thua Thien Hue   0  2      0      0
  Tien Giang       2  0      0      0
  Tra Vinh         2  0      0      0
  Vinh Long        1  0      0      0
  Vinh Phuc        0  0      2      0
> 
> # Repondents involved in the design of agricultural plans and sowing schedules in [hhidprovince] ?
> table (CS$S1_Q5)

 no yes 
 34  40 
> 
> # 19/40 respondents declared using drought categoriy to assess the upcoming risk
> table (CS$S2a_Q2_Drought [CS$S1_Q5 == 'yes'])

 dk  na  no yes 
  1   5  15  19 
> 
> # 10./38 declared the maps were also handed over at lower administrative layers (districts), with proper guidance and instructions on how to use them.
> table (CS$S3_Q1b [CS$S1_Q5 == 'yes']) # Guidance and instructions

 dk  no yes 
  8  21  10 
> table (CS$region [CS$S1_Q5 == 'yes'], CS$S3_Q1b [CS$S1_Q5 == 'yes']) # Guidance and instructions
                   
                    dk no yes
  Central Highlands  0  0   1
  MRD                1 10   3
  NCCCA              2  7   4
  NMMA               0  1   1
  RRD                5  3   1
> 
> 
> 
> # When asked about the use of the CS-Maps in the next cropping season following the map design/release, 40% of respondents in the MRD, 22% in the NM-RRD and 31% in the SCC-CH responded positively. 
> table (CS$S2a_Q7[CS$S1_Q5 == 'yes'], CS$Region_CSMaps [CS$S1_Q5 == 'yes'])
     
      MRD NC NM-RRD SCC-CH
  no    4  3      6      3
  yes  10  2      5      6
> round (prop.table (table (CS$S2a_Q7[CS$S1_Q5 == 'yes'], CS$Region_CSMaps [CS$S1_Q5 == 'yes']), 2), 2)
     
       MRD   NC NM-RRD SCC-CH
  no  0.29 0.60   0.55   0.33
  yes 0.71 0.40   0.45   0.67
> 
> # In the last three years, which years were categorised as severe or moderate years in [hhidprovince]?
> table (CS$hhidprovince, CS$S2a_Q100b) # 2023
                
                 dk Moderate na Severe
  An Giang        0        1  0      0
  Bac Giang       0        1  0      0
  Bac Lieu        0        1  0      0
  Bac Ninh        0        1  0      0
  Ben Tre         0        1  0      0
  Binh Dinh       0        1  0      0
  Binh Thuan      0        1  0      0
  Ca Mau          0        1  0      0
  Can Tho         0        0  1      0
  Da Nang         0        1  0      0
  Dak Lak         1        0  0      0
  Dong Thap       0        1  0      0
  Gia Lai         0        0  0      0
  Ha Nam          0        0  0      0
  Ha Noi          0        1  0      0
  Ha Tinh         0        1  0      0
  Hai Duong       0        1  0      0
  Hai Phong       0        1  0      0
  Hau Giang       0        1  0      0
  Hung Yen        0        1  0      0
  Khanh Hoa       0        1  0      0
  Kien Giang      1        0  0      0
  Long An         0        1  0      0
  Nam Dinh        0        1  0      0
  Nghe An         0        0  0      1
  Ninh Binh       0        1  0      0
  Ninh Thuan      0        1  0      0
  Phu Tho         0        0  0      1
  Phu Yen         0        1  0      0
  Quang Binh      0        1  0      0
  Quang Nam       0        1  0      0
  Quang Ngai      0        1  0      0
  Quang Tri       0        1  0      0
  Soc Trang       0        1  0      0
  Thai Binh       0        1  0      0
  Thanh Hoa       0        1  0      0
  Thua Thien Hue  0        1  0      0
  Tien Giang      0        2  0      0
  Tra Vinh        0        1  0      0
  Vinh Long       0        1  0      0
  Vinh Phuc       0        1  0      0
> table (CS$hhidprovince, CS$S2a_Q100c) # 2022
                
                 dk Moderate na Severe
  An Giang        0        1  0      0
  Bac Giang       0        1  0      0
  Bac Lieu        0        1  0      0
  Bac Ninh        0        1  0      0
  Ben Tre         0        1  0      0
  Binh Dinh       0        1  0      0
  Binh Thuan      0        1  0      0
  Ca Mau          0        1  0      0
  Can Tho         0        0  1      0
  Da Nang         0        1  0      0
  Dak Lak         1        0  0      0
  Dong Thap       0        1  0      0
  Gia Lai         0        0  0      0
  Ha Nam          0        0  0      0
  Ha Noi          0        0  0      1
  Ha Tinh         0        1  0      0
  Hai Duong       0        0  0      1
  Hai Phong       0        1  0      0
  Hau Giang       0        1  0      0
  Hung Yen        0        1  0      0
  Khanh Hoa       0        1  0      0
  Kien Giang      0        0  0      1
  Long An         0        1  0      0
  Nam Dinh        0        1  0      0
  Nghe An         0        0  0      1
  Ninh Binh       0        0  0      1
  Ninh Thuan      0        1  0      0
  Phu Tho         0        1  0      0
  Phu Yen         0        1  0      0
  Quang Binh      0        1  0      0
  Quang Nam       0        1  0      0
  Quang Ngai      0        0  0      1
  Quang Tri       0        1  0      0
  Soc Trang       0        1  0      0
  Thai Binh       0        1  0      0
  Thanh Hoa       0        1  0      0
  Thua Thien Hue  0        0  0      1
  Tien Giang      0        2  0      0
  Tra Vinh        0        1  0      0
  Vinh Long       0        1  0      0
  Vinh Phuc       0        1  0      0
> table (CS$hhidprovince, CS$S2a_Q100d) # 2021
                
                 dk Moderate na Severe
  An Giang        0        1  0      0
  Bac Giang       0        1  0      0
  Bac Lieu        0        1  0      0
  Bac Ninh        0        1  0      0
  Ben Tre         0        1  0      0
  Binh Dinh       0        1  0      0
  Binh Thuan      0        1  0      0
  Ca Mau          0        1  0      0
  Can Tho         0        0  1      0
  Da Nang         0        1  0      0
  Dak Lak         1        0  0      0
  Dong Thap       0        1  0      0
  Gia Lai         0        0  0      0
  Ha Nam          0        0  0      0
  Ha Noi          0        0  0      1
  Ha Tinh         0        1  0      0
  Hai Duong       0        1  0      0
  Hai Phong       0        1  0      0
  Hau Giang       0        1  0      0
  Hung Yen        0        1  0      0
  Khanh Hoa       0        1  0      0
  Kien Giang      0        1  0      0
  Long An         0        1  0      0
  Nam Dinh        0        1  0      0
  Nghe An         0        0  0      1
  Ninh Binh       0        1  0      0
  Ninh Thuan      0        0  0      1
  Phu Tho         0        1  0      0
  Phu Yen         0        1  0      0
  Quang Binh      0        1  0      0
  Quang Nam       0        1  0      0
  Quang Ngai      0        0  0      1
  Quang Tri       0        0  0      1
  Soc Trang       0        1  0      0
  Thai Binh       0        1  0      0
  Thanh Hoa       0        1  0      0
  Thua Thien Hue  0        0  0      1
  Tien Giang      0        2  0      0
  Tra Vinh        0        1  0      0
  Vinh Long       0        1  0      0
  Vinh Phuc       0        1  0      0
> 
> # Frequency and percentage of respondents who declared having used the CS-Maps in the last three years, by region. 
> table (CS$S2a_Q11b [CS$S1_Q5 == 'yes'], CS$Region_CSMaps [CS$S1_Q5 == 'yes'])
     
      MRD NC NM-RRD SCC-CH
  dk    1  1      0      0
  na    2  2      1      4
  no    2  1      6      0
  yes   9  1      4      5
> table (CS$S2a_Q11c [CS$S1_Q5 == 'yes'], CS$Region_CSMaps [CS$S1_Q5 == 'yes'])
     
      MRD NC NM-RRD SCC-CH
  dk    1  1      0      0
  na    2  2      1      4
  no    3  2      6      1
  yes   8  0      4      4
> table (CS$S2a_Q11d [CS$S1_Q5 == 'yes'], CS$Region_CSMaps [CS$S1_Q5 == 'yes'])
     
      MRD NC NM-RRD SCC-CH
  dk    1  1      0      0
  na    2  2      1      3
  no    3  1      5      0
  yes   8  1      5      6
> table (CS$S2a_Q11e [CS$S1_Q5 == 'yes'], CS$Region_CSMaps [CS$S1_Q5 == 'yes'])
     
      MRD NC NM-RRD SCC-CH
  dk    1  1      0      0
  na    2  2      1      4
  no    1  1      6      0
  yes  10  1      4      5
> table (CS$S2a_Q11f [CS$S1_Q5 == 'yes'], CS$Region_CSMaps [CS$S1_Q5 == 'yes'])
     
      MRD NC NM-RRD SCC-CH
  dk    1  1      0      0
  na    2  2      1      4
  no    2  2      5      1
  yes   9  0      5      4
> table (CS$S2a_Q11g [CS$S1_Q5 == 'yes'], CS$Region_CSMaps [CS$S1_Q5 == 'yes'])
     
      MRD NC NM-RRD SCC-CH
  dk    1  1      0      0
  na    2  2      1      3
  no    2  1      6      0
  yes   9  1      4      6
> 
> # Assuming CS is your data frame and 'Yes' is coded as 1, 'No' as 0
> CS$aggreg <- rowSums(CS[, c("S2a_Q11b", "S2a_Q11c", "S2a_Q11d", "S2a_Q11e", "S2a_Q11f", "S2a_Q11g")] == 'yes', na.rm = TRUE)
> table (CS$aggreg, CS$Region_CSMaps)
   
    MRD NC NM-RRD SCC-CH
  0  15 11     18      8
  2   1  0      0      0
  3   1  0      1      0
  4   0  1      0      3
  5   0  0      1      0
  6   8  0      3      3
> by (CS$aggreg, CS$Region_CSMaps, sum)
CS$Region_CSMaps: MRD
[1] 53
------------------------------------------------------------------------------- 
CS$Region_CSMaps: NC
[1] 4
------------------------------------------------------------------------------- 
CS$Region_CSMaps: NM-RRD
[1] 26
------------------------------------------------------------------------------- 
CS$Region_CSMaps: SCC-CH
[1] 30
> 
> 
> # A total of 9 respondents could precisely detail the adaptation options recommended. 
> table (CS$S2a_Q10a[CS$S1_Q5 == 'yes'], CS$region [CS$S1_Q5 == 'yes'])
     
      Central Highlands MRD NCCCA NMMA RRD
  dk                  0   0     0    1   0
  no                  0   1     4    0   2
  yes                 0   8     1    0   0
> table (CS$S2a_Q10b[CS$S1_Q5 == 'yes'], CS$region [CS$S1_Q5 == 'yes'])
     
      Central Highlands MRD NCCCA NMMA RRD
  dk                  0   0     0    1   0
  no                  0   8     2    0   0
  yes                 0   1     3    0   2
> table (CS$S2a_Q10c[CS$S1_Q5 == 'yes'], CS$region [CS$S1_Q5 == 'yes'])
     
      Central Highlands MRD NCCCA NMMA RRD
  dk                  0   0     0    1   0
  no                  0   7     5    0   2
  yes                 0   2     0    0   0
> table (CS$S2a_Q10d[CS$S1_Q5 == 'yes'], CS$region [CS$S1_Q5 == 'yes'])
     
      Central Highlands MRD NCCCA NMMA RRD
  dk                  0   0     0    1   0
  no                  0   8     5    0   1
  yes                 0   1     0    0   1
> table (CS$S2a_Q10e[CS$S1_Q5 == 'yes'], CS$region [CS$S1_Q5 == 'yes'])
     
      Central Highlands MRD NCCCA NMMA RRD
  dk                  0   0     0    1   0
  no                  0   8     2    0   1
  yes                 0   1     3    0   1
> table (CS$S2a_Q10f[CS$S1_Q5 == 'yes'], CS$region [CS$S1_Q5 == 'yes'])
     
      Central Highlands MRD NCCCA NMMA RRD
  dk                  0   0     0    1   0
  no                  0   7     5    0   2
  yes                 0   2     0    0   0
> table (CS$S2a_Q10g[CS$S1_Q5 == 'yes'], CS$region [CS$S1_Q5 == 'yes'])
     
      Central Highlands MRD NCCCA NMMA RRD
  dk                  0   0     0    1   0
  no                  0   8     5    0   2
  yes                 0   1     0    0   0
> # Change in planting dates in the MRD provinces: merging georeferenced maps with VHLSS ----
> curl_function("data/raw/VHLSS_2022_Household/datasets/Ho_Muc4B11_edited.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Ho_Muc4B11_edited.csv'
Content type 'text/plain; charset=utf-8' length 1992008 bytes (1.9 MB)
downloaded 1.9 MB

> VH22 <- read.csv ("data/raw/VHLSS_2022_Household/datasets/Ho_Muc4B11_edited.csv") # 4B Dates
> curl_function("data/raw/VHLSS_2023_Household/Combined_modules/M4B11A5.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B11A5.csv'
Content type 'text/plain; charset=utf-8' length 957844 bytes (935 KB)
downloaded 935 KB

> VH23 <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/M4B11A5.csv")
> 
> VH22$P_date <- as.Date(paste(VH22$M4B11_C3AY, VH22$M4B11_C3AM, VH22$M4B11_C3AD, sep = "-"))
> 
> VH22$month <- as.numeric(format(VH22$P_date, "%m"))
> VH22$day <- as.numeric(format(VH22$P_date, "%d"))
> VH22$P_date_fortnight <- ifelse(VH22$day <= 14, VH22$month, VH22$month + 0.5) # Converting to fortnight format
> 
> # Recode the lunar dates..
> VH22 <- VH22[VH22$M4B11_MA == 2 & VH22$M4B11_C3B == 2, ]
> table (VH22$M4B11_MA) # 2 =WS season

  2 
349 
> table (VH22$M4B11_C3B)  # 2 = Gregorian calendar is 15% of Obs. 

  2 
349 
> 
> 
> # MRD household merging
> curl_function("data/processed/VH22_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_data.csv'
Content type 'text/plain; charset=utf-8' length 2257649 bytes (2.2 MB)
downloaded 2.2 MB

> df_22 <- read.csv("data/processed/VH22_data.csv")
> 
> curl_function("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> df_23 <- read.csv("data/processed/VH23_data.csv")
> 
> curl_function("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv'
Content type 'text/plain; charset=utf-8' length 1799 bytes
downloaded 1799 bytes

> Provinces_IDs <- read.csv("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv")
> 
> Provinces_IDs$MATINH <- as.numeric (Provinces_IDs$MATINH); df_22$MATINH <- as.numeric (df_22$MATINH); df_23$MATINH <- as.numeric (df_23$MATINH)
> df_22 <- merge (df_22, Provinces_IDs, all.x =TRUE)
> df_23 <- merge (df_23, Provinces_IDs, all.x =TRUE)
> 
> table (df_22$Region)

              1_RRD              2_NMMA             3_NCCCA 4_Central Highlands        5_South East 
               2737                5091                3753                 763                 108 
              6_MRD 
               1576 
> table (df_23$Region)

              1_RRD              2_NMMA             3_NCCCA 4_Central Highlands        5_South East 
               2524                4763                3665                1728                  93 
              6_MRD 
               1668 
> 
> 
> # CS-Maps merging - Generate clean dataset with District names for merging with CS-Maps georeferences
> curl_function ("data/processed/Adm_CSMAP_spatial_join_dataset.xlsx")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/Adm_CSMAP_spatial_join_dataset.xlsx'
Content type 'application/octet-stream' length 34289563 bytes (32.7 MB)
downloaded 32.7 MB

> Merge <- read_excel("data/processed/Adm_CSMAP_spatial_join_dataset.xlsx")
> 
> col_from<-c("Land_Use_c", "Flood_norm", "Flood_extr", "Salinity_N", "Salinity_E",
+             "LandUse_No", "LandUse_Ex", "LU_code_No", "LU_code_Ex", "Planting_W",
+             "Planting_S", "Planting_A", "Planting_1", "Planting_2", "Planting_3",
+             "Planting_4", "Planting_5")
> 
> col_to<-c("Land_Use_code",     "Flood_normal",      "Flood_extreme",    
+           "Salinity_Normal",   "Salinity_Extreme",  "LandUse_Normal",   
+           "LandUse_Extreme",   "LU_code_Normal",    "LU_code_Extreme",  
+           "Planting_WS_N",     "Planting_SA_N",     "Planting_AW_N",    
+           "Planting_Summer_N", "Planting_WS_E",     "Planting_SA_E",    
+           "Planting_AW_E",     "Planting_Summer_E")
> 
> Merge <- Merge %>% rename_at(vars(col_from), ~col_to)
Warning message:
Using an external vector in selections was deprecated in tidyselect 1.1.0.
ℹ Please use `all_of()` or `any_of()` instead.
  # Was:
  data %>% select(col_from)

  # Now:
  data %>% select(all_of(col_from))

See <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.
This warning is displayed once every 8 hours.
Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. 
> 
> Merge$ProDis <- paste (Merge$NAME_1, Merge$NAME_2, sep='-')
> 
> names(Merge)
 [1] "GID_3"             "GID_0"             "COUNTRY"           "GID_1"             "NAME_1"           
 [6] "NL_NAME_1"         "GID_2"             "NAME_2"            "NL_NAME_2"         "NAME_3"           
[11] "VARNAME_3"         "NL_NAME_3"         "TYPE_3"            "ENGTYPE_3"         "CC_3"             
[16] "HASC_3"            "OBJECTID"          "Area_ha"           "Land_Use_code"     "Province"         
[21] "Flood_normal"      "Flood_extreme"     "Salinity_Normal"   "Salinity_Extreme"  "LandUse_Normal"   
[26] "LandUse_Extreme"   "LU_code_Normal"    "LU_code_Extreme"   "Planting_WS_N"     "Planting_SA_N"    
[31] "Planting_AW_N"     "Planting_Summer_N" "Planting_WS_E"     "Planting_SA_E"     "Planting_AW_E"    
[36] "Planting_Summer_E" "T1_N"              "T2_N"              "T3_N"              "T4_N"             
[41] "T5_N"              "T6_N"              "T7_N"              "T8_N"              "T9_N"             
[46] "T10_N"             "T11_N"             "T12_N"             "T1_E"              "T2_E"             
[51] "T3_E"              "T4_E"              "T5_E"              "T6_E"              "T7_E"             
[56] "T8_E"              "T9_E"              "T10_E"             "T11_E"             "T12_E"            
[61] "ProDis"           
> 
> # Calculate the MODE and MEAN of planting dates at the district-level (n=140 districts)
> # Check the distribution and conclude on mean or mode
> 
> # Replace 0 values with NA for the specified columns
> Merge <- Merge %>%
+   mutate(
+     Planting_SA_E = ifelse(Planting_SA_E == 0, NA, Planting_SA_E),
+     Planting_SA_N = ifelse(Planting_SA_N == 0, NA, Planting_SA_N),
+     Planting_WS_E = ifelse(Planting_WS_E == 0, NA, Planting_WS_E),
+     Planting_WS_N = ifelse(Planting_WS_N == 0, NA, Planting_WS_N)
+   )
> 
> # Define a function to calculate the mode
> calculate_mode <- function(x) {
+   x <- x[!is.na(x)]  # Remove NA values
+   if (length(x) == 0) return(NA)  # Return NA if no values are present
+   uniq_vals <- unique(x)
+   uniq_vals[which.max(tabulate(match(x, uniq_vals)))]
+ }
> 
> # Calculate the mean and mode at the district level
> district_summary <- Merge %>%
+   group_by(ProDis) %>%
+   summarise(
+     avg_Planting_SA_E = mean(Planting_SA_E, na.rm = TRUE),
+     mode_Planting_SA_E = calculate_mode(Planting_SA_E),
+     
+     avg_Planting_SA_N = mean(Planting_SA_N, na.rm = TRUE),
+     mode_Planting_SA_N = calculate_mode(Planting_SA_N),
+     
+     avg_Planting_WS_E = mean(Planting_WS_E, na.rm = TRUE),
+     mode_Planting_WS_E = calculate_mode(Planting_WS_E),
+     
+     avg_Planting_WS_N = mean(Planting_WS_N, na.rm = TRUE),
+     mode_Planting_WS_N = calculate_mode(Planting_WS_N)
+   )
> 
> # PLace NaN by NAs in averages
> district_summary <- district_summary %>%
+   mutate(
+     avg_Planting_WS_N = ifelse(is.nan(avg_Planting_WS_N), NA, avg_Planting_WS_N),
+     avg_Planting_WS_E = ifelse(is.nan(avg_Planting_WS_E), NA, avg_Planting_WS_E),
+     avg_Planting_SA_N = ifelse(is.nan(avg_Planting_SA_N), NA, avg_Planting_SA_N),
+     avg_Planting_SA_E = ifelse(is.nan(avg_Planting_SA_E), NA, avg_Planting_SA_E)
+   )
> 
> print(district_summary)
# A tibble: 140 × 9
   ProDis      avg_Planting_SA_E mode_Planting_SA_E avg_Planting_SA_N mode_Planting_SA_N avg_Planting_WS_E
   <chr>                   <dbl>              <dbl>             <dbl>              <dbl>             <dbl>
 1 An Giang-A…              3.69                3                3.69                3                11.9
 2 An Giang-C…              3.5                 3.5              3.5                 3.5              10.5
 3 An Giang-C…              3.5                 3.5              3.5                 3.5              10.5
 4 An Giang-C…              3.5                 3.5              3.5                 3.5              10.7
 5 An Giang-C…              4.5                 4.5              3.5                 3.5              12.5
 6 An Giang-L…              3.81                3.5              3.81                3.5              10.6
 7 An Giang-P…              4.5                 4.5              4.5                 4.5              12.5
 8 An Giang-T…              3.41                3.5              3.42                3.5              10.3
 9 An Giang-T…              3.36                3.5              3.40                3.5              10.3
10 An Giang-T…              4.5                 4.5              4.5                 4.5              12.5
# ℹ 130 more rows
# ℹ 3 more variables: mode_Planting_WS_E <dbl>, avg_Planting_WS_N <dbl>, mode_Planting_WS_N <dbl>
# ℹ Use `print(n = ...)` to see more rows
> 
> 
> # Test of significance between mean and mode
> t_test_SA <- t.test(
+   district_summary$avg_Planting_SA_E,
+   district_summary$mode_Planting_SA_E,
+   paired = TRUE,
+   alternative = "two.sided"
+ )
> 
> t_test_WS <- t.test(
+   district_summary$avg_Planting_WS_E,
+   district_summary$mode_Planting_WS_E,
+   paired = TRUE,
+   alternative = "two.sided"
+ )
> 
> # Print the results
> print("Significance test for Planting_SA_E (mean vs. mode):")
[1] "Significance test for Planting_SA_E (mean vs. mode):"
> print(t_test_SA)

	Paired t-test

data:  district_summary$avg_Planting_SA_E and district_summary$mode_Planting_SA_E
t = 1.9717, df = 122, p-value = 0.0509
alternative hypothesis: true mean difference is not equal to 0
95 percent confidence interval:
 -0.0002255287  0.1134364048
sample estimates:
mean difference 
     0.05660544 

> 
> print("Significance test for Planting_WS_E (mean vs. mode):")
[1] "Significance test for Planting_WS_E (mean vs. mode):"
> print(t_test_WS)

	Paired t-test

data:  district_summary$avg_Planting_WS_E and district_summary$mode_Planting_WS_E
t = 0.16687, df = 118, p-value = 0.8678
alternative hypothesis: true mean difference is not equal to 0
95 percent confidence interval:
 -0.04831845  0.05721112
sample estimates:
mean difference 
    0.004446339 

> # = Sign differences for WS but not for SA
> 
> # Skewness by District
> skewness_summary <- Merge %>%
+   group_by(ProDis) %>%
+   summarise(
+     skewness_Planting_SA_E = skewness(Planting_SA_E, na.rm = TRUE),
+     skewness_Planting_SA_N = skewness(Planting_SA_N, na.rm = TRUE),
+     skewness_Planting_WS_E = skewness(Planting_WS_E, na.rm = TRUE),
+     skewness_Planting_WS_N = skewness(Planting_WS_N, na.rm = TRUE)
+   )
> 
> # View the skewness summary
> # NaNs indicate a similar distribution, no variation
> print(skewness_summary)
# A tibble: 140 × 5
   ProDis      skewness_Planting_SA_E skewness_Planting_SA_N skewness_Planting_WS_E skewness_Planting_WS_N
   <chr>                        <dbl>                  <dbl>                  <dbl>                  <dbl>
 1 An Giang-A…                 0.205                  0.195                   0.320                  0.320
 2 An Giang-C…               NaN                    NaN                     NaN                    NaN    
 3 An Giang-C…               NaN                    NaN                     NaN                    NaN    
 4 An Giang-C…               NaN                    NaN                       1.68                   1.68 
 5 An Giang-C…               NaN                    NaN                     NaN                    NaN    
 6 An Giang-L…                -0.0746                -0.0746                  1.49                   1.72 
 7 An Giang-P…               NaN                    NaN                     NaN                    NaN    
 8 An Giang-T…                 2.13                   2.13                    1.67                   1.07 
 9 An Giang-T…                 1.06                   1.27                    2.08                   1.86 
10 An Giang-T…               NaN                    NaN                     NaN                    NaN    
# ℹ 130 more rows
# ℹ Use `print(n = ...)` to see more rows
> 
> # Plots
> hist (skewness_summary$skewness_Planting_WS_E , col = "red", breaks = 50, xlim = c(-50, 50)) # Basic Histogramme (Breaks = nb of bars)
> hist (skewness_summary$skewness_Planting_SA_E, col = "blue", breaks = 50, xlim = c(-50, 50)) # Basic Histogramme (Breaks = nb of bars)
> 
> print (mean_skewness_Planting_WS_E <- mean(
+   skewness_summary$skewness_Planting_WS_E[!is.nan(skewness_summary$skewness_Planting_WS_E)],
+   na.rm = TRUE
+ )) # Mean skewness by district = 0.42
[1] 0.4229573
> 
> print (mean_skewness_Planting_SA_E <- mean(
+   skewness_summary$skewness_Planting_SA_E[!is.nan(skewness_summary$skewness_Planting_SA_E)],
+   na.rm = TRUE
+ )) # Mean skewness by district = 0.94
[1] 0.9412896
> 
> # Conclusion: mean and mode are significantly different
> # skewness is positive for both seasons, the distribution has a right tail (more lower values), so the mode (the most frequent value) is used
> # Can also Split the provinces by those with homogeneous planting dates (use the mean) and those with disparate dates (use the mode).
> # Use variable district_summary for merging
> 
> 
> 
> 
> # Input GSO commune data
> curl_function ("data/processed/GSO_dstricts.xls")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GSO_dstricts.xls'
Content type 'application/octet-stream' length 1762304 bytes (1.7 MB)
downloaded 1.7 MB

> GSO_Dist <- read_excel("data/processed/GSO_dstricts.xls")
New names:
• `` -> `...7`
• `` -> `...8`
> 
> GSO_Dist$Province_name <- gsub("^Tỉnh\\s*", "", GSO_Dist$Province_name, ignore.case = TRUE)
> GSO_Dist$Province_name <- gsub("^Thành phố\\s*", "", GSO_Dist$Province_name, ignore.case = TRUE)
> 
> GSO_Dist$District_name <- gsub("^Thành phố\\s*", "", GSO_Dist$District_name, ignore.case = TRUE)
> GSO_Dist$District_name <- gsub("^Quận\\s*", "", GSO_Dist$District_name, ignore.case = TRUE)
> GSO_Dist$District_name <- gsub("^Thị xã\\s*", "", GSO_Dist$District_name, ignore.case = TRUE)
> GSO_Dist$District_name <- gsub("^Huyện\\s*", "", GSO_Dist$District_name, ignore.case = TRUE)
> 
> Merge$NAME_2 <- gsub("\\s*Thị xã", "", Merge$NAME_2, ignore.case = TRUE); 
> Merge$NAME_2 <- gsub("\\s*Thành phố", "", Merge$NAME_2, ignore.case = TRUE)
> Merge$NAME_2 <- gsub("\\s*\\(.*?\\)", "", Merge$NAME_2, ignore.case = TRUE); 
> 
> GSO_Dist$ProDis <- paste (GSO_Dist$Province_name, GSO_Dist$District_name, sep='-')
> 
> 
> # 1a. Calculate district-level mode and means
> 
> # Replace 0 values with NA for the specified columns
> Merge <- Merge %>%
+   mutate(
+     Planting_SA_E = ifelse(Planting_SA_E == 0, NA, Planting_SA_E),
+     Planting_SA_N = ifelse(Planting_SA_N == 0, NA, Planting_SA_N),
+     Planting_WS_E = ifelse(Planting_WS_E == 0, NA, Planting_WS_E),
+     Planting_WS_N = ifelse(Planting_WS_N == 0, NA, Planting_WS_N)
+   )
> 
> # Define a function to calculate the mode
> calculate_mode <- function(x) {
+   x <- x[!is.na(x)]  # Remove NA values
+   if (length(x) == 0) return(NA)  # Return NA if no values are present
+   uniq_vals <- unique(x)
+   uniq_vals[which.max(tabulate(match(x, uniq_vals)))]
+ }
> 
> # Calculate the mean and mode at the district level
> district_summary <- Merge %>%
+   group_by(ProDis) %>%
+   summarise(
+     avg_Planting_SA_E = mean(Planting_SA_E, na.rm = TRUE),
+     mode_Planting_SA_E = calculate_mode(Planting_SA_E),
+     
+     avg_Planting_SA_N = mean(Planting_SA_N, na.rm = TRUE),
+     mode_Planting_SA_N = calculate_mode(Planting_SA_N),
+     
+     avg_Planting_WS_E = mean(Planting_WS_E, na.rm = TRUE),
+     mode_Planting_WS_E = calculate_mode(Planting_WS_E),
+     
+     avg_Planting_WS_N = mean(Planting_WS_N, na.rm = TRUE),
+     mode_Planting_WS_N = calculate_mode(Planting_WS_N)
+   )
> 
> # PLace NaN by NAs in averages
> district_summary <- district_summary %>%
+   mutate(
+     avg_Planting_WS_N = ifelse(is.nan(avg_Planting_WS_N), NA, avg_Planting_WS_N),
+     avg_Planting_WS_E = ifelse(is.nan(avg_Planting_WS_E), NA, avg_Planting_WS_E),
+     avg_Planting_SA_N = ifelse(is.nan(avg_Planting_SA_N), NA, avg_Planting_SA_N),
+     avg_Planting_SA_E = ifelse(is.nan(avg_Planting_SA_E), NA, avg_Planting_SA_E)
+   )
> 
> 
> # 1b. We first merge commune names to obtain GSO admin codes
> district_summary <- merge(district_summary, GSO_Dist [, c(1:4,9)], by = "ProDis", all.x = TRUE); 
> district_summary <- district_summary %>% distinct(ProDis, .keep_all = TRUE)
> 
> Unmatched <- district_summary [is.na (district_summary$MATINH) & !duplicated(district_summary$ProDis),] # Test if unmatched CS-MAPs Districts exist in the data
> Unmatched
                            ProDis avg_Planting_SA_E mode_Planting_SA_E avg_Planting_SA_N
47  Đồng Tháp-Cao Lãnh (Thành phố)          2.500000                2.5          2.500000
50     Đồng Tháp-Hồng Ngự (Thị xã)          3.061224                3.5          3.065657
61      Hậu Giang-Long Mỹ (Thị xã)          3.248945                3.0          3.248945
115    Tiền Giang-Cai Lậy (Thị xã)          2.823256                2.5          2.509311
129    Trà Vinh-Duyên Hải (Thị xã)          6.000000                6.0          6.000000
    mode_Planting_SA_N avg_Planting_WS_E mode_Planting_WS_E avg_Planting_WS_N mode_Planting_WS_N
47                 2.5          10.50000               10.5          10.50000               10.5
50                 3.5          10.72727               10.5          10.72727               10.5
61                 3.0          11.64627               11.5          11.68706               11.5
115                2.5          11.02048               11.0          11.50000               11.5
129                6.0          10.00000               10.0          12.00000               12.0
    Province_name MATINH District_name MAHUYEN
47           <NA>   <NA>          <NA>    <NA>
50           <NA>   <NA>          <NA>    <NA>
61           <NA>   <NA>          <NA>    <NA>
115          <NA>   <NA>          <NA>    <NA>
129          <NA>   <NA>          <NA>    <NA>
> 
> # STILL has 5 UNMATCHED DISTRICTS TO CORRECT ................................................
> 
> 
> 
> # 2. Add lunar dates from VHLSS; 
> curl_function ("data/raw/VHLSS_2022_Household/datasets/Ho_Muc4B11_edited.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Ho_Muc4B11_edited.csv'
Content type 'text/plain; charset=utf-8' length 1992008 bytes (1.9 MB)
downloaded 1.9 MB

> VH22 <- read.csv ("data/raw/VHLSS_2022_Household/datasets/Ho_Muc4B11_edited.csv") # 2022
> 
> curl_function ("data/raw/VHLSS_2023_Household/Final/Muc4B11.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Final/Muc4B11.csv'
Content type 'text/plain; charset=utf-8' length 1846335 bytes (1.8 MB)
downloaded 1.8 MB

> VH23 <- read.csv ("data/raw/VHLSS_2023_Household/Final/Muc4B11.csv") # 2023
> VH22$Year <- '2022'; VH23$Year <- '2023'
> 
> district_summary$ID <- paste (district_summary$MATINH, district_summary$MAHUYEN, sep='-')
> VH22$ID <- paste (VH22$MATINH, VH22$MAHUYEN, sep='-'); VH23$ID <- paste (VH23$MATINH, VH23$MAHUYEN, sep='-')
> 
> curl_function ("data/processed/result_gregorian_final.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/result_gregorian_final.csv'
Content type 'text/plain; charset=utf-8' length 41849 bytes (40 KB)
downloaded 40 KB

> Lunar <- read.csv ("data/processed/result_gregorian_final.csv")
> # = Dataset converting Lunar dates into gregorian dates
> 
> # Convert lunar dates into Dates format
> VH22$Dates_VH <- as.Date(paste(VH22$M4B11_C3AY, VH22$M4B11_C3AM, VH22$M4B11_C3AD, sep = "-"), format = "%Y-%m-%d")
> VH23$Dates_VH <- as.Date(paste(VH23$M4B11_C3AY, VH23$M4B11_C3AM, VH23$M4B11_C3AD, sep = "-"), format = "%Y-%m-%d")
> Lunar$Date_collected <- as.Date(paste(Lunar$sowing_year, Lunar$sowing_month, Lunar$sowing_day, sep = "-"), format = "%Y-%m-%d")
> Lunar$Date_converted <- as.Date(paste(Lunar$gre_year, Lunar$gre_month, Lunar$sowing_day, sep = "-"), format = "%Y-%m-%d")
> 
> VH22 <- VH22[!is.na (VH22$Dates_VH), ]; VH23 <- VH23[!is.na (VH23$Dates_VH), ]
> 
> VH22 <- merge (VH22, Lunar, by.x = "Dates_VH", by.y = "Date_collected", all.x = TRUE); VH22 <- VH22[!is.na (VH22$Dates_VH), ] # 20,616 planting dates in VH22
> VH23 <- merge (VH23, Lunar, by.x = "Dates_VH", by.y = "Date_collected", all.x = TRUE); VH23 <- VH23[!is.na (VH23$Dates_VH), ] # 21,800 planting dates in VH23
> 
> VH22$Pl_date <- ifelse (VH22$M4B11_C3B.x == 1, VH22$Date_converted, VH22$Dates_VH)
> VH23$Pl_date <- ifelse (VH23$M4B11_C3B.x == 1, VH23$Date_converted, VH23$Dates_VH)
> 
> VH22$Pl_date <- as.Date(VH22$Pl_date)
> VH23$Pl_date <- as.Date(VH23$Pl_date)
> 
> 
> # 3. Merge VHLSS with CS- Maps dataset
> VH23 <- VH23 [, -17]
> VH <- full_join (VH22, VH23)
Joining with `by = join_by(Dates_VH, IDHO, MATINH, MAHUYEN, MAXA, MADIABAN, HOSO, KYDIEUTRA, M4B11_MA,
M4B11_C3, M4B11_C3AD, M4B11_C3AM, M4B11_C3AY, M4B11_C3B.x, M4B11_C4, M4B11_C5, Year, ID, result,
sowing_day, sowing_month, sowing_year, M4B11_C3B.y, gre_day, gre_month, gre_year, Date_converted,
Pl_date)`
> VH <- VH [VH$MATINH %in% c(80, 82, 83, 84, 86, 87 ,89, 91, 92, 93,94,95,96) & VH$M4B11_MA %in% c(2, 3), ] # Drop non MRD procvinces - 6,037 Obs (4,504 in two crop seasons WS and SA)
> 
> VH_merged <- merge (district_summary, VH, by= "ID", all.y=TRUE)
> VH_merged <- VH_merged[!is.na (VH_merged$Pl_date), ] # NAs on Planting dates. Total = 3,264 Obs
> 
> VH$M4B11_MA <- as.character(VH$M4B11_MA); VH[VH$M4B11_MA == "2", "M4B11_MA"] <- 'WS'
> VH$M4B11_MA <- as.character(VH$M4B11_MA); VH[VH$M4B11_MA == "3", "M4B11_MA"] <- 'SA'
> 
> 
> # Describe events-HHs
> non_unique_IDHO <- VH$IDHO[duplicated(VH$IDHO) | duplicated(VH$IDHO, fromLast = TRUE)]
> 
> VH_non_unique <- VH[VH$IDHO %in% non_unique_IDHO, ]
> summary_stats <- aggregate(cbind(M4B11_MA, Year) ~ IDHO, data = VH_non_unique, 
+                            function(x) c(Count = length(x), Unique = length(unique(x)), 
+                                          Values = paste(unique(x), collapse = ", ")))  # Describe the events per hhs [2:4 rows/events]
> 
> table (summary_stats$Year) # 357 events are repeated in 2022 and 2023

         1          2       2022 2022, 2023       2023          3          4 
      1482       1866        770        357        712         33        297 
> length(unique(VH$IDHO)) # 2038 hhs have more than one date. Often two seasons
[1] 2038
> 
> write.csv (VH_merged, 'Output/VH_merged.csv')
> rm(list=setdiff(ls(), c("VH_merged", "curl_function", "token")))
> 
> 
> # We keep Planting_WS_E as it is (numeric)
> 
> 
> 
> # Merge with VHLSS hhs to get sample size
> 
> 
> 
> # Reformat at district level
> VH_Dis <- VH_merged %>% distinct(ProDis, .keep_all = TRUE) # 92 districts
> 
> VH_Dis$Comp_WS <- ifelse (VH_Dis$mode_Planting_WS_N == VH_Dis$mode_Planting_WS_E, 'No change', 
+                           ifelse (VH_Dis$mode_Planting_WS_N > VH_Dis$mode_Planting_WS_E, 'Early planting',
+                                   ifelse (VH_Dis$mode_Planting_WS_N < VH_Dis$mode_Planting_WS_E, 'Late planting', NA)))
> 
> VH_Dis <- VH_Dis %>%
+   mutate(Comp_WS_magn = case_when(
+     Comp_WS != 'No change' & mode_Planting_WS_E == 0 & mode_Planting_WS_N != 0 ~ NA_real_,
+     Comp_WS != 'No change' ~ mode_Planting_WS_E - mode_Planting_WS_N,
+     TRUE ~ 0
+   ))
> 
> 
> VH_Dis$Comp_SA <- ifelse (VH_Dis$mode_Planting_SA_N == VH_Dis$mode_Planting_SA_E, 'No change', 
+                           ifelse (VH_Dis$mode_Planting_SA_N > VH_Dis$mode_Planting_SA_E, 'Early planting',
+                                   ifelse (VH_Dis$mode_Planting_SA_N < VH_Dis$mode_Planting_SA_E, 'Late planting', NA)))
> 
> VH_Dis <- VH_Dis %>%
+   mutate(Comp_SA_magn = case_when(
+     Comp_SA != 'No change' & mode_Planting_SA_E == 0 & mode_Planting_SA_N != 0 ~ NA_real_,
+     Comp_SA != 'No change' ~ mode_Planting_SA_E - mode_Planting_SA_N,
+     TRUE ~ 0
+   ))
> 
> VH_merged2 <- merge (VH_merged, VH_Dis [, c(2,42:45)], by='ProDis', all.x=TRUE)
> table (VH_merged2$Comp_WS); table (VH_merged2$Comp_SA);                                     

Early planting  Late planting      No change 
           998            186           1825 

Early planting  Late planting      No change 
           482            688           1885 
> 
> # Irregulariries
> # ~25 Obs go from 5.5 to 0 in extreme scenario (SA). Idem in WS from 11.5 to 0
> # Figure 23. Magnitude of change in planting dates recommended by the CS-MAPs in extreme scenarios -----
> 
> round (prop.table (table (VH_merged2$Comp_WS_magn)), 2) * 100

  -2 -1.5   -1 -0.5    0  0.5 
   2    3   11   14   64    6 
> round (prop.table (table (VH_merged2$Comp_SA_magn)), 2) * 100
numeric(0)
> 
> data <- data.frame(
+   Diff = c(-3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3),
+   WS = c(0, 0, 2, 3, 11, 14, 64, 6, 0, 0, 0, 0, 0),
+   SA = c(0, 2, 2, 3, 2, 7, 64, 16, 0, 4, 0, 0, 0)
+ )
> 
> 
> # Transform the data into long format for ggplot2
> data_long <- data %>%
+   pivot_longer(cols = c("WS", "SA"), names_to = "Season", values_to = "Percentage") %>%
+   mutate(Season = recode(Season, "WS" = "Winter-Spring", "SA" = "Summer-Autumn"))
> 
> # Plot using ggplot2
> ggplot(data_long, aes(x = Diff, y = Percentage, fill = Season)) +
+   geom_bar(stat = "identity", position = "dodge") +
+   scale_fill_manual(values = c("cornflowerblue", "salmon")) +
+   scale_x_continuous(breaks = seq(-3, 3, by = 0.5)) +
+   scale_y_continuous(breaks = seq(0, 100, by = 10)) +  # Add horizontal lines at each 10 points on y-axis
+   labs(
+     x = "Diff. in planting dates (months)",
+     y = "% households"
+   ) +
+   coord_cartesian(ylim = c(0, 100)) +  # Set y-axis limits
+   theme_minimal() +
+   theme(
+     legend.title = element_blank(),
+     panel.grid.major.x = element_blank(),  # Remove major grid lines on x-axis
+     panel.grid.minor = element_blank(),     # Remove minor grid lines
+     plot.margin = margin(t = 10, r = 10, b = 30, l = 10),  # Add space at the bottom of the plot
+     axis.title.x = element_text(margin = margin(t = 15))   # Add space between x-axis label and axis
+   )
> # Table 20. Association of households’ planting dates with CS-MAP recommended dates: Winter-Spring and Summer-Autoumn seasons ----
> 
> VH_merged <- VH_merged %>%
+   mutate(
+     # Recode Pl_date to numeric format based on day ranges
+     Pl_date_numeric = case_when(
+       day(Pl_date) >= 1 & day(Pl_date) <= 15 ~ month(Pl_date) + 0.0,  # First half of the month
+       day(Pl_date) > 15 ~ month(Pl_date) + 0.5                          # Second half of the month
+     )
+   )
> 
> 
> VH_merged$Pl_date_year <- year (VH_merged$Pl_date) # Indicate the Year of hh planting date
> 
> convert_to_fortnight <- function(date) { # Events are number as fortnight per year, from 1 to 26.
+   day <- day(date)
+   month <- month(date)
+   fortnight <- ifelse(day <= 15, 0, 1) 
+   return((month - 1) * 2 + fortnight + 1)
+ }
> 
> VH_merged$Pl_date_fortnight <- sapply(VH_merged$Pl_date, convert_to_fortnight)
> 
> # Convert CS-Maps dates
> convert_to_date <- function(variable) {
+   as.Date(paste("2000",  # Use a placeholder year
+                 floor(as.numeric(variable)), 
+                 ifelse(as.numeric(variable) %% 1 == 0, 1, 16), 
+                 sep = "-"))
+ }
> 
> # Apply the helper function to each planting variable
> VH_merged$Planting_WS_E <- convert_to_date(VH_merged$mode_Planting_WS_E)
> VH_merged$Planting_SA_E <- convert_to_date(VH_merged$mode_Planting_SA_E)
> VH_merged$Planting_WS_N <- convert_to_date(VH_merged$mode_Planting_WS_N)
> VH_merged$Planting_SA_N <- convert_to_date(VH_merged$mode_Planting_SA_N)
> 
> # Apply the conversion to fortnights
> VH_merged$Planting_SA_E_fortnight <- sapply(VH_merged$Planting_SA_E, convert_to_fortnight)
> VH_merged$Planting_WS_E_fortnight <- sapply(VH_merged$Planting_WS_E, convert_to_fortnight)
> VH_merged$Planting_SA_N_fortnight <- sapply(VH_merged$Planting_SA_N, convert_to_fortnight)
> VH_merged$Planting_WS_N_fortnight <- sapply(VH_merged$Planting_WS_N, convert_to_fortnight)
> 
> # Covnert from fortnight to days to faciliate regression readings
> VH_merged$Pl_date_fortnight <- VH_merged$Pl_date_fortnight * 14
> VH_merged$Planting_SA_E_fortnight <- VH_merged$Planting_SA_E_fortnight * 14
> VH_merged$Planting_WS_E_fortnight <- VH_merged$Planting_WS_E_fortnight * 14
> VH_merged$Planting_SA_N_fortnight <- VH_merged$Planting_SA_N_fortnight * 14
> VH_merged$Planting_WS_N_fortnight <- VH_merged$Planting_WS_N_fortnight * 14
> 
> 
> # Add Ag plans
> NAME_1 <- c("Bạc Liêu", "Cà Mau", "An Giang", "Kiên Giang", "Long An", 
+             "Trà Vinh", "Bến Tre", "Cần Thơ", "Hậu Giang", 
+             "Sóc Trăng", "Đồng Tháp", "Vinh Long", "Tien Giang")
> 
> years <- c(2021, 2022, 2023, 2024)
> 
> # Create a data frame with all combinations of provinces and years
> elnino_data <- expand.grid(Province = NAME_1, Year = years)
> 
> # Set Ave_High_mention to 1 for the specified conditions
> high_mentions <- data.frame(
+   Province = c("Bạc Liêu", "Cà Mau", "Bạc Liêu", "An Giang", "An Giang", 
+                "Kiên Giang", "Kiên Giang", "Long An", "Long An", 
+                "Trà Vinh"),
+   Year = c(2021, 2022, 2023, 2023, 2024, 2023, 2024, 2023, 2024, 2023),
+   Ave_High_mention = 1
+ )
> 
> # Merge the two data frames and set Ave_High_mention to 0 where not present
> elnino_data <- merge(elnino_data, high_mentions, by = c("Province", "Year"), all.x = TRUE)
> elnino_data$Ave_High_mention[is.na(elnino_data$Ave_High_mention)] <- 0
> names (elnino_data)[1] <- 'NAME_1'
> 
> table (elnino_data$Ave_High_mention)

 0  1 
42 10 
> 
> elnino_data$Ave_High_mention_2021 <- ifelse(elnino_data$Year == 2021, elnino_data$Ave_High_mention, 0)
> elnino_data$Ave_High_mention_2022 <- ifelse(elnino_data$Year == 2022, elnino_data$Ave_High_mention, 0)
> elnino_data$Ave_High_mention_2023 <- ifelse(elnino_data$Year == 2023, elnino_data$Ave_High_mention, 0)
> 
> # Merge based on VH_merged$Pl_date_year and elnino_data$Year
> VH_merged <- merge(VH_merged, elnino_data, 
+                    by.x = c('Province_name', 'Pl_date_year'), 
+                    by.y = c('NAME_1', 'Year'), 
+                    all.x = TRUE)
> 
> 
> # Modelling
> 
> VH_merged$WS_Advice_received <- ifelse (VH_merged$Pl_date_year == '2021' & VH_merged$Ave_High_mention_2021 == 1, VH_merged$Planting_WS_E_fortnight, 
+                                         ifelse (VH_merged$Pl_date_year == '2021' & VH_merged$Ave_High_mention_2021 ==0, VH_merged$Planting_WS_N_fortnight,   
+                                                 ifelse (VH_merged$Pl_date_year == '2022' & VH_merged$Ave_High_mention_2022 == 1, VH_merged$Planting_WS_E_fortnight, 
+                                                         ifelse (VH_merged$Pl_date_year == '2022' & VH_merged$Ave_High_mention_2022 ==0, VH_merged$Planting_WS_N_fortnight,   
+                                                                 ifelse (VH_merged$Pl_date_year == '2023' & VH_merged$Ave_High_mention_2023 == 1, VH_merged$Planting_WS_E_fortnight, 
+                                                                         ifelse (VH_merged$Pl_date_year == '2023' & VH_merged$Ave_High_mention_2023 ==0, VH_merged$Planting_WS_N_fortnight, NA))))))
> 
> 
> VH_merged$SA_Advice_received <- ifelse (VH_merged$Pl_date_year == '2021' & VH_merged$Ave_High_mention_2021 == 1, VH_merged$Planting_SA_E_fortnight, 
+                                         ifelse (VH_merged$Pl_date_year == '2021' & VH_merged$Ave_High_mention_2021 ==0, VH_merged$Planting_SA_N_fortnight,   
+                                                 ifelse (VH_merged$Pl_date_year == '2022' & VH_merged$Ave_High_mention_2022 == 1, VH_merged$Planting_SA_E_fortnight, 
+                                                         ifelse (VH_merged$Pl_date_year == '2022' & VH_merged$Ave_High_mention_2022 ==0, VH_merged$Planting_SA_N_fortnight,   
+                                                                 ifelse (VH_merged$Pl_date_year == '2023' & VH_merged$Ave_High_mention_2023 == 1, VH_merged$Planting_SA_E_fortnight, 
+                                                                         ifelse (VH_merged$Pl_date_year == '2023' & VH_merged$Ave_High_mention_2023 ==0, VH_merged$Planting_SA_N_fortnight, NA))))))
> 
> 
> VH_merged$Year.2022 <- ifelse (VH_merged$Pl_date_year == '2022', 1, 0)
> VH_merged$Year.2023 <- ifelse (VH_merged$Pl_date_year == '2023', 1, 0)
> 
> VH_merged$SA_Ext_year <- ifelse (VH_merged$Ave_High_mention_2021 == 1 | VH_merged$Ave_High_mention_2022 == 1 | VH_merged$Ave_High_mention_2023 == 1, 1, 0)
> VH_merged$WS_Ext_year <- ifelse (VH_merged$Ave_High_mention_2021 == 1 | VH_merged$Ave_High_mention_2022 == 1 | VH_merged$Ave_High_mention_2023 == 1, 1, 0)
> 
> VH_merged$EA_ID <- paste (VH_merged$MATINH.x, VH_merged$MAHUYEN.x, VH_merged$MAXA, VH_merged$MADIABAN, sep='-')
> 
> 
> # A) SUmmer-Automn
> VH_SA <- VH_merged [VH_merged$M4B11_MA == 3 ,]
> 
> # OLS 1 - DO farmer follow the advice?
> #model_1 <- lm (Pl_date_numeric ~ SA_Advice_received,  clusters = EA_ID, data = VH_SA )
> model_1 <- lm_robust(Pl_date_numeric ~ SA_Advice_received, data = VH_SA, clusters = EA_ID)
> 
> # OLS 2 - DO farmer follow the advice, after controlling for year variations?
> model_2 <- lm_robust (Pl_date_numeric ~ SA_Advice_received + Year.2022 + Year.2023, clusters = EA_ID, data = VH_SA )
> 
> # OLS 3 - DO farmer better follow the advice in extreme vs normal year?
> model_3 <- lm_robust (Pl_date_numeric ~ SA_Advice_received + SA_Ext_year + SA_Advice_received * SA_Ext_year + Year.2022 + Year.2023, clusters = EA_ID, data = VH_SA )
> 
> models <- list(model_1, model_2, model_3)
> models <- models[!sapply(models, is.null)]
> 
> SA_table <- export_summs(models, 
+                          error_format = "({std.error})", 
+                          statistics = c(N = "nobs", R2 = "r.squared"),
+                          digits = 3)
> 
> write.xlsx(SA_table, "Output/Tab20.CSMAP_model_SA.xlsx")
> 
> 
> # B) Winter_Spring
> VH_WS <- VH_merged [VH_merged$M4B11_MA == 2 ,]
> 
> # OLS 1 - DO farmer follow the advice?
> model_1b <- lm_robust (Pl_date_numeric ~ WS_Advice_received, clusters = EA_ID, data = VH_WS )
> 
> # OLS 2 - DO farmer follow the advice, after controlling for year variations?
> model_2b <- lm_robust (Pl_date_numeric ~ WS_Advice_received + Year.2022 + Year.2023, clusters = EA_ID, data = VH_WS )
> 
> # OLS 3 - DO farmer better follow the advice in extreme vs normal year?
> model_3b <- lm_robust (Pl_date_numeric ~ WS_Advice_received + WS_Ext_year + WS_Advice_received * WS_Ext_year + Year.2022 + Year.2023, clusters = EA_ID, data = VH_WS )
> 
> models <- list(model_1b, model_2b, model_3b)
> models <- models[!sapply(models, is.null)]
> 
> # Method 2: Display summary using export_summs from jtools
> WS_table <- export_summs(models, 
+                          error_format = "({std.error})", 
+                          statistics = c(N = "nobs", R2 = "r.squared"),
+                          digits = 3)
> 
> write.xlsx(WS_table, "Output/Tab20.CSMAP_model_WS.xlsx")
> # CS-MAPs number of adopters ----
> 
> VH_merged <- VH_merged %>%
+   mutate(
+     # Recode Pl_date to numeric format based on day ranges
+     Pl_date_numeric = case_when(
+       day(Pl_date) >= 1 & day(Pl_date) <= 15 ~ month(Pl_date) + 0.0,  # First half of the month
+       day(Pl_date) > 15 ~ month(Pl_date) + 0.5                          # Second half of the month
+     )
+   )
> 
> 
> VH_merged$Pl_date_year <- year (VH_merged$Pl_date) # Indicate the Year of hh planting date
> 
> convert_to_fortnight <- function(date) { # Events are number as fortnight per year, from 1 to 26.
+   day <- day(date)
+   month <- month(date)
+   fortnight <- ifelse(day <= 15, 0, 1) 
+   return((month - 1) * 2 + fortnight + 1)
+ }
> convert_to_date <- function(variable) {
+   as.Date(paste("2000",  # Use a placeholder year
+                 floor(as.numeric(variable)), 
+                 ifelse(as.numeric(variable) %% 1 == 0, 1, 16), 
+                 sep = "-"))
+ }
> 
> VH_merged$Pl_date_fortnight <- sapply(VH_merged$Pl_date, convert_to_fortnight)
> 
> # Apply the helper function to each planting variable
> VH_merged$Planting_WS_E <- convert_to_date(VH_merged$mode_Planting_WS_E)
> VH_merged$Planting_SA_E <- convert_to_date(VH_merged$mode_Planting_SA_E)
> VH_merged$Planting_WS_N <- convert_to_date(VH_merged$mode_Planting_WS_N)
> VH_merged$Planting_SA_N <- convert_to_date(VH_merged$mode_Planting_SA_N)
> 
> # Apply the conversion to fortnights
> VH_merged$Planting_SA_E_fortnight <- sapply(VH_merged$Planting_SA_E, convert_to_fortnight)
> VH_merged$Planting_WS_E_fortnight <- sapply(VH_merged$Planting_WS_E, convert_to_fortnight)
> VH_merged$Planting_SA_N_fortnight <- sapply(VH_merged$Planting_SA_N, convert_to_fortnight)
> VH_merged$Planting_WS_N_fortnight <- sapply(VH_merged$Planting_WS_N, convert_to_fortnight)
> 
> #VH_merged$Planting_SA_E_fortnight; VH_merged$Planting_SA_E_fortnight; 
> VH_merged$Planting_SA_E # date
   [1] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
   [8] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [15] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [22] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [29] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [36] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [43] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [50] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [57] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [64] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [71] "2000-03-16" "2000-03-01" "2000-03-16" "2000-03-01" "2000-03-16" "2000-03-16" "2000-03-16"
  [78] "2000-03-16" "2000-03-16" "2000-04-16" "2000-03-16" "2000-04-16" "2000-04-16" "2000-03-16"
  [85] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [92] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
  [99] "2000-03-01" "2000-03-16" "2000-03-01" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [106] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [113] "2000-03-16" "2000-03-16" "2000-03-16" "2000-04-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [120] "2000-03-16" "2000-04-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [127] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [134] "2000-03-16" "2000-03-16" "2000-04-16" "2000-03-16" "2000-04-16" "2000-04-16" "2000-03-16"
 [141] "2000-03-16" "2000-04-16" "2000-03-16" "2000-04-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [148] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [155] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [162] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-04-16"
 [169] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [176] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-04-16" "2000-03-16"
 [183] "2000-04-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-06-01" "2000-04-01" "2000-06-01"
 [190] "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [197] "2000-06-01" "2000-06-01" "2000-06-01" "2000-04-01" "2000-04-01" "2000-06-01" "2000-06-01"
 [204] "2000-04-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [211] "2000-04-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [218] "2000-04-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-04-01" "2000-04-01" "2000-04-01"
 [225] "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [232] "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01"
 [239] "2000-04-01" "2000-06-01" "2000-04-01" "2000-04-01" "2000-06-01" "2000-04-01" "2000-04-01"
 [246] "2000-06-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-06-01" "2000-04-01"
 [253] "2000-04-01" "2000-04-01" "2000-04-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [260] "2000-06-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-06-01"
 [267] "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-06-01"
 [274] "2000-04-01" "2000-04-01" "2000-06-01" "2000-04-01" "2000-04-01" "2000-06-01" "2000-04-01"
 [281] "2000-04-01" "2000-04-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-04-01" "2000-04-01"
 [288] "2000-04-01" "2000-04-01" "2000-04-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-04-01"
 [295] "2000-04-01" "2000-06-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-04-01" "2000-04-01"
 [302] "2000-06-01" "2000-04-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [309] "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [316] "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [323] "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [330] "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [337] "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-06-01"
 [344] "2000-06-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-06-01"
 [351] "2000-06-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-06-01" "2000-06-01" "2000-06-01"
 [358] "2000-06-01" "2000-06-01" "2000-06-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-06-01"
 [365] "2000-04-01" "2000-04-01" "2000-04-01" "2000-04-01" "2000-06-01" "2000-04-01" "2000-06-01"
 [372] "2000-06-01" "2000-06-01" "2000-06-01" "2000-04-01" "2000-06-01" "2000-05-16" "2000-05-16"
 [379] "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16"
 [386] "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16"
 [393] "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16"
 [400] "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16"
 [407] "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16" "2000-05-16"
 [414] "2000-05-16" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01"
 [421] "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01"
 [428] "2000-05-01" "2000-06-16" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01"
 [435] "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01"
 [442] "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01"
 [449] "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01"
 [456] "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01"
 [463] "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01"
 [470] "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01" "2000-05-01"
 [477] "2000-05-01" "2000-05-01" "2000-05-01" "2000-06-16" "2000-05-01" "2000-05-01" "2000-05-01"
 [484] "2000-05-01" "2000-04-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-04-16"
 [491] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-01"
 [498] "2000-03-01" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [505] "2000-02-16" "2000-04-16" "2000-02-16" "2000-04-16" "2000-03-01" "2000-02-16" "2000-02-16"
 [512] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-04-16" "2000-04-16" "2000-02-16"
 [519] "2000-04-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-01" "2000-03-01"
 [526] "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-01" "2000-03-01" "2000-02-16" "2000-02-16"
 [533] "2000-04-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-01" "2000-04-16"
 [540] "2000-02-16" "2000-02-16" "2000-02-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-02-16"
 [547] "2000-04-16" "2000-02-16" "2000-04-16" "2000-03-01" "2000-02-16" "2000-02-16" "2000-02-16"
 [554] "2000-04-16" "2000-03-01" "2000-02-16" "2000-03-01" "2000-03-01" "2000-02-16" "2000-02-16"
 [561] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-01" "2000-03-01" "2000-03-01"
 [568] "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-01" "2000-03-01" "2000-02-16" "2000-03-01"
 [575] "2000-02-16" "2000-04-16" "2000-02-16" "2000-03-01" "2000-02-16" "2000-02-16" "2000-04-16"
 [582] "2000-04-16" "2000-04-16" "2000-03-01" "2000-02-16" "2000-04-16" "2000-03-01" "2000-03-01"
 [589] "2000-02-16" "2000-03-01" "2000-03-01" "2000-04-16" "2000-03-01" "2000-04-16" "2000-02-16"
 [596] "2000-04-16" "2000-03-01" "2000-02-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-04-16"
 [603] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-04-16" "2000-04-16" "2000-02-16"
 [610] "2000-02-16" "2000-02-16" "2000-03-01" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-01"
 [617] "2000-03-01" "2000-03-01" "2000-04-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [624] "2000-04-16" "2000-02-16" "2000-02-16" "2000-03-01" "2000-02-16" "2000-02-16" "2000-03-01"
 [631] "2000-04-16" "2000-03-01" "2000-02-16" "2000-02-16" "2000-02-16" "2000-04-16" "2000-02-16"
 [638] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-01" "2000-04-16" "2000-04-16"
 [645] "2000-03-01" "2000-04-16" "2000-03-01" "2000-03-01" "2000-02-16" "2000-03-01" "2000-02-16"
 [652] "2000-02-16" "2000-02-16" "2000-04-16" "2000-03-01" "2000-02-16" "2000-02-16" "2000-04-16"
 [659] "2000-02-16" "2000-03-01" "2000-02-16" "2000-04-16" "2000-03-01" "2000-02-16" "2000-04-16"
 [666] "2000-04-16" "2000-02-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-02-16" "2000-04-16"
 [673] "2000-02-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-02-16"
 [680] "2000-03-01" "2000-04-16" "2000-04-16" "2000-04-16" "2000-02-16" "2000-02-16" "2000-04-16"
 [687] "2000-02-16" "2000-04-16" "2000-03-01" "2000-02-16" "2000-04-16" "2000-03-01" "2000-03-01"
 [694] "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-01" "2000-04-16" "2000-02-16" "2000-02-16"
 [701] "2000-02-16" "2000-04-16" "2000-04-16" "2000-02-16" "2000-03-01" "2000-04-16" "2000-03-01"
 [708] "2000-02-16" "2000-04-16" "2000-03-01" "2000-04-16" "2000-04-16" "2000-02-16" "2000-04-16"
 [715] "2000-04-16" "2000-03-01" "2000-02-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-02-16"
 [722] "2000-04-16" "2000-04-16" "2000-04-16" "2000-02-16" "2000-04-16" "2000-04-16" "2000-02-16"
 [729] "2000-02-16" "2000-04-16" "2000-04-16" "2000-02-16" "2000-04-16" "2000-03-01" "2000-04-16"
 [736] "2000-04-16" "2000-04-16" "2000-04-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [743] "2000-02-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-02-16" "2000-04-16" "2000-02-16"
 [750] "2000-04-16" "2000-04-16" "2000-03-01" "2000-04-16" "2000-02-16" "2000-03-01" "2000-04-16"
 [757] "2000-02-16" "2000-02-16" "2000-02-16" "2000-04-16" "2000-02-16" "2000-04-16" "2000-04-16"
 [764] "2000-02-16" "2000-02-16" "2000-04-16" "2000-04-16" "2000-02-16" "2000-03-01" "2000-04-16"
 [771] "2000-04-16" "2000-04-16" "2000-02-16" "2000-04-16" "2000-04-16" "2000-04-16" "2000-04-16"
 [778] "2000-02-16" "2000-02-16" "2000-04-16" "2000-03-16" "2000-02-16" "2000-03-16" "2000-03-16"
 [785] "2000-02-16" "2000-03-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-02-16" "2000-03-16"
 [792] "2000-02-16" "2000-02-16" "2000-03-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [799] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-02-16"
 [806] "2000-03-16" "2000-03-16" "2000-02-16" "2000-03-16" "2000-02-16" "2000-03-16" "2000-02-16"
 [813] "2000-03-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-03-16"
 [820] "2000-02-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [827] "2000-03-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-02-16"
 [834] "2000-03-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [841] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-02-16" "2000-02-16"
 [848] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [855] "2000-03-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-02-16"
 [862] "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-02-16" "2000-02-16" "2000-03-16"
 [869] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16"
 [876] "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-02-16"
 [883] "2000-02-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-02-16"
 [890] "2000-03-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [897] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-02-16"
 [904] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-03-16"
 [911] "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-03-16"
 [918] "2000-03-16" "2000-02-16" "2000-03-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [925] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [932] "2000-03-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [939] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [946] "2000-02-16" "2000-03-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-02-16"
 [953] "2000-02-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [960] "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-02-16" "2000-03-16" "2000-03-16"
 [967] "2000-03-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-03-16"
 [974] "2000-02-16" "2000-03-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-03-16"
 [981] "2000-03-16" "2000-03-16" "2000-03-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-02-16"
 [988] "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16" "2000-02-16"
 [995] "2000-03-16" "2000-02-16" "2000-03-16" "2000-03-16" "2000-03-16" "2000-02-16"
 [ reached 'max' / getOption("max.print") -- omitted 2264 entries ]
> VH_merged$Pl_date # date
   [1] "2020-12-20" "2020-12-20" "2020-11-15" "2020-12-10" "2020-11-14" "2021-06-21" "2021-11-25"
   [8] "2021-06-11" "2021-04-22" "2021-04-15" "2021-04-25" "2021-06-05" "2021-12-15" "2021-12-09"
  [15] "2021-05-09" "2021-04-14" "2021-04-20" "2021-04-12" "2021-01-12" "2021-12-15" "2021-05-20"
  [22] "2021-11-05" "2021-04-15" "2021-12-10" "2021-12-14" "2021-05-02" "2021-11-25" "2021-12-12"
  [29] "2022-04-18" "2022-04-09" "2022-04-10" "2022-10-01" "2022-04-06" "2022-11-19" "2022-11-10"
  [36] "2022-11-24" "2022-11-25" "2022-04-10" "2022-12-16" "2022-11-20" "2022-11-22" "2022-11-08"
  [43] "2022-12-25" "2022-06-06" "2022-11-20" "2022-01-17" "2022-12-18" "2022-11-20" "2022-11-20"
  [50] "2022-12-07" "2022-01-22" "2022-04-18" "2022-11-10" "2022-06-20" "2022-12-18" "2022-12-15"
  [57] "2022-04-18" "2022-06-25" "2022-12-20" "2022-11-30" "2022-05-10" "2022-04-17" "2022-12-17"
  [64] "2022-02-15" "2022-12-09" "2022-11-11" "2022-12-17" "2022-04-16" "2022-12-30" "2022-12-15"
  [71] "2022-12-12" "2022-04-25" "2022-12-10" "2022-11-10" "2022-04-29" "2022-04-12" "2022-11-10"
  [78] "2022-12-18" "2022-03-24" "2022-05-10" "2022-05-10" "2022-12-10" "2022-12-18" "2022-03-20"
  [85] "2022-04-04" "2022-12-12" "2022-04-26" "2022-04-04" "2022-11-15" "2022-03-10" "2022-12-12"
  [92] "2022-04-01" "2022-01-13" "2022-04-25" "2022-04-15" "2022-05-01" "2022-04-21" "2022-04-26"
  [99] "2022-11-10" "2022-11-15" "2022-11-20" "2022-04-02" "2022-12-17" "2022-06-01" "2022-11-17"
 [106] "2022-12-03" "2022-12-25" "2022-11-10" "2022-12-12" "2022-12-16" "2022-11-16" "2022-12-12"
 [113] "2022-12-15" "2022-11-16" "2022-11-20" "2022-12-10" "2022-11-16" "2022-12-16" "2022-12-10"
 [120] "2022-06-10" "2022-05-15" "2022-12-12" "2022-12-10" "2022-04-18" "2022-12-18" "2022-11-22"
 [127] "2022-11-15" "2022-05-05" "2022-11-15" "2022-12-16" "2022-11-13" "2022-11-02" "2022-11-12"
 [134] "2022-11-25" "2022-05-04" "2022-12-10" "2022-11-22" "2022-12-10" "2022-12-09" "2022-11-15"
 [141] "2022-01-28" "2023-04-12" "2023-04-18" "2023-05-16" "2023-05-15" "2023-04-08" "2023-04-09"
 [148] "2023-04-26" "2023-04-06" "2023-04-15" "2023-04-20" "2023-04-19" "2023-04-10" "2023-04-28"
 [155] "2023-05-15" "2023-04-20" "2023-07-10" "2023-04-10" "2023-04-15" "2023-04-09" "2023-04-18"
 [162] "2023-04-09" "2023-04-09" "2023-04-16" "2023-04-10" "2023-04-08" "2023-04-15" "2023-05-12"
 [169] "2023-04-17" "2023-04-18" "2023-04-18" "2023-04-20" "2023-04-15" "2023-04-13" "2023-04-09"
 [176] "2023-04-09" "2023-03-15" "2023-04-16" "2023-04-20" "2023-04-25" "2023-05-08" "2023-04-09"
 [183] "2021-11-15" "2021-12-05" "2021-05-21" "2021-12-20" "2021-06-08" "2021-05-22" "2021-12-18"
 [190] "2021-12-19" "2021-05-15" "2021-05-07" "2021-05-15" "2021-05-05" "2021-12-18" "2021-06-10"
 [197] "2021-12-20" "2021-12-22" "2021-12-20" "2021-11-11" "2021-12-06" "2021-07-30" "2021-05-15"
 [204] "2021-04-19" "2021-05-03" "2021-05-15" "2021-12-20" "2021-09-08" "2021-12-20" "2021-05-15"
 [211] "2021-04-18" "2021-05-06" "2021-11-12" "2021-06-10" "2021-05-18" "2021-06-12" "2021-06-10"
 [218] "2021-11-16" "2021-12-18" "2021-04-15" "2021-05-20" "2021-04-15" "2021-11-19" "2022-10-04"
 [225] "2022-05-05" "2022-11-15" "2022-11-10" "2022-04-25" "2022-05-07" "2022-05-10" "2022-05-07"
 [232] "2022-11-25" "2022-11-09" "2022-04-06" "2022-04-28" "2022-11-16" "2022-11-22" "2022-04-25"
 [239] "2022-10-04" "2022-12-20" "2022-11-18" "2022-11-22" "2022-01-02" "2022-10-10" "2022-04-22"
 [246] "2022-12-20" "2022-11-20" "2022-11-20" "2022-11-12" "2022-10-18" "2022-04-15" "2022-04-27"
 [253] "2022-10-07" "2022-04-25" "2022-11-24" "2022-05-15" "2022-12-20" "2022-12-20" "2022-05-15"
 [260] "2022-12-18" "2022-05-15" "2022-11-18" "2022-05-15" "2022-10-10" "2022-05-10" "2022-12-20"
 [267] "2022-04-26" "2022-11-27" "2022-04-21" "2022-11-28" "2022-12-10" "2022-04-24" "2022-12-18"
 [274] "2022-11-27" "2022-11-22" "2022-05-15" "2022-11-20" "2022-11-20" "2022-05-15" "2022-04-20"
 [281] "2022-11-18" "2022-10-15" "2022-05-07" "2022-04-25" "2022-01-02" "2022-11-25" "2022-04-28"
 [288] "2022-11-14" "2022-04-16" "2022-12-20" "2022-04-16" "2022-07-14" "2022-12-20" "2022-04-08"
 [295] "2022-04-25" "2022-05-05" "2022-01-03" "2022-11-15" "2022-05-07" "2022-11-16" "2022-11-16"
 [302] "2022-12-18" "2022-04-09" "2022-05-09" "2022-05-07" "2022-12-20" "2022-05-15" "2022-05-08"
 [309] "2022-05-04" "2022-12-20" "2022-05-15" "2022-05-09" "2022-05-06" "2022-05-20" "2022-12-15"
 [316] "2022-05-08" "2022-05-09" "2022-12-20" "2022-12-18" "2022-05-02" "2022-01-03" "2022-12-20"
 [323] "2022-12-20" "2022-04-16" "2022-05-06" "2022-05-15" "2022-05-15" "2022-04-16" "2022-04-16"
 [330] "2022-05-10" "2022-12-20" "2022-05-15" "2022-04-15" "2022-05-16" "2022-05-15" "2022-05-15"
 [337] "2022-05-09" "2022-12-20" "2022-12-18" "2023-01-06" "2023-07-28" "2023-01-20" "2023-05-04"
 [344] "2023-05-02" "2023-05-20" "2023-05-20" "2023-07-20" "2023-05-10" "2023-05-20" "2023-05-03"
 [351] "2023-05-02" "2023-05-02" "2023-04-10" "2023-05-05" "2023-05-15" "2023-06-15" "2023-01-03"
 [358] "2023-05-04" "2023-05-02" "2023-01-03" "2023-06-10" "2023-06-12" "2023-07-20" "2023-01-06"
 [365] "2023-06-08" "2023-06-24" "2023-05-23" "2023-07-15" "2023-05-03" "2023-05-18" "2023-05-02"
 [372] "2023-01-04" "2023-05-04" "2023-05-02" "2023-06-08" "2023-01-20" "2021-12-19" "2021-12-09"
 [379] "2021-11-16" "2021-07-20" "2021-05-20" "2021-11-19" "2021-05-20" "2022-07-20" "2022-06-02"
 [386] "2022-11-16" "2022-01-15" "2022-12-20" "2022-05-02" "2022-10-09" "2022-05-29" "2022-07-25"
 [393] "2022-06-15" "2022-06-23" "2022-05-15" "2022-11-10" "2022-05-16" "2022-06-16" "2022-06-10"
 [400] "2022-12-08" "2022-12-25" "2022-06-10" "2022-06-15" "2022-11-10" "2022-12-02" "2022-05-10"
 [407] "2022-06-22" "2023-06-10" "2023-01-10" "2023-07-05" "2023-06-06" "2023-01-01" "2023-06-01"
 [414] "2023-02-01" "2021-05-25" "2021-06-02" "2021-05-27" "2021-05-20" "2021-06-25" "2022-05-08"
 [421] "2022-10-06" "2022-06-04" "2022-10-10" "2022-05-24" "2022-10-10" "2022-04-25" "2022-05-25"
 [428] "2022-10-04" "2022-07-05" "2022-05-15" "2022-05-08" "2022-05-24" "2022-09-05" "2022-12-08"
 [435] "2022-11-20" "2022-05-07" "2022-11-20" "2022-05-17" "2022-11-20" "2022-10-15" "2022-11-12"
 [442] "2022-11-20" "2022-10-10" "2022-05-15" "2022-11-20" "2022-11-20" "2022-05-15" "2022-05-06"
 [449] "2022-11-19" "2022-09-29" "2022-10-10" "2022-04-15" "2022-05-20" "2022-11-20" "2022-05-16"
 [456] "2022-05-09" "2022-06-20" "2022-05-10" "2022-05-15" "2022-11-24" "2022-11-20" "2022-11-20"
 [463] "2022-05-20" "2022-05-08" "2022-11-20" "2022-05-11" "2022-05-20" "2022-11-20" "2022-05-15"
 [470] "2022-10-20" "2022-11-10" "2022-11-03" "2022-11-14" "2022-05-20" "2022-05-20" "2022-10-11"
 [477] "2022-11-20" "2022-05-15" "2022-04-15" "2023-01-01" "2023-05-26" "2023-05-20" "2023-05-20"
 [484] "2023-05-20" "2020-11-10" "2020-11-14" "2020-12-19" "2020-11-06" "2020-11-09" "2020-11-06"
 [491] "2020-12-20" "2020-12-22" "2020-12-24" "2020-12-25" "2020-12-25" "2020-11-15" "2020-12-25"
 [498] "2020-12-25" "2021-11-08" "2021-11-06" "2021-05-10" "2021-06-16" "2021-06-15" "2021-11-06"
 [505] "2021-11-21" "2021-11-13" "2021-06-15" "2021-11-20" "2021-12-28" "2021-11-10" "2021-11-04"
 [512] "2021-11-25" "2021-11-09" "2021-03-22" "2021-03-15" "2021-03-10" "2021-11-20" "2021-03-05"
 [519] "2021-03-15" "2021-03-18" "2021-11-06" "2021-11-10" "2021-03-22" "2021-11-20" "2021-11-14"
 [526] "2021-11-10" "2021-11-10" "2021-06-14" "2021-11-04" "2021-11-15" "2021-11-08" "2021-11-16"
 [533] "2021-12-15" "2021-05-10" "2021-11-06" "2021-11-11" "2021-06-15" "2021-11-25" "2021-11-20"
 [540] "2021-11-10" "2021-11-24" "2021-11-06" "2021-11-18" "2021-11-09" "2021-11-06" "2021-05-10"
 [547] "2021-11-08" "2021-11-14" "2021-03-15" "2021-11-20" "2021-03-10" "2021-11-14" "2021-03-04"
 [554] "2021-07-25" "2021-11-26" "2021-11-24" "2021-04-20" "2021-11-15" "2021-06-25" "2021-03-05"
 [561] "2021-11-08" "2021-03-28" "2021-11-15" "2021-11-06" "2021-04-28" "2021-04-20" "2021-11-12"
 [568] "2021-10-19" "2021-11-10" "2021-11-16" "2021-11-20" "2021-12-06" "2021-03-30" "2021-11-26"
 [575] "2021-06-14" "2021-11-09" "2021-11-24" "2021-11-26" "2021-03-07" "2021-11-05" "2021-11-20"
 [582] "2021-11-19" "2021-11-19" "2021-04-20" "2021-11-10" "2021-04-26" "2021-12-10" "2021-11-16"
 [589] "2021-11-19" "2021-12-10" "2021-03-15" "2021-03-25" "2021-12-06" "2021-11-20" "2021-11-14"
 [596] "2021-11-10" "2021-12-10" "2021-03-25" "2021-04-27" "2021-03-28" "2021-05-18" "2021-05-18"
 [603] "2021-03-19" "2021-11-06" "2021-11-15" "2021-11-13" "2021-11-10" "2021-03-15" "2021-11-14"
 [610] "2022-11-10" "2022-11-20" "2022-11-12" "2022-03-15" "2022-11-10" "2022-03-10" "2022-11-24"
 [617] "2022-03-10" "2022-04-12" "2022-03-20" "2022-11-09" "2022-03-19" "2022-11-09" "2022-02-25"
 [624] "2022-03-24" "2022-11-10" "2022-11-08" "2022-03-13" "2022-03-13" "2022-11-15" "2022-03-04"
 [631] "2022-03-25" "2022-03-25" "2022-03-19" "2022-04-09" "2022-11-20" "2022-11-09" "2022-03-20"
 [638] "2022-03-04" "2022-02-24" "2022-03-25" "2022-11-15" "2022-03-20" "2022-11-29" "2022-11-08"
 [645] "2022-04-15" "2022-10-02" "2022-03-08" "2022-11-08" "2022-03-06" "2022-04-15" "2022-11-19"
 [652] "2022-03-14" "2022-11-25" "2022-10-05" "2022-05-15" "2022-11-10" "2022-11-10" "2022-03-15"
 [659] "2022-03-14" "2022-03-10" "2022-03-06" "2022-11-24" "2022-11-10" "2022-11-08" "2022-11-15"
 [666] "2022-11-20" "2022-03-16" "2022-11-10" "2022-03-10" "2022-11-10" "2022-11-20" "2022-03-06"
 [673] "2022-02-25" "2022-03-15" "2022-11-10" "2022-02-20" "2022-03-16" "2022-11-10" "2022-02-10"
 [680] "2022-03-21" "2022-11-25" "2022-11-08" "2022-02-25" "2022-03-05" "2022-10-24" "2022-11-24"
 [687] "2022-10-12" "2022-02-20" "2022-11-09" "2022-11-15" "2022-11-10" "2022-03-19" "2022-03-20"
 [694] "2022-03-06" "2022-11-10" "2022-03-15" "2022-11-12" "2022-03-20" "2022-03-06" "2022-03-02"
 [701] "2022-02-22" "2022-06-20" "2022-03-15" "2022-03-06" "2022-03-21" "2022-11-25" "2022-03-10"
 [708] "2022-03-15" "2022-03-10" "2022-03-15" "2022-11-10" "2022-11-24" "2022-11-10" "2022-11-20"
 [715] "2022-02-20" "2022-03-06" "2022-03-14" "2022-11-25" "2022-02-25" "2022-11-12" "2022-04-10"
 [722] "2022-03-15" "2022-03-20" "2022-11-15" "2022-10-10" "2022-11-08" "2022-11-10" "2022-11-11"
 [729] "2022-03-05" "2022-03-26" "2022-11-10" "2022-11-15" "2022-03-25" "2022-03-15" "2022-11-09"
 [736] "2022-11-25" "2022-03-06" "2022-03-20" "2022-11-08" "2022-11-11" "2022-03-08" "2022-04-10"
 [743] "2022-02-22" "2022-11-25" "2022-03-15" "2022-03-15" "2022-11-08" "2022-03-20" "2022-11-20"
 [750] "2022-11-28" "2023-04-25" "2023-03-03" "2023-03-10" "2023-03-10" "2023-04-04" "2023-03-10"
 [757] "2023-03-10" "2023-02-20" "2023-03-11" "2023-04-20" "2023-04-04" "2023-02-28" "2023-04-29"
 [764] "2023-02-20" "2023-03-10" "2023-02-26" "2023-03-15" "2023-02-08" "2023-02-06" "2023-03-15"
 [771] "2023-02-24" "2023-03-15" "2023-01-08" "2023-03-12" "2023-02-25" "2023-02-09" "2023-03-15"
 [778] "2023-03-10" "2023-02-08" "2023-03-16" "2020-11-12" "2020-12-10" "2020-11-10" "2021-11-13"
 [785] "2021-11-15" "2021-11-11" "2021-11-25" "2021-12-20" "2021-11-10" "2021-12-10" "2021-11-10"
 [792] "2021-10-10" "2021-12-19" "2021-11-20" "2021-08-01" "2021-04-05" "2021-04-10" "2021-11-09"
 [799] "2021-12-17" "2021-05-12" "2021-11-15" "2021-11-17" "2021-12-27" "2021-11-09" "2021-11-15"
 [806] "2021-03-11" "2021-04-10" "2021-11-25" "2021-03-12" "2021-11-15" "2021-11-10" "2021-11-17"
 [813] "2021-11-10" "2021-11-09" "2021-03-11" "2021-10-20" "2021-11-10" "2021-03-10" "2021-11-10"
 [820] "2021-03-04" "2021-11-10" "2021-03-11" "2021-11-10" "2021-03-11" "2021-11-12" "2021-11-10"
 [827] "2021-11-10" "2021-12-03" "2021-11-15" "2021-12-20" "2021-11-10" "2021-11-10" "2021-05-10"
 [834] "2021-03-11" "2021-11-10" "2021-11-11" "2021-12-10" "2021-11-11" "2021-03-11" "2021-12-01"
 [841] "2022-04-20" "2022-12-02" "2022-11-16" "2022-04-05" "2022-11-20" "2022-04-15" "2022-11-10"
 [848] "2022-04-24" "2022-03-10" "2022-04-15" "2022-11-25" "2022-10-25" "2022-01-29" "2022-03-20"
 [855] "2022-04-13" "2022-11-10" "2022-11-09" "2022-11-10" "2022-04-13" "2022-05-16" "2022-04-02"
 [862] "2022-10-15" "2022-04-02" "2022-05-15" "2022-03-20" "2022-10-10" "2022-03-20" "2022-04-13"
 [869] "2022-11-12" "2022-10-10" "2022-11-15" "2022-10-15" "2022-11-05" "2022-02-10" "2022-11-09"
 [876] "2022-01-10" "2022-11-06" "2022-11-01" "2022-03-24" "2022-12-12" "2022-03-20" "2022-03-05"
 [883] "2022-04-16" "2022-11-20" "2022-12-11" "2022-11-09" "2022-11-02" "2022-11-03" "2022-03-22"
 [890] "2022-11-09" "2022-12-10" "2022-04-18" "2022-03-01" "2022-09-26" "2022-05-10" "2022-03-15"
 [897] "2022-01-25" "2022-03-22" "2022-11-10" "2022-04-10" "2022-12-12" "2022-11-15" "2022-02-10"
 [904] "2022-03-25" "2022-01-25" "2022-01-15" "2022-09-25" "2022-10-10" "2022-03-24" "2022-11-10"
 [911] "2022-11-08" "2022-12-02" "2022-03-23" "2022-03-15" "2022-11-10" "2022-03-15" "2022-12-12"
 [918] "2022-11-11" "2022-05-20" "2022-11-07" "2022-10-18" "2022-12-17" "2022-03-20" "2022-04-02"
 [925] "2022-10-28" "2022-04-17" "2022-04-01" "2022-10-30" "2022-04-15" "2022-10-11" "2022-05-10"
 [932] "2022-02-15" "2022-05-10" "2022-05-19" "2022-02-25" "2022-10-29" "2022-10-09" "2022-11-06"
 [939] "2022-04-15" "2022-03-22" "2022-10-07" "2022-11-11" "2022-10-15" "2022-06-05" "2022-12-10"
 [946] "2022-02-25" "2022-03-20" "2022-11-24" "2022-10-07" "2022-04-05" "2022-11-12" "2022-11-15"
 [953] "2022-12-15" "2022-11-20" "2022-11-10" "2022-03-15" "2022-02-10" "2022-11-14" "2022-11-15"
 [960] "2022-12-20" "2022-07-01" "2022-10-29" "2022-11-10" "2022-10-10" "2022-03-12" "2022-11-10"
 [967] "2022-02-25" "2022-04-15" "2022-01-14" "2022-10-15" "2022-11-09" "2022-04-06" "2022-04-04"
 [974] "2022-12-10" "2022-05-02" "2022-03-05" "2022-11-10" "2022-10-28" "2022-02-20" "2022-03-24"
 [981] "2022-03-14" "2022-03-24" "2022-12-06" "2022-04-09" "2022-03-16" "2022-04-20" "2022-11-10"
 [988] "2022-11-15" "2022-12-28" "2022-10-25" "2022-06-10" "2022-12-21" "2022-08-02" "2022-11-30"
 [995] "2022-11-10" "2022-05-15" "2022-04-03" "2022-02-18" "2022-04-25" "2022-10-29"
 [ reached 'max' / getOption("max.print") -- omitted 2264 entries ]
> 
> table(VH_merged$Pl_date, VH_merged$Pl_date)
            
             2020-11-06 2020-11-09 2020-11-10 2020-11-11 2020-11-12 2020-11-14 2020-11-15 2020-12-01
  2020-11-06          2          0          0          0          0          0          0          0
            
             2020-12-03 2020-12-05 2020-12-09 2020-12-10 2020-12-15 2020-12-16 2020-12-17 2020-12-18
  2020-11-06          0          0          0          0          0          0          0          0
            
             2020-12-19 2020-12-20 2020-12-21 2020-12-22 2020-12-24 2020-12-25 2020-12-30 2021-01-10
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-01-12 2021-01-18 2021-01-19 2021-01-20 2021-01-22 2021-01-24 2021-02-12 2021-02-15
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-02-16 2021-03-01 2021-03-02 2021-03-04 2021-03-05 2021-03-07 2021-03-08 2021-03-09
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-03-10 2021-03-11 2021-03-12 2021-03-13 2021-03-14 2021-03-15 2021-03-16 2021-03-18
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-03-19 2021-03-20 2021-03-22 2021-03-23 2021-03-25 2021-03-26 2021-03-28 2021-03-30
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-04-01 2021-04-02 2021-04-03 2021-04-04 2021-04-05 2021-04-09 2021-04-10 2021-04-12
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-04-14 2021-04-15 2021-04-16 2021-04-18 2021-04-19 2021-04-20 2021-04-22 2021-04-24
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-04-25 2021-04-26 2021-04-27 2021-04-28 2021-05-02 2021-05-03 2021-05-05 2021-05-06
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-05-07 2021-05-08 2021-05-09 2021-05-10 2021-05-11 2021-05-12 2021-05-13 2021-05-14
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-05-15 2021-05-16 2021-05-17 2021-05-18 2021-05-20 2021-05-21 2021-05-22 2021-05-24
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-05-25 2021-05-26 2021-05-27 2021-05-29 2021-05-30 2021-06-01 2021-06-02 2021-06-03
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-06-05 2021-06-06 2021-06-07 2021-06-08 2021-06-10 2021-06-11 2021-06-12 2021-06-14
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-06-15 2021-06-16 2021-06-17 2021-06-18 2021-06-19 2021-06-20 2021-06-21 2021-06-22
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-06-25 2021-06-26 2021-06-29 2021-07-01 2021-07-09 2021-07-15 2021-07-17 2021-07-18
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-07-20 2021-07-22 2021-07-23 2021-07-25 2021-07-26 2021-07-30 2021-08-01 2021-08-10
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-08-14 2021-08-18 2021-08-20 2021-08-22 2021-08-25 2021-08-28 2021-09-08 2021-09-20
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-10-04 2021-10-06 2021-10-07 2021-10-08 2021-10-09 2021-10-10 2021-10-13 2021-10-14
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-10-15 2021-10-16 2021-10-17 2021-10-18 2021-10-19 2021-10-20 2021-10-22 2021-10-25
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-10-26 2021-11-01 2021-11-02 2021-11-03 2021-11-04 2021-11-05 2021-11-06 2021-11-07
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-11-08 2021-11-09 2021-11-10 2021-11-11 2021-11-12 2021-11-13 2021-11-14 2021-11-15
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-11-16 2021-11-17 2021-11-18 2021-11-19 2021-11-20 2021-11-21 2021-11-22 2021-11-23
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-11-24 2021-11-25 2021-11-26 2021-11-28 2021-11-30 2021-12-01 2021-12-02 2021-12-03
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-12-04 2021-12-05 2021-12-06 2021-12-07 2021-12-08 2021-12-09 2021-12-10 2021-12-11
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-12-12 2021-12-13 2021-12-14 2021-12-15 2021-12-16 2021-12-17 2021-12-18 2021-12-19
  2020-11-06          0          0          0          0          0          0          0          0
            
             2021-12-20 2021-12-21 2021-12-22 2021-12-25 2021-12-27 2021-12-28 2021-12-29 2021-12-30
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-01-01 2022-01-02 2022-01-03 2022-01-04 2022-01-05 2022-01-06 2022-01-08 2022-01-09
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-01-10 2022-01-11 2022-01-12 2022-01-13 2022-01-14 2022-01-15 2022-01-16 2022-01-17
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-01-18 2022-01-20 2022-01-21 2022-01-22 2022-01-24 2022-01-25 2022-01-28 2022-01-29
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-02-01 2022-02-04 2022-02-05 2022-02-08 2022-02-09 2022-02-10 2022-02-11 2022-02-12
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-02-14 2022-02-15 2022-02-18 2022-02-20 2022-02-22 2022-02-24 2022-02-25 2022-02-26
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-02-28 2022-03-01 2022-03-02 2022-03-03 2022-03-04 2022-03-05 2022-03-06 2022-03-07
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-03-08 2022-03-09 2022-03-10 2022-03-12 2022-03-13 2022-03-14 2022-03-15 2022-03-16
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-03-18 2022-03-19 2022-03-20 2022-03-21 2022-03-22 2022-03-23 2022-03-24 2022-03-25
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-03-26 2022-03-27 2022-03-28 2022-04-01 2022-04-02 2022-04-03 2022-04-04 2022-04-05
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-04-06 2022-04-07 2022-04-08 2022-04-09 2022-04-10 2022-04-11 2022-04-12 2022-04-13
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-04-14 2022-04-15 2022-04-16 2022-04-17 2022-04-18 2022-04-19 2022-04-20 2022-04-21
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-04-22 2022-04-23 2022-04-24 2022-04-25 2022-04-26 2022-04-27 2022-04-28 2022-04-29
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-04-30 2022-05-01 2022-05-02 2022-05-03 2022-05-04 2022-05-05 2022-05-06 2022-05-07
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-05-08 2022-05-09 2022-05-10 2022-05-11 2022-05-12 2022-05-13 2022-05-14 2022-05-15
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-05-16 2022-05-17 2022-05-18 2022-05-19 2022-05-20 2022-05-21 2022-05-22 2022-05-23
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-05-24 2022-05-25 2022-05-26 2022-05-28 2022-05-29 2022-06-01 2022-06-02 2022-06-03
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-06-04 2022-06-05 2022-06-06 2022-06-07 2022-06-08 2022-06-09 2022-06-10 2022-06-12
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-06-13 2022-06-14 2022-06-15 2022-06-16 2022-06-17 2022-06-18 2022-06-19 2022-06-20
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-06-21 2022-06-22 2022-06-23 2022-06-24 2022-06-25 2022-06-26 2022-06-29 2022-07-01
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-07-02 2022-07-03 2022-07-05 2022-07-07 2022-07-08 2022-07-09 2022-07-10 2022-07-12
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-07-13 2022-07-14 2022-07-15 2022-07-19 2022-07-20 2022-07-25 2022-07-28 2022-08-01
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-08-02 2022-08-04 2022-08-05 2022-08-13 2022-08-15 2022-08-20 2022-09-01 2022-09-02
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-09-03 2022-09-04 2022-09-05 2022-09-06 2022-09-09 2022-09-12 2022-09-14 2022-09-15
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-09-16 2022-09-20 2022-09-23 2022-09-25 2022-09-26 2022-09-28 2022-09-29 2022-10-01
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-10-02 2022-10-03 2022-10-04 2022-10-05 2022-10-06 2022-10-07 2022-10-08 2022-10-09
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-10-10 2022-10-11 2022-10-12 2022-10-13 2022-10-14 2022-10-15 2022-10-16 2022-10-17
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-10-18 2022-10-19 2022-10-20 2022-10-21 2022-10-24 2022-10-25 2022-10-28 2022-10-29
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-10-30 2022-11-01 2022-11-02 2022-11-03 2022-11-04 2022-11-05 2022-11-06 2022-11-07
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-11-08 2022-11-09 2022-11-10 2022-11-11 2022-11-12 2022-11-13 2022-11-14 2022-11-15
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-11-16 2022-11-17 2022-11-18 2022-11-19 2022-11-20 2022-11-21 2022-11-22 2022-11-23
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-11-24 2022-11-25 2022-11-26 2022-11-27 2022-11-28 2022-11-29 2022-11-30 2022-12-01
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-12-02 2022-12-03 2022-12-04 2022-12-05 2022-12-06 2022-12-07 2022-12-08 2022-12-09
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-12-10 2022-12-11 2022-12-12 2022-12-13 2022-12-14 2022-12-15 2022-12-16 2022-12-17
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-12-18 2022-12-20 2022-12-21 2022-12-22 2022-12-24 2022-12-25 2022-12-27 2022-12-28
  2020-11-06          0          0          0          0          0          0          0          0
            
             2022-12-30 2023-01-01 2023-01-02 2023-01-03 2023-01-04 2023-01-05 2023-01-06 2023-01-07
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-01-08 2023-01-10 2023-01-11 2023-01-12 2023-01-13 2023-01-14 2023-01-15 2023-01-16
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-01-17 2023-01-20 2023-01-24 2023-01-25 2023-01-26 2023-01-28 2023-02-01 2023-02-02
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-02-03 2023-02-04 2023-02-06 2023-02-07 2023-02-08 2023-02-09 2023-02-10 2023-02-13
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-02-14 2023-02-18 2023-02-20 2023-02-22 2023-02-24 2023-02-25 2023-02-26 2023-02-27
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-02-28 2023-03-03 2023-03-04 2023-03-05 2023-03-07 2023-03-09 2023-03-10 2023-03-11
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-03-12 2023-03-14 2023-03-15 2023-03-16 2023-03-17 2023-03-18 2023-03-19 2023-03-20
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-03-21 2023-03-22 2023-03-23 2023-03-24 2023-03-25 2023-03-26 2023-03-27 2023-03-28
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-04-01 2023-04-02 2023-04-03 2023-04-04 2023-04-05 2023-04-06 2023-04-07 2023-04-08
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-04-09 2023-04-10 2023-04-11 2023-04-12 2023-04-13 2023-04-14 2023-04-15 2023-04-16
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-04-17 2023-04-18 2023-04-19 2023-04-20 2023-04-22 2023-04-23 2023-04-24 2023-04-25
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-04-26 2023-04-27 2023-04-28 2023-04-29 2023-04-30 2023-05-01 2023-05-02 2023-05-03
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-05-04 2023-05-05 2023-05-06 2023-05-07 2023-05-08 2023-05-09 2023-05-10 2023-05-12
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-05-13 2023-05-14 2023-05-15 2023-05-16 2023-05-18 2023-05-19 2023-05-20 2023-05-23
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-05-25 2023-05-26 2023-05-29 2023-05-30 2023-06-01 2023-06-04 2023-06-05 2023-06-06
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-06-07 2023-06-08 2023-06-09 2023-06-10 2023-06-12 2023-06-13 2023-06-15 2023-06-17
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-06-20 2023-06-24 2023-06-25 2023-06-28 2023-06-29 2023-06-30 2023-07-02 2023-07-05
  2020-11-06          0          0          0          0          0          0          0          0
            
             2023-07-10 2023-07-13 2023-07-15 2023-07-16 2023-07-20 2023-07-28 2023-09-10 2023-11-03
  2020-11-06          0          0          0          0          0          0          0          0
 [ reached getOption("max.print") -- omitted 631 rows ]
> 
> 
> 
> # SUmmer-Automn
> VH_merged$CS_reach_SA <- ifelse (VH_merged$M4B11_MA == 3 & VH_merged$Pl_date_fortnight == VH_merged$Planting_SA_E_fortnight, TRUE, FALSE) # 230 hhs
> 
> 
> table (VH_merged$M4B11_MA, VH_merged$CS_reach_SA) # 69 hhs
   
    FALSE TRUE
  2  1633    0
  3  1297  230
> 
> # Winter-SPring
> VH_merged$CS_reach_WS <- ifelse (VH_merged$M4B11_MA == 2 & VH_merged$Pl_date_fortnight == VH_merged$Planting_WS_E_fortnight, TRUE, FALSE) # 234 hh
> 
> 
> table (VH_merged$M4B11_MA, VH_merged$CS_reach_WS) # 5 hhs
   
    FALSE TRUE
  2  1271  234
  3  1631    0
> 
> 
> VH_merged$CSMAP_reach <- ifelse (VH_merged$CS_reach_WS == TRUE | VH_merged$CS_reach_SA == TRUE, 1, 0)
> 
> table (VH_merged$CSMAP_reach)

   0    1 
2568  464 
> 
> CS <- VH_merged %>%
+   rename(
+     MAHUYEN = MAHUYEN.y,
+     MATINH = MATINH.y,
+     MAXA = MAXA,
+     MADIABAN = MADIABAN,
+     HOSO = HOSO,
+     panel = Year
+   )
> 
> write.csv (CS [, c(17:21,31,41,53:55)], 
+            'Output/CSMAPs.vars.22.23.csv') 
> 
> table (VH_merged$CSMAP_reach [VH_merged$Year == 2023])

   0    1 
1697  307 
> 
> names(VH_merged)
 [1] "Province_name"           "Pl_date_year"            "ID"                     
 [4] "ProDis"                  "avg_Planting_SA_E"       "mode_Planting_SA_E"     
 [7] "avg_Planting_SA_N"       "mode_Planting_SA_N"      "avg_Planting_WS_E"      
[10] "mode_Planting_WS_E"      "avg_Planting_WS_N"       "mode_Planting_WS_N"     
[13] "MATINH.x"                "District_name"           "MAHUYEN.x"              
[16] "Dates_VH"                "IDHO"                    "MATINH.y"               
[19] "MAHUYEN.y"               "MAXA"                    "MADIABAN"               
[22] "HOSO"                    "KYDIEUTRA"               "M4B11_MA"               
[25] "M4B11_C3"                "M4B11_C3AD"              "M4B11_C3AM"             
[28] "M4B11_C3AY"              "M4B11_C3B.x"             "M4B11_C4"               
[31] "M4B11_C5"                "wt45"                    "Year"                   
[34] "result"                  "sowing_day"              "sowing_month"           
[37] "sowing_year"             "M4B11_C3B.y"             "gre_day"                
[40] "gre_month"               "gre_year"                "Date_converted"         
[43] "Pl_date"                 "Pl_date_numeric"         "Pl_date_fortnight"      
[46] "Planting_WS_E"           "Planting_SA_E"           "Planting_WS_N"          
[49] "Planting_SA_N"           "Planting_SA_E_fortnight" "Planting_WS_E_fortnight"
[52] "Planting_SA_N_fortnight" "Planting_WS_N_fortnight" "Ave_High_mention"       
[55] "Ave_High_mention_2021"   "Ave_High_mention_2022"   "Ave_High_mention_2023"  
[58] "WS_Advice_received"      "SA_Advice_received"      "Year.2022"              
[61] "Year.2023"               "SA_Ext_year"             "WS_Ext_year"            
[64] "EA_ID"                   "CS_reach_SA"             "CS_reach_WS"            
[67] "CSMAP_reach"            
> 
> Check <- VH_merged [VH_merged$CS_reach_SA == TRUE | VH_merged$CS_reach_WS == TRUE ,]
> Check <- Check [!is.na (Check$ID) ,]
> Check <- Check [!duplicated(Check$IDHO) ,] # 69 hhs are concerned
> # 417 hhs-cropping seasons. ~50 have 2 cropping seasons
> 
> 
> 
> # Add the weights variable
> curl_function ("data/processed/VH22_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_data.csv'
Content type 'text/plain; charset=utf-8' length 2257649 bytes (2.2 MB)
downloaded 2.2 MB

> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> df_22 <- read.csv("data/processed/VH22_data.csv")
> df_23 <- read.csv("data/processed/VH23_data.csv")
> 
> df_22 <- df_22 %>%
+   select (c("MATINH":"HOSO", IDHO, csmap_final, weight_final_rice))
> 
> df_23 <- df_23 %>%
+   select (c("MATINH":"HOSO", IDHO, csmap_final, weight_final_rice))
> 
> Merge.22 <- merge (Check [Check$Year == 2022 ,], df_22 , by='IDHO', all.x=TRUE)
> Merge.23 <- merge (Check [Check$Year == 2023 ,], df_23, by='IDHO', all.x=TRUE)
> 
> table (Merge.23$CSMAP_reach)

  1 
261 
> 
> Check <- rbind (Merge.22, Merge.23)
> 
> Check$CSMAP_reach <- ifelse (Check$CS_reach_WS == TRUE | Check$CS_reach_SA == TRUE, TRUE, FALSE)
> table (Check$CSMAP_reach) # 69 hhs are there

TRUE 
 406 
> 
> weighted_table <- tapply(Check$weight_final_rice, Check$CSMAP_reach, sum, na.rm = TRUE) 
> weighted_table_23 <- tapply(Check$weight_final_rice [Check$Year == 2023], Check$CSMAP_reach [Check$Year == 2023], sum, na.rm = TRUE) # 
> table (Check$Province_name)

  An Giang   Bạc Liêu    Bến Tre     Cà Mau    Cần Thơ  Đồng Tháp  Hậu Giang Kiên Giang    Long An 
         4         17          4         14         65         29         28         62         28 
 Sóc Trăng Tiền Giang   Trà Vinh  Vĩnh Long 
        36         31         40         48 
> 
> 
> Merge.23$CSMAP_reach <- ifelse (Merge.23$CS_reach_WS == TRUE | Merge.23$CS_reach_SA == TRUE, TRUE, FALSE)
> weighted_table <- tapply(Merge.23$weight_final_rice, Merge.23$CSMAP_reach, sum, na.rm = TRUE) # 27,199 hhs
> 
> 
> 
> # Export the CS-MAP hhs, from 2022 and 2023, two cropping seasons
> write.csv (Check, 'Output/CSMAPs.vars.22.23.csv') 
> 
> 
> setdiff(names(Merge.22), names(Merge.23))
character(0)
> 
> # Columns in Merge.23 but not in Merge.22
> setdiff(names(Merge.23), names(Merge.22))
character(0)
> 
> 
> 
> 
> names(df_22)
[1] "MATINH"            "IDHO"              "MAHUYEN"           "MAXA"              "MADIABAN"         
[6] "HOSO"              "csmap_final"       "weight_final_rice"
> combined_table
Error: object 'combined_table' not found
>  ("data/raw/VHLSS_2024_Commune/Q1/SPIA_ThongTinXa.dta")
[1] "data/raw/VHLSS_2024_Commune/Q1/SPIA_ThongTinXa.dta"
> curl_function ("data/raw/VHLSS_2024_Commune/Q2/SPIA_ThongTinXa.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2024_Commune/Q2/SPIA_ThongTinXa.dta'
Content type 'application/octet-stream' length 150794 bytes (147 KB)
downloaded 147 KB

> curl_function ("data/raw/VHLSS_2024_Commune/Q3/SPIA_ThongTinXa.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2024_Commune/Q3/SPIA_ThongTinXa.dta'
Content type 'application/octet-stream' length 133667 bytes (130 KB)
downloaded 130 KB

> curl_function ("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv'
Content type 'text/plain; charset=utf-8' length 1799 bytes
downloaded 1799 bytes

> Q1 <- read_dta ("data/raw/VHLSS_2024_Commune/Q1/SPIA_ThongTinXa.dta")
Error: 'data/raw/VHLSS_2024_Commune/Q1/SPIA_ThongTinXa.dta' does not exist in current working directory ('C:/Users/FKosmowski/Documents').
> curl_function ("data/raw/VHLSS_2024_Commune/Q1/SPIA_ThongTinXa.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2024_Commune/Q1/SPIA_ThongTinXa.dta'
Content type 'application/octet-stream' length 155411 bytes (151 KB)
downloaded 151 KB

> curl_function ("data/raw/VHLSS_2024_Commune/Q2/SPIA_ThongTinXa.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2024_Commune/Q2/SPIA_ThongTinXa.dta'
Content type 'application/octet-stream' length 150794 bytes (147 KB)
downloaded 147 KB

> curl_function ("data/raw/VHLSS_2024_Commune/Q3/SPIA_ThongTinXa.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2024_Commune/Q3/SPIA_ThongTinXa.dta'
Content type 'application/octet-stream' length 133667 bytes (130 KB)
downloaded 130 KB

> curl_function ("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv'
Content type 'text/plain; charset=utf-8' length 1799 bytes
downloaded 1799 bytes

> Q1 <- read_dta ("data/raw/VHLSS_2024_Commune/Q1/SPIA_ThongTinXa.dta")
> Q2 <- read_dta ("data/raw/VHLSS_2024_Commune/Q2/SPIA_ThongTinXa.dta")
> Q3 <- read_dta ("data/raw/VHLSS_2024_Commune/Q3/SPIA_ThongTinXa.dta")
> df_24 <- rbind (Q1, Q2, Q3)
> 
> 
> Provinces_IDs <- read.csv ("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv")
> Provinces_IDs$MATINH <- as.character(Provinces_IDs$MATINH)
> 
> df_24 <- df_24 %>%
+   left_join (Provinces_IDs)
Joining with `by = join_by(MATINH)`
> 
> # Do farmers in this commune receive Agro-Climatic Bulletins that deliver recommendations based on weather forecasts ? 
> df_24$Bulletins <- ifelse (df_24$M42_C13 == 1, TRUE, 
+                            ifelse (df_24$M42_C13 %in% c(2,3), FALSE, df_24$M42_C13))
> 
> table (df_24$Bulletins) # 144/347 (41%)

  0   1 
144 203 
> 
> # Agro-Climatic Bulletins were in 7 MRD provinces in 2022. # All but Dong Thap, Ben Tre, Vinh Lang, Bac Lieu and Ca Mau
> table (df_24$Province_name [df_24$Region == '6_MRD'], df_24$Bulletins [df_24$Region == '6_MRD']) 
                   
                     0  1
  Thanh pho Can Tho  6  3
  Tinh An Giang      6 25
  Tinh Bac Lieu      5 16
  Tinh Ben Tre      29  5
  Tinh Ca Mau       16 10
  Tinh Dong Thap     9 24
  Tinh Hau Giang     2 18
  Tinh Kien Giang   12 17
  Tinh Long An      13 19
  Tinh Soc Trang     4 21
  Tinh Tien Giang   16 20
  Tinh Tra Vinh     16 10
  Tinh Vinh Long    10 15
> table (df_24$Province_name [df_24$Region == '6_MRD'], df_24$M42_C141 [df_24$Region == '6_MRD']) # Seasonal
                   
                     1  2
  Thanh pho Can Tho  3  0
  Tinh An Giang     17  8
  Tinh Bac Lieu     12  4
  Tinh Ben Tre       4  1
  Tinh Ca Mau       10  0
  Tinh Dong Thap    15  9
  Tinh Hau Giang    16  2
  Tinh Kien Giang   10  7
  Tinh Long An      15  4
  Tinh Soc Trang    20  1
  Tinh Tien Giang   16  4
  Tinh Tra Vinh      5  5
  Tinh Vinh Long    12  3
> table (df_24$Province_name [df_24$Region == '6_MRD'], df_24$M42_C142 [df_24$Region == '6_MRD']) # Monthly
                   
                     1  2
  Thanh pho Can Tho  0  3
  Tinh An Giang      5 20
  Tinh Bac Lieu      2 14
  Tinh Ben Tre       0  5
  Tinh Ca Mau        1  9
  Tinh Dong Thap    12 12
  Tinh Hau Giang     3 15
  Tinh Kien Giang    0 17
  Tinh Long An       5 14
  Tinh Soc Trang     5 16
  Tinh Tien Giang    4 16
  Tinh Tra Vinh      3  7
  Tinh Vinh Long     1 14
> table (df_24$Province_name [df_24$Region == '6_MRD'], df_24$M42_C143 [df_24$Region == '6_MRD']) # Every 10 days
                   
                     1  2
  Thanh pho Can Tho  0  3
  Tinh An Giang      8 17
  Tinh Bac Lieu      1 15
  Tinh Ben Tre       0  5
  Tinh Ca Mau        0 10
  Tinh Dong Thap     2 22
  Tinh Hau Giang     2 16
  Tinh Kien Giang    6 11
  Tinh Long An       3 16
  Tinh Soc Trang     2 19
  Tinh Tien Giang    2 18
  Tinh Tra Vinh      2  8
  Tinh Vinh Long     1 14
> 
> df_summary <- dfSummary(df_24 [, 22:32])
> view(df_summary, file = "Output/Bulletins_summary.html")
Output file written: C:\Users\FKosmowski\Documents\Output\Bulletins_summary.html
> 
> # Farmers mostly receive seasonal bulletins (74%), followed monthly (20%) and 10-days (14.6%) 
> # 98% of bulletins about rice
> 
> 
> # Table 
> # Subset data for Region == '6_MRD'
> subset_MRD <- df_24[df_24$Region == '6_MRD', ]
> 
> # Create tables for each time period
> seasonal_table <- table(subset_MRD$Province_name, subset_MRD$M42_C141)
> monthly_table <- table(subset_MRD$Province_name, subset_MRD$M42_C142)
> ten_days_table <- table(subset_MRD$Province_name, subset_MRD$M42_C143)
> 
> # Calculate total respondents per province
> total_respondents <- table(subset_MRD$Province_name)
> 
> # Calculate the count of Bulletins == TRUE per province
> bulletins_true <- table(subset_MRD$Province_name[subset_MRD$Bulletins == TRUE])
> 
> # Combine all tables into one data frame
> combined_table <- data.frame(
+   Province = rownames(total_respondents),  # Province names
+   Total.EAs = as.vector(total_respondents),  # Total respondents
+   Receive.Bulletins = as.vector(bulletins_true[rownames(total_respondents)]),  # TRUE Bulletins per province
+   Type.Seasonal = as.vector(seasonal_table),  # Seasonal values
+   Type.Monthly = as.vector(monthly_table),    # Monthly values
+   Type.10_days = as.vector(ten_days_table)  # Every 10 days values
+ )
> 
> # Replace NA values with 0 (in case some provinces don't have Bulletins == TRUE)
> combined_table$Receive.Bulletins[is.na(combined_table$Receive.Bulletins)] <- 0
> combined_table <- combined_table [1:13 ,]
> 
> write.csv (combined_table, "Output/Tab_17_VH24.csv")
> combined_table
            Province Total.EAs Receive.Bulletins Type.Seasonal Type.Monthly Type.10_days
1  Thanh pho Can Tho         9                 3             3            0            0
2      Tinh An Giang        31                25            17            5            8
3      Tinh Bac Lieu        21                16            12            2            1
4       Tinh Ben Tre        34                 5             4            0            0
5        Tinh Ca Mau        26                10            10            1            0
6     Tinh Dong Thap        33                24            15           12            2
7     Tinh Hau Giang        20                18            16            3            2
8    Tinh Kien Giang        29                17            10            0            6
9       Tinh Long An        32                19            15            5            3
10    Tinh Soc Trang        25                21            20            5            2
11   Tinh Tien Giang        36                20            16            4            2
12     Tinh Tra Vinh        26                10             5            3            2
13    Tinh Vinh Long        25                15            12            1            1
> rm (list = ls ())
> library (this.path)
Warning message:
package ‘this.path’ was built under R version 4.3.2 
> library (tidyverse)
> library (ggplot2)
> library (gridExtra)
> library (fastDummies)
Thank you for using fastDummies!
To acknowledge our work, please cite the package:
Kaplan, J. & Schlegel, B. (2023). fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables. Version 1.7.1. URL: https://github.com/jacobkap/fastDummies, https://jacobkap.github.io/fastDummies/.
Warning message:
package ‘fastDummies’ was built under R version 4.3.2 
> library (ggpolypath)
Warning message:
package ‘ggpolypath’ was built under R version 4.3.3 
> library (eulerr)
Warning message:
package ‘eulerr’ was built under R version 4.3.3 
> library (readxl)
> library (stringr)
> library (flextable)

Attaching package: ‘flextable’

The following object is masked from ‘package:purrr’:

    compose

The following object is masked from ‘package:jtools’:

    theme_apa

Warning message:
package ‘flextable’ was built under R version 4.3.2 
> library (sf)
Linking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE
Warning message:
package ‘sf’ was built under R version 4.3.2 
> library (curl)
Using libcurl 8.3.0 with Schannel

Attaching package: ‘curl’

The following object is masked from ‘package:readr’:

    parse_date

The following object is masked from ‘package:httr’:

    handle_reset

Warning message:
package ‘curl’ was built under R version 4.3.2 
> library (survey)
Loading required package: grid
Loading required package: Matrix

Attaching package: ‘Matrix’

The following objects are masked from ‘package:tidyr’:

    expand, pack, unpack

Loading required package: survival

Attaching package: ‘survey’

The following object is masked from ‘package:graphics’:

    dotchart

Warning messages:
1: package ‘survey’ was built under R version 4.3.3 
2: package ‘Matrix’ was built under R version 4.3.2 
> library (haven)
> library (httr)
> 
> #Set working directory to the location of this file
> setwd(this.path::here())   
> 
> #Customized function to format ID
> 
> format_ID <- function(df, columns, widths, pad_char = "0") {
+   # Loop through each column and its corresponding width
+   for (i in seq_along(columns)) {
+     column <- columns[i]
+     width <- widths[i]
+     
+     # Pad the column with the specified width and character
+     df[[column]] <- str_pad(df[[column]], width = width, pad = pad_char)
+   }
+   return(df)
+ }
> 
> 
> 
> # Function to curl data from GitHub----
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> # Prepare layer maps ----
> # Load provincial data: 
> province <- read.csv(curl("https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Provinces_IDs.csv")) %>%
+   select (-c("CSMAP"))#load provincial data
> 
> province$Province_name <- str_remove(province$Province_name, "Tinh ")  #remove "Province" word
> province$Province_name <- str_remove(province$Province_name, "Thanh pho ")  #remove "City" word
> province$Province_name[province$Province_name=="Ba Ria Vung Tau"] <- "Ba Ria - Vung Tau" #rewording
> 
> province$MATINH <- str_pad(province$MATINH, width = 2, pad = 0) # format ID of provincial data
> 
> 
> map <- st_read("/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp")
Reading layer `Province_with_Islands' from data source 
  `/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 65 features and 18 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 102.144 ymin: 7.180931 xmax: 117.8355 ymax: 23.39221
Geodetic CRS:  WGS 84
> 
> map$MATINH <- 0
> map$Region <- ""
> t <- rep(NA,65)
> for (i in 1:length(map$ADM1_EN)) {
+   t[[i]] <- ifelse(grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         province$Province_name) %>% length()==0,0,
+                    grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         province$Province_name))
+   map$MATINH[[i]] <- ifelse(t[[i]]==0,NA,province$MATINH[[t[[i]]]])
+   map$Region[[i]] <- ifelse(t[[i]]==0,NA,province$Region[[t[[i]]]])}
> map %>% head() %>% print(width = 120) %>% colnames()
Simple feature collection with 6 features and 20 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 104.7781 ymin: 7.180931 xmax: 117.8355 ymax: 22.7396
Geodetic CRS:  WGS 84
  Shape_Leng Shape_Area           ADM1_EN                 ADM1_VI ADM1_PCODE ADM1_REF ADM1ALT1EN ADM1ALT2EN ADM1ALT1VI
1   8.860833  0.1361665   Paracel Islands                Hoang Sa     VN2001     <NA>       <NA>       <NA>       <NA>
2  32.976015  0.7353349   Spratly Islands               Truong Sa     VN2002     <NA>       <NA>       <NA>       <NA>
3   2.900742  0.2920399          An Giang                An Giang      VN805     <NA>       <NA>       <NA>       <NA>
4   3.419187  0.1630328 Ba Ria - Vung Tau B\xe0 R?a - V?ng T\xe0u      VN717     <NA>       <NA>       <NA>       <NA>
5   4.514786  0.3387481         Bac Giang               B?c Giang      VN221     <NA>       <NA>       <NA>       <NA>
6   4.207590  0.4255983           Bac Kan                 B?c K?n      VN207     <NA>       <NA>       <NA>       <NA>
  ADM1ALT2VI  ADM0_EN  ADM0_VI ADM0_PCODE       date    validOn validTo   layer
1       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
2       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
3       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
4       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
5       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
6       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
                                                                                                                                                                                                                                                            path
1 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
2 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
3 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
4 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
5 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
6 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
                        geometry MATINH Region
1 MULTIPOLYGON (((112.212 15....   <NA>   <NA>
2 MULTIPOLYGON (((114.8438 7....   <NA>   <NA>
3 MULTIPOLYGON (((105.1172 10...     89  6_MRD
4 MULTIPOLYGON (((106.5574 8....   <NA>   <NA>
5 MULTIPOLYGON (((106.163 21....     24 2_NMMA
6 MULTIPOLYGON (((105.7415 22...     06 2_NMMA
 [1] "Shape_Leng" "Shape_Area" "ADM1_EN"    "ADM1_VI"    "ADM1_PCODE" "ADM1_REF"   "ADM1ALT1EN"
 [8] "ADM1ALT2EN" "ADM1ALT1VI" "ADM1ALT2VI" "ADM0_EN"    "ADM0_VI"    "ADM0_PCODE" "date"      
[15] "validOn"    "validTo"    "layer"      "path"       "geometry"   "MATINH"     "Region"    
> TS <- map %>% filter(ADM1_VI=="Truong Sa") %>% st_geometry()
> cnTS = st_centroid(TS)
> TS_m = (TS-cnTS) * .25 + cnTS + c(-5,0)
> HS <- map %>% filter(ADM1_VI=="Hoang Sa") %>% st_geometry()
> cnHS = st_centroid(HS)
> HS_m =  (HS-cnHS) *.25 + cnHS + c(-2.5,0)
> modified_map <- map %>% filter(!(ADM1_VI %in% c("Truong Sa","Hoang Sa")))
> crs <- st_crs(modified_map)
> TS_map <- map %>% filter(ADM1_VI=="Truong Sa") %>% st_set_geometry(TS_m) %>% st_set_crs(crs)
> HS_map <- map %>% filter(ADM1_VI=="Hoang Sa") %>% st_set_geometry(HS_m) %>% st_set_crs(crs)
> modified_map <- rbind(modified_map,TS_map,HS_map)
> rm(TS,TS_m,TS_map,HS,HS_m,HS_map,cnHS,cnTS,crs, map, i, t)
> 
> 
> 
> 
> 
> # Prepare data----
> curl_function ("data/raw/VHLSS_2024_Commune/SPIA_ThongTinXa_Q1-3.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2024_Commune/SPIA_ThongTinXa_Q1-3.csv'
Content type 'text/plain; charset=utf-8' length 266833 bytes (260 KB)
downloaded 260 KB

> pfes <- read.csv ("data/raw/VHLSS_2024_Commune/SPIA_ThongTinXa_Q1-3.csv") %>%
+   select (c(MATINH, MAHUYEN, MAXA, M43_C1, M43_C2))%>%
+   mutate (pfes_dummy = case_when (M43_C1 == 1 ~ 1,
+                                   TRUE ~ 0)) %>%
+   select (-M43_C1) %>%
+   mutate (panel = 2024)
> pfes <- format_ID(pfes, columns = c("MATINH", "MAHUYEN", "MAXA"), widths = c(2,3,5))
> 
> 
> curl_function ("data/raw/Weight/Census_household_communelevel_clean.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/Census_household_communelevel_clean.csv'
Content type 'text/plain; charset=utf-8' length 665472 bytes (649 KB)
downloaded 649 KB

> n_hh_pop <- read.csv ("data/raw/Weight/Census_household_communelevel_clean.csv") %>%
+   select (c(MATINH, MAXA, n_hh)) %>%
+   rename (n_hh_pop = n_hh) 
> #merge by Commune ID (MAXA) because of some administrative change 
> # (486 missing if merge by prov, dist, comm ID --> 470 missing if merge by prov and comm ID)
> 
> n_hh_pop <- format_ID(n_hh_pop, columns = c("MATINH", "MAXA"), widths = c(2, 5))
> 
> pfes_joined <- pfes %>%
+   left_join (n_hh_pop)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> # Estimate of the reach of PFES ----
> reach_pfes <- pfes_joined %>%
+   filter (!is.na(pfes_dummy)) %>%
+   group_by (panel, pfes_dummy) %>%
+   summarise (twt = sum (n_hh_pop, na.rm = TRUE)) %>%
+   group_by (panel) %>%
+   mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+   filter (pfes_dummy == 1) %>%
+   select (-c(sum_twt))
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
> reach_pfes
# A tibble: 1 × 3
# Groups:   panel [1]
  panel pfes_dummy    twt
  <dbl>      <dbl>  <int>
1  2024          1 227331
> # Figure 25a. Percentage of communes per province with operational Payments for Forest Environmental Services areas ----
> pfes_prep <- pfes_joined %>%
+   filter (!is.na(pfes_dummy)) %>%
+   group_by (pfes_dummy, MATINH) %>%
+   summarise (twt = sum (n_hh_pop, na.rm = TRUE)) %>%
+   group_by (MATINH) %>%
+   mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+   mutate (pct = twt * 100 /sum_twt) %>%
+   filter (pfes_dummy == 1) %>%
+   select (-c(pfes_dummy, twt, sum_twt))
`summarise()` has grouped output by 'pfes_dummy'. You can override using the `.groups` argument.
> 
> pfes_prep <- left_join (pfes_prep, province)
Joining with `by = join_by(MATINH)`
> 
> 
> # Maps ----
> 
> 
> 
> modified_map <- modified_map %>% 
+   left_join(pfes_prep,by="MATINH")
> 
> pfes_map <- modified_map %>%
+   ggplot() + 
+   aes(fill = pct) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> 
> ggsave (pfes_map, filename = "Output/Figure 24a.png",
+         width = 6, height = 10, dpi = 1024)
> 
> 
> 
> # Figure 25b. Percentage of communes per province and total forest area under PFES in 2023 ----
> pfes_area <- pfes %>%
+   filter (!is.na(M43_C2)) %>%
+   group_by (MATINH) %>%
+   summarise (sum_area = sum(M43_C2, na.rm = TRUE)) 
> 
> 
> modified_map <- modified_map %>% 
+   left_join(pfes_area,by="MATINH")
> 
> pfes_map_area <- modified_map %>%
+   ggplot() + 
+   aes(fill = sum_area) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In hectare") +  
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> pfes_map_area
> 
> ggsave (pfes_map_area, filename = "Figure 24b.png",
+         width = 6, height = 10, dpi = 1024)
> rm(list=ls()) ## Clean work space
> library(sf)
> library(ggplot2)
> library(ggspatial)
Error in library(ggspatial) : there is no package called ‘ggspatial’
> library(ggspatial)

Attaching package: ‘ggspatial’

The following object is masked from ‘package:ggpolypath’:

    geom_polypath

Warning message:
package ‘ggspatial’ was built under R version 4.3.3 
> # Function to format ID values
> 
> format_ID <- function(df, columns, widths, pad_char = "0") {
+   # Loop through each column and its corresponding width
+   for (i in seq_along(columns)) {
+     column <- columns[i]
+     width <- widths[i]
+     
+     # Pad the column with the specified width and character
+     df[[column]] <- str_pad(df[[column]], width = width, pad = pad_char)
+   }
+   return(df)
+ }
> 
> 
> # Define columns and widths
> columns <- c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO")
> widths <- c(2, 3, 5, 3, 3)
> 
> 
> 
> 
> # Function to download datasets from GitHub----
> 
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> # Load and merge data----
> ## VH22
> curl_function ("data/processed/VH22_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_data.csv'
Content type 'text/plain; charset=utf-8' length 2257649 bytes (2.2 MB)
downloaded 2.2 MB

> vh22 <- read.csv ("data/processed/VH22_data.csv")
> vh22 <- format_ID (vh22, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), 
+                    widths = c(2, 3, 5, 3, 3))
> vh22$IDHO <- paste0 (vh22$MAXA, vh22$MADIABAN, vh22$HOSO)
> 
> 
> ## VH23
> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> vh23 <- read.csv ("data/processed/VH23_data.csv") %>%
+   select (-c (starts_with("straw"), Genotype_rec,
+               Genotype, CMD, DMC, CIAT.related, starts_with("Strain"), SWCP, SWCP_confirmed, weight_cass, 
+               weight_gift, weight_coffee)) %>%
+   distinct() 
> vh23 <- format_ID (vh23, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), 
+                    widths = c(2, 3, 5, 3, 3))
> vh23$IDHO <- paste0 (vh23$MAXA, vh23$MADIABAN, vh23$HOSO)
> 
> vh <- full_join (vh22, vh23) %>%
+   filter (IDHO != "NANANA") #merge them
Joining with `by = join_by(MATINH, IDHO, MAHUYEN, MAXA, MADIABAN, HOSO, mech_combine_harvester,
mech_straw_baler, mech_seed_blower, panel, mean_csmap, csmap_final, weight_final_rice, THUNHAP,
TONGCHITIEU, ethnic, rls_head, age, edu_grade, n_member, male, female, internet, land_area_sum,
Quintiles, Bottom_20, Bottom_40)`
> 
> 
> # Match them with province name
> province <- read.csv(curl("https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Provinces_IDs.csv")) %>%
+   select (-c("CSMAP"))#load provincial data
> 
> province$Province_name <- str_remove(province$Province_name, "Tinh ")  #remove "Province" word
> province$Province_name <- str_remove(province$Province_name, "Thanh pho ")  #remove "City" word
> province$Province_name[province$Province_name=="Ba Ria Vung Tau"] <- "Ba Ria - Vung Tau" #rewording
> 
> province$MATINH <- str_pad(province$MATINH, width = 2, pad = 0) # format ID of provincial data
> 
> 
> vh <- left_join (vh, province, by = "MATINH") %>%
+   relocate (c("Province_name", "Region"), .after = MATINH) #join two datasets
> 
> 
> # Adoption at province level
> adopt_prep <- vh %>%
+   #filter(!is.na(weight_final_rice)) %>% 
+   group_by(MATINH, panel) %>%
+   summarize(
+     adopt_mcbh = sum((mech_mini_combiner == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     adopt_cbh = sum((mech_combine_harvester == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     adopt_straw_baler = sum((mech_straw_baler == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     adopt_lll = sum((mech_laser_level == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     adopt_row_seeder = sum((mech_row_seeder == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     adopt_seed_blower = sum((mech_seed_blower == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+   ) %>%
+   pivot_wider(names_from = panel, values_from = starts_with("adopt")) %>%
+   select (-c("adopt_mcbh_2023", "adopt_lll_2022", "adopt_row_seeder_2023"))
`summarise()` has grouped output by 'MATINH'. You can override using the `.groups` argument.
> 
> 
> # Mapping:
> # Function to plot and save maps:
> 
> plot_map <- function(var) {
+   # Set gradient limits based on the variable
+   if (var %in% c("adopt_lll_2023", "adopt_lll_2022")) {
+     gradient_limits <- c(0, 100)
+     midpoint <- 50
+   } else {
+     gradient_limits <- c(0, 50)
+     midpoint <- 25
+   }
+   
+   # Create the plot
+   plot <- modified_map %>%
+     ggplot() + 
+     aes(fill = .data[[var]]) + 
+     geom_sf() + 
+     scale_fill_gradient2(midpoint = midpoint, 
+                          low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                          na.value = "#dbd0d0", space = "Lab", name = "In %", 
+                          limits = gradient_limits) +  
+     theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+     annotate("rect", xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10, 
+              linewidth = 0.1, color = "black", fill = NA) +
+     annotate("rect", xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7, 
+              linewidth = 0.1, color = "black", fill = NA) + 
+     theme(axis.text.x = element_blank(), 
+           axis.text.y = element_blank(),
+           axis.ticks = element_blank())
+   
+   # Save the plot
+   ggsave(plot, filename = paste0("Output/9.%20Mechanisation/", var, ".png"), 
+          width = 1080, height = 600, units = "px")
+   
+   return(plot)
+ }
> map <- st_read("/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp")
Reading layer `Province_with_Islands' from data source 
  `/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 65 features and 18 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 102.144 ymin: 7.180931 xmax: 117.8355 ymax: 23.39221
Geodetic CRS:  WGS 84
> 
> map$MATINH <- 0
> map$Region <- ""
> t <- rep(NA,65)
> for (i in 1:length(map$ADM1_EN)) {
+   t[[i]] <- ifelse(grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         IDProv$Province_name) %>% length()==0,0,
+                    grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         IDProv$Province_name))
+   map$MATINH[[i]] <- ifelse(t[[i]]==0,NA,IDProv$MATINH[[t[[i]]]])
+   map$Region[[i]] <- ifelse(t[[i]]==0,NA,IDProv$Region[[t[[i]]]])}
Error: object 'IDProv' not found
> plot_map
function(var) {
  # Set gradient limits based on the variable
  if (var %in% c("adopt_lll_2023", "adopt_lll_2022")) {
    gradient_limits <- c(0, 100)
    midpoint <- 50
  } else {
    gradient_limits <- c(0, 50)
    midpoint <- 25
  }
  
  # Create the plot
  plot <- modified_map %>%
    ggplot() + 
    aes(fill = .data[[var]]) + 
    geom_sf() + 
    scale_fill_gradient2(midpoint = midpoint, 
                         low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
                         na.value = "#dbd0d0", space = "Lab", name = "In %", 
                         limits = gradient_limits) +  
    theme(plot.title = element_text(size = 8, hjust = 0.5)) +
    annotate("rect", xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10, 
             linewidth = 0.1, color = "black", fill = NA) +
    annotate("rect", xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7, 
             linewidth = 0.1, color = "black", fill = NA) + 
    theme(axis.text.x = element_blank(), 
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
  
  # Save the plot
  ggsave(plot, filename = paste0("Output/9.%20Mechanisation/", var, ".png"), 
         width = 1080, height = 600, units = "px")
  
  return(plot)
}
> rm(list = ls()) #start clean
> library (this.path)
> library (tidyverse)
> library (ggplot2)
> library (gridExtra)
> library (fastDummies)
> library (ggpolypath)
> library (eulerr)
> library (readxl)
> library (stringr)
> library (flextable)
> library (sf)
> library (curl)
> library (survey)
> library (haven)
> library (httr)
> #Set working directory to the location of this file
> setwd(this.path::here())   
> 
> 
> #Customized function to format ID
> 
> format_ID <- function(df, columns, widths, pad_char = "0") {
+   # Loop through each column and its corresponding width
+   for (i in seq_along(columns)) {
+     column <- columns[i]
+     width <- widths[i]
+     
+     # Pad the column with the specified width and character
+     df[[column]] <- str_pad(df[[column]], width = width, pad = pad_char)
+   }
+   return(df)
+ }
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> # Prepare layer maps ----
> # Load provincial data: 
> province <- read.csv(curl("https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Provinces_IDs.csv")) %>%
+   select (-c("CSMAP"))#load provincial data
> 
> province$Province_name <- str_remove(province$Province_name, "Tinh ")  #remove "Province" word
> province$Province_name <- str_remove(province$Province_name, "Thanh pho ")  #remove "City" word
> province$Province_name[province$Province_name=="Ba Ria Vung Tau"] <- "Ba Ria - Vung Tau" #rewording
> 
> province$MATINH <- str_pad(province$MATINH, width = 2, pad = 0) # format ID of provincial data
> 
> 
> map <- st_read("/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp")
Reading layer `Province_with_Islands' from data source 
  `/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 65 features and 18 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 102.144 ymin: 7.180931 xmax: 117.8355 ymax: 23.39221
Geodetic CRS:  WGS 84
> 
> map$MATINH <- 0
> map$Region <- ""
> t <- rep(NA,65)
> for (i in 1:length(map$ADM1_EN)) {
+   t[[i]] <- ifelse(grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         province$Province_name) %>% length()==0,0,
+                    grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         province$Province_name))
+   map$MATINH[[i]] <- ifelse(t[[i]]==0,NA,province$MATINH[[t[[i]]]])
+   map$Region[[i]] <- ifelse(t[[i]]==0,NA,province$Region[[t[[i]]]])}
> map %>% head() %>% print(width = 120) %>% colnames()
Simple feature collection with 6 features and 20 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 104.7781 ymin: 7.180931 xmax: 117.8355 ymax: 22.7396
Geodetic CRS:  WGS 84
  Shape_Leng Shape_Area           ADM1_EN                 ADM1_VI ADM1_PCODE ADM1_REF ADM1ALT1EN ADM1ALT2EN ADM1ALT1VI
1   8.860833  0.1361665   Paracel Islands                Hoang Sa     VN2001     <NA>       <NA>       <NA>       <NA>
2  32.976015  0.7353349   Spratly Islands               Truong Sa     VN2002     <NA>       <NA>       <NA>       <NA>
3   2.900742  0.2920399          An Giang                An Giang      VN805     <NA>       <NA>       <NA>       <NA>
4   3.419187  0.1630328 Ba Ria - Vung Tau B\xe0 R?a - V?ng T\xe0u      VN717     <NA>       <NA>       <NA>       <NA>
5   4.514786  0.3387481         Bac Giang               B?c Giang      VN221     <NA>       <NA>       <NA>       <NA>
6   4.207590  0.4255983           Bac Kan                 B?c K?n      VN207     <NA>       <NA>       <NA>       <NA>
  ADM1ALT2VI  ADM0_EN  ADM0_VI ADM0_PCODE       date    validOn validTo   layer
1       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
2       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
3       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
4       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
5       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
6       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
                                                                                                                                                                                                                                                            path
1 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
2 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
3 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
4 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
5 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
6 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
                        geometry MATINH Region
1 MULTIPOLYGON (((112.212 15....   <NA>   <NA>
2 MULTIPOLYGON (((114.8438 7....   <NA>   <NA>
3 MULTIPOLYGON (((105.1172 10...     89  6_MRD
4 MULTIPOLYGON (((106.5574 8....   <NA>   <NA>
5 MULTIPOLYGON (((106.163 21....     24 2_NMMA
6 MULTIPOLYGON (((105.7415 22...     06 2_NMMA
 [1] "Shape_Leng" "Shape_Area" "ADM1_EN"    "ADM1_VI"    "ADM1_PCODE" "ADM1_REF"   "ADM1ALT1EN"
 [8] "ADM1ALT2EN" "ADM1ALT1VI" "ADM1ALT2VI" "ADM0_EN"    "ADM0_VI"    "ADM0_PCODE" "date"      
[15] "validOn"    "validTo"    "layer"      "path"       "geometry"   "MATINH"     "Region"    
> TS <- map %>% filter(ADM1_VI=="Truong Sa") %>% st_geometry()
> cnTS = st_centroid(TS)
> TS_m = (TS-cnTS) * .25 + cnTS + c(-5,0)
> HS <- map %>% filter(ADM1_VI=="Hoang Sa") %>% st_geometry()
> cnHS = st_centroid(HS)
> HS_m =  (HS-cnHS) *.25 + cnHS + c(-2.5,0)
> modified_map <- map %>% filter(!(ADM1_VI %in% c("Truong Sa","Hoang Sa")))
> crs <- st_crs(modified_map)
> TS_map <- map %>% filter(ADM1_VI=="Truong Sa") %>% st_set_geometry(TS_m) %>% st_set_crs(crs)
> HS_map <- map %>% filter(ADM1_VI=="Hoang Sa") %>% st_set_geometry(HS_m) %>% st_set_crs(crs)
> modified_map <- rbind(modified_map,TS_map,HS_map)
> rm(TS,TS_m,TS_map,HS,HS_m,HS_map,cnHS,cnTS,crs, map, i, t)
> 
> 
> 
> 
> # Function to plot maps: ----
> create_plot <- function(modified_map, fill_var, title) {
+   ggplot(modified_map) + 
+     aes_string(fill = paste0("factor(", fill_var, ")")) +  # Convert fill_var to factor
+     geom_sf() + 
+     scale_fill_manual(
+       values = c("0" = "#ffffff", "1" = "#1c4c6f"),  # Define colors for each factor level
+       na.value = "#dbd0d0",                          # Color for NA values
+       name = NULL,                                    # Remove legend title
+       breaks = c("1", "0"),                           # Define which levels to display
+       labels = c("Yes", "No")                         # Custom labels for the legend
+     ) +  
+     ggtitle(title) +
+     theme(
+       plot.title = element_text(size = 12, hjust = 0.5),  # Increase title size to 14
+       axis.text.x = element_blank(), 
+       axis.text.y = element_blank(),
+       legend.title = element_blank(), # Ensure legend title is removed
+       axis.ticks = element_blank() 
+     ) +
+     geom_rect(
+       aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+       linewidth = 0.1, color = "black", fill = NA
+     ) +
+     geom_rect(
+       aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+       linewidth = 0.1, color = "black", fill = NA
+     )
+ }
> 
> 
> # Function to plot by component:----
> plot_component_fn <- function(data, footnote) {
+   plot <- data %>%
+     ggplot() +
+     geom_bar(aes(x=position, y=adoption, fill = group), stat="identity", alpha=0.8) +
+     labs (caption = footnote,
+           fill = "") +
+     xlab("") +
+     ylab ("(%)") +
+     theme (axis.text.y = element_text (size = 12),
+            legend.text = element_text(size = 12),
+            legend.title = element_text(size = 15),
+            title = element_text (size = 14),
+            axis.text.x = element_blank (),
+            axis.ticks.x = element_blank(),
+            legend.position = "bottom",
+            strip.text = element_text (size = 12),
+            plot.caption = element_text (hjust = 0)) +
+     scale_y_continuous(breaks = seq(0, 100, 20),
+                        expand = c(0,1)) +
+     scale_fill_manual (labels = c("Strict", "Lenient"),
+                        values = c("#006D5B", "#FFBF00")) +
+     facet_wrap(~component, scales = "fixed") 
+   
+   return (plot)
+ }
> 
> 
> # Load and merge data----
> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> 
> df_23 <- read.csv ("data/processed/VH23_data.csv") %>%
+   select (-c (starts_with("mech"), starts_with("straw"), Genotype_rec,
+               Genotype, CMD, DMC, CIAT.related, starts_with("Strain"), SWCP, SWCP_confirmed, weight_cass, 
+               weight_gift, weight_coffee)) %>%
+   distinct() 
> 
> df_23 <- format_ID(df_23, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), widths = c(2, 3, 5, 3, 3) ) #format admin-ID
> 
> df_23$IDHO <- paste0 (df_23$MAXA, df_23$MADIABAN, df_23$HOSO) #unique household ID
> 
> 
> 
> 
> curl_function("data/raw/VHLSS_2023_Household/Final/Final_1M5R/final_1m5r.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Final/Final_1M5R/final_1m5r.csv'
Content type 'text/plain; charset=utf-8' length 6039601 bytes (5.8 MB)
downloaded 5.8 MB

> df_1m5r <- read.csv ("data/raw/VHLSS_2023_Household/Final/Final_1M5R/final_1m5r.csv") %>%
+   select (c(MATINH:IDHO, rice_name, ws_rate_ha, seed_rate_duration, pre_rate_ha, KYDIEUTRA, 
+             ws_method, n_app_fert, sum_fert, 
+             d_1m5r_nitrogen_2app, d_1m5r_nitrogen_110kg, d_1m5r_nitrogen_3app, d_1m5r_nitrogen_100kg,
+             n_app_drug, n_app_insect_fungi, starts_with("n_drug_mix_"),
+             d_1m5r_pest_3app, d_1m5r_pest_6app, d_1m5r_pest_40d_sowing, 
+             d_1m5r_pest_flowering, d_1m5r_pest_20d_harvest,
+             starts_with ("purpose")))
> 
> df_1m5r <- format_ID(df_1m5r, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), widths = c(2, 3, 5, 3, 3) ) 
> 
> df_1m5r$IDHO <- paste0 (df_1m5r$MAXA, df_1m5r$MADIABAN, df_1m5r$HOSO)
> 
> dup_id <- df_1m5r[which(duplicated(df_1m5r$IDHO)),]$IDHO
> 
> df_1m5r <- df_1m5r %>%
+   filter (!IDHO %in% dup_id | (IDHO %in% dup_id & KYDIEUTRA == 4)) #there are some households surveyed in two quarters, but quarter 4 seems to be purposefully collected, so we keep values of Q4
> 
> 
> df <- left_join (df_23, df_1m5r) #join two dataset
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN, HOSO, IDHO)`
> 
> 
> df <- df %>%
+   rename (c("strict_1m" = "d_1m5r_certified",
+             "strict_1r" = "d_1m5r_seed_100kg",
+             "lenient_1r" = "d_1m5r_seed_120kg"))  #rename vars
> 
> df$MATINH <- str_pad(df$MATINH, width = 2, pad = 0) # format ID of newly merged data
> 
> 
> df <- left_join (df, province, by = "MATINH") %>%
+   relocate (c("Province_name", "Region"), .after = MATINH) #join two datasets
> # Figure 34: Vietnamese provinces referencing 1M5R/3R3G components in provincial agriculture plans  ----
> curl_function ("data/processed/ag_plan_1m5r_recode.xlsx")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/ag_plan_1m5r_recode.xlsx'
Content type 'application/octet-stream' length 13602 bytes (13 KB)
downloaded 13 KB

> ag_plan <- read_excel ("data/processed/ag_plan_1m5r_recode.xlsx") #load ag_plan data
> 
> # Add MATINH to the original excel file
> 
> ag_plan <- ag_plan %>%
+   mutate (package = case_when (mention_1M5R == 1 | mention_3R3G == 1 ~ 1,
+                                mention_1M5R == 0 & mention_3R3G == 0 ~ 0,
+                                TRUE ~ NA))  #recode variable
> ag_plan$MATINH <- as.character(ag_plan$MATINH)
> 
> ag_plan <- ag_plan %>%
+   left_join (province)
Joining with `by = join_by(MATINH)`
> # Recode agriculture plan:----
> 
> ag_plan <- ag_plan %>%
+   group_by(Province, Year) %>%
+   summarize (
+     MATINH = first(MATINH),
+     package_prv = case_when (mean(package) > 0 ~ 1,
+                              mean(package) == 0 ~ 0),
+     certified_prv = case_when (mean(certified) > 0 ~ 1,
+                                mean(certified) == 0 ~ 0),
+     seeding_prv = case_when (mean(seeding) > 0 ~ 1,
+                              mean(seeding) == 0 ~ 0),
+     pesticide_prv = case_when (mean(pesticide) > 0 ~ 1,
+                                mean(pesticide) == 0 ~ 0),
+     fertilizer_prv = case_when (mean(fertilizer) > 0 ~ 1,
+                                 mean(fertilizer) == 0 ~ 0)) %>%
+   mutate (n = n()) %>%
+   pivot_wider (names_from = Year, values_from = c(package_prv, certified_prv, seeding_prv, pesticide_prv, fertilizer_prv)) %>%
+   mutate (mention_package = case_when (mean(c_across(starts_with("package")), na.rm = TRUE) > 0 ~ 1, #if they mention at least once
+                                        TRUE ~ 0),
+           mention_certified = case_when (mean(c_across(starts_with("certified")), na.rm = TRUE) > 0 ~ 1,
+                                          TRUE ~ 0),
+           mention_seeding = case_when (mean(c_across(starts_with("seeding")), na.rm = TRUE) > 0 ~ 1,
+                                        TRUE ~ 0),
+           mention_pest = case_when (mean(c_across(starts_with("pest")), na.rm = TRUE) > 0 ~ 1,
+                                     TRUE ~ 0),
+           mention_fert = case_when (mean(c_across(starts_with("fert")), na.rm = TRUE) > 0 ~ 1,
+                                     TRUE ~ 0)) %>%
+   mutate (repeat_package = case_when (mean(c_across(starts_with("package")), na.rm = TRUE) > 1/n ~ 1, #if they repeatedly mention (more than once)
+                                       TRUE ~ 0),
+           repeat_certified = case_when (mean(c_across(starts_with("certified")), na.rm = TRUE) > 1/n ~ 1,
+                                         TRUE ~ 0),
+           repeat_seeding = case_when (mean(c_across(starts_with("seeding")), na.rm = TRUE) > 1/n ~ 1,
+                                       TRUE ~ 0),
+           repeat_pest = case_when (mean(c_across(starts_with("pest")), na.rm = TRUE) > 1/n ~ 1,
+                                    TRUE ~ 0),
+           repeat_fert = case_when (mean(c_across(starts_with("fert")), na.rm = TRUE) > 1/n ~ 1,
+                                    TRUE ~ 0)) 
`summarise()` has grouped output by 'Province'. You can override using the `.groups` argument.
> 
> modified_map <- modified_map %>% left_join(ag_plan,by="MATINH")
> 
> 
> 
> 
> plot_info <- list(
+   list(title = "Certified seeds (Mentioned)", fill_var = "mention_certified"),
+   list(title = "Certified seeds (Repeated)", fill_var = "repeat_certified"),
+   list(title = "1R: Seed rate (Mentioned)", fill_var = "mention_seeding"),
+   list(title = "1R: Seed rate (Repeated)", fill_var = "repeat_seeding"),
+   list(title = "2R: Fertilizer (Mentioned)", fill_var = "mention_fert"),
+   list(title = "2R: Fertilizer (Repeated)", fill_var = "repeat_fert"),
+   #list(title = "3R: Pesticide (Mentioned)", fill_var = "mention_pest"),
+   #list(title = "3R: Pesticide (Repeated)", fill_var = "repeat_pest"),
+   list(title = "3R3G/1M5R (Mentioned)", fill_var = "mention_package"),
+   list(title = "3R3G/1M5R (Repeated)", fill_var = "repeat_package")
+ )
> 
> 
> plots <- lapply(plot_info, function(info) {
+   create_plot(modified_map, info$fill_var, info$title) +
+     theme(axis.ticks = element_blank()) # Remove axis ticks
+ })
Warning message:
`aes_string()` was deprecated in ggplot2 3.0.0.
ℹ Please use tidy evaluation idioms with `aes()`.
ℹ See also `vignette("ggplot2-in-packages")` for more information.
This warning is displayed once every 8 hours.
Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. 
> 
> # gridExtra::grid.arrange(grobs = c(plots[1], plots[2], plots[3], plots[4], ncol = 2)
> map_ag_plan <- gridExtra::arrangeGrob(grobs = plots, ncol = 2)
> 
> ggsave (plot = map_ag_plan,
+         "Output/10. 3R3G_figures/Figure 36.png", 
+         width = 6, height = 15, dpi = 1024)
> 
> 
> 
> 
> 
> ## Figure 34: Adoption Rates of 1M5R/3R3G Practices by lenient and strict Criteria in Viet Nam in 2023  ----
> 
> var_lenient <- c("lenient_1m", "lenient_1r", "lenient_2r", "lenient_3r")
> var_strict <- c("strict_1m", "strict_1r", "strict_2r", "strict_3r")
> 
> adopt_all <- list()
> 
> calculate_adoption <- function(var_set, criterion_type) {
+   adopt_list <- list()
+   for (var in var_set) {
+     adopt <- df %>%
+       filter(!is.na(.data[[var]])) %>%
+       group_by(.data[[var]]) %>%
+       summarise(twt = sum(weight_final_rice, na.rm = TRUE)) %>%
+       mutate(sum_twt = sum(twt, na.rm = TRUE)) %>%
+       mutate(adoption = twt * 100 / sum_twt) %>%
+       filter(.data[[var]] == 1) %>%
+       select(-c(sum_twt, twt)) %>%
+       mutate(group = var, criterion = criterion_type)
+     adopt_list[[var]] <- adopt
+   }
+   return(adopt_list)
+ }
> 
> adopt_lenient <- calculate_adoption(var_lenient, "Lenient")
> adopt_strict <- calculate_adoption(var_strict, "Strict")
> 
> adopt_all <- c(adopt_lenient, adopt_strict)
> tab_adopt <- bind_rows(adopt_all) %>%
+   select(group, adoption, criterion)
> 
> tab_adopt$group <- factor(tab_adopt$group, 
+                           levels = c("lenient_1m", "lenient_1r", "lenient_2r", "lenient_3r",
+                                      "strict_1m", "strict_1r", "strict_2r", "strict_3r"),
+                           labels = c("Certified seeds", "1R: Seed rate", "2R: Fertilizer", "3R: Pesticide",
+                                      "Certified seeds", "1R: Seed rate", "2R: Fertilizer", "3R: Pesticide"))
> 
> # Plot with separate bars for lenient and strict criteria
> plot <- tab_adopt %>%
+   ggplot(aes(x = group, y = adoption, fill = criterion)) +
+   geom_bar(stat = "identity", position = position_dodge(width = 0.9), alpha = 0.8) +  # Increase dodge width to avoid overlap
+   labs(x = " ", y = "Percentage of Adopters", fill = "Criteria") +
+   scale_y_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100)) +
+   theme_minimal() +
+   theme(
+     axis.text.x = element_text(size = 10),  # Keep x-axis labels horizontal
+     axis.text.y = element_text(size = 12),  # Adjust y-axis font size
+     panel.grid = element_blank(),  # Remove background grid
+     legend.position = "top"  # Position legend at the top
+   ) +
+   scale_fill_manual(values = c("Lenient" = "#1c4c6f", "Strict" = "#d95f02"))  # Different colors for criteria
> 
> 
> plot
> 
> ggsave (plot, filename = "Output/10. 3R3G_figures/3R3G_Figure 3.png", width = 10, height = 6, dpi = 1024)
> plot
> ## Figure 35: Household adoption rates at the province-level for (a) Certified seeds, (b) 1R: Seed rate, (c) 2R: Fertilizer, (d) 3R: Pesticide----
> 
> tab_fig4 <- df %>%
+   #filter(!is.na(weight_final_rice)) %>% 
+   group_by(MATINH) %>%
+   summarize(
+     lenient_1m = sum((lenient_1m == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     lenient_1r = sum((lenient_1r == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     lenient_2r = sum((lenient_2r == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     lenient_3r = sum((lenient_3r == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100
+   )
> 
> 
> modified_map <- modified_map %>% left_join(tab_fig4,by="MATINH")
> 
> OneM <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_1m) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(a) Certified seeds') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank())
> 
> R1 <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_1r) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(a) 1R: Seed rate') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank())
> 
> R2 <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_2r) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(c) 2R: Fertilizer') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank())
> 
> 
> R3 <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_3r) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(d) 3R: Pesticide') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank())
> 
> 
> OneM <- OneM + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> R1 <- R1 + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> R2 <- R2 + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> R3 <- R3 + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> 
> 
> a <- grid.arrange(OneM, R1, R2, R3, nrow = 2, ncol = 2)
> 
> a
TableGrob (2 x 2) "arrange": 4 grobs
  z     cells    name           grob
1 1 (1-1,1-1) arrange gtable[layout]
2 2 (1-1,2-2) arrange gtable[layout]
3 3 (2-2,1-1) arrange gtable[layout]
4 4 (2-2,2-2) arrange gtable[layout]
> 
> ggsave (a, filename = "Output/10. 3R3G_figures/Figure4_spatial.png", 
+         height = 15, width = 8, 
+         dpi = 1024)
> ## Figure 37: Incidence of different rice seeding and transplanting methods among (a) non-adopters (b) adopters of 120kg of seed per ha (lenient criterion) and (c) adopters of 100kg of seed per ha (strict criterion)	165 ----
> 
> df <- df %>%
+   mutate (ws_method_lab = case_when (ws_method == 1 ~ "Hand seeding",
+                                      ws_method == 2 ~ "Row seeder/drum seeder",
+                                      ws_method == 3 ~ "Seed blower",
+                                      ws_method == 4 ~ "Transplanting by hand using hard plating",
+                                      ws_method == 5 ~ "Transplanting by hand using soft plating",
+                                      ws_method == 6 ~ "Transplanting by machine",
+                                      ws_method == 7 ~ "Others",
+                                      TRUE ~ "Unknown"))
> 
> df$ws_method_lab <- factor(df$ws_method_lab, 
+                            levels = rev(c("Hand seeding", "Row seeder/drum seeder", "Seed blower",
+                                           "Transplanting by hand using hard plating",
+                                           "Transplanting by hand using soft plating",
+                                           "Transplanting by machine",
+                                           "Others", 
+                                           "Unknown")))
> 
> seed_rate <- df %>%
+   filter (lenient_1r == 1) 
> 
> seed_rate_strict <- df %>%
+   filter (strict_1r == 1)
> 
> seed_rate_non_adopt <- df %>%
+   filter (strict_1r == 0 & lenient_1r == 0)
> 
> seed_rate$ws_method[seed_rate$ws_method == 99] <- NA
> seed_rate_strict$ws_method[seed_rate_strict$ws_method == 99] <- NA
> seed_rate_non_adopt$ws_method[seed_rate_non_adopt$ws_method == 99] <- NA
> 
> seed_method_group <- list ("(a) Non adopters" = seed_rate_non_adopt,
+                            "(b) Adopters (lenient criterion)" = seed_rate,
+                            "(c) Adopters (strict criterion)" = seed_rate_strict)
> 
> list_seed_method <- list () 
> for (data in (1:length(seed_method_group))){
+   adopt <- seed_method_group[[data]] %>%
+     group_by(ws_method_lab) %>%
+     summarise (twt = sum(weight_final_rice, na.rm = TRUE)) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     mutate (group = names (seed_method_group[data]))
+   
+   list_seed_method[[data]] <- adopt
+ }
> 
> list_seed_method
[[1]]
# A tibble: 8 × 5
  ws_method_lab                                 twt  sum_twt adoption group           
  <fct>                                       <dbl>    <dbl>    <dbl> <chr>           
1 Unknown                                    21052. 1731262.    1.22  (a) Non adopters
2 Others                                      6585. 1731262.    0.380 (a) Non adopters
3 Transplanting by machine                   21132. 1731262.    1.22  (a) Non adopters
4 Transplanting by hand using soft plating   69260. 1731262.    4.00  (a) Non adopters
5 Transplanting by hand using hard plating   32335. 1731262.    1.87  (a) Non adopters
6 Seed blower                               197194. 1731262.   11.4   (a) Non adopters
7 Row seeder/drum seeder                     67113. 1731262.    3.88  (a) Non adopters
8 Hand seeding                             1316592. 1731262.   76.0   (a) Non adopters

[[2]]
# A tibble: 8 × 5
  ws_method_lab                                 twt  sum_twt adoption group                           
  <fct>                                       <dbl>    <dbl>    <dbl> <chr>                           
1 Unknown                                    58316. 5426983.    1.07  (b) Adopters (lenient criterion)
2 Others                                      8071. 5426983.    0.149 (b) Adopters (lenient criterion)
3 Transplanting by machine                  154618. 5426983.    2.85  (b) Adopters (lenient criterion)
4 Transplanting by hand using soft plating 1270452. 5426983.   23.4   (b) Adopters (lenient criterion)
5 Transplanting by hand using hard plating 1049622. 5426983.   19.3   (b) Adopters (lenient criterion)
6 Seed blower                                67128. 5426983.    1.24  (b) Adopters (lenient criterion)
7 Row seeder/drum seeder                     54195. 5426983.    0.999 (b) Adopters (lenient criterion)
8 Hand seeding                             2764580. 5426983.   50.9   (b) Adopters (lenient criterion)

[[3]]
# A tibble: 8 × 5
  ws_method_lab                                 twt  sum_twt adoption group                          
  <fct>                                       <dbl>    <dbl>    <dbl> <chr>                          
1 Unknown                                    54017. 5079770.    1.06  (c) Adopters (strict criterion)
2 Others                                      8071. 5079770.    0.159 (c) Adopters (strict criterion)
3 Transplanting by machine                  153399. 5079770.    3.02  (c) Adopters (strict criterion)
4 Transplanting by hand using soft plating 1256184. 5079770.   24.7   (c) Adopters (strict criterion)
5 Transplanting by hand using hard plating 1040083. 5079770.   20.5   (c) Adopters (strict criterion)
6 Seed blower                                37032. 5079770.    0.729 (c) Adopters (strict criterion)
7 Row seeder/drum seeder                     41298. 5079770.    0.813 (c) Adopters (strict criterion)
8 Hand seeding                             2489687. 5079770.   49.0   (c) Adopters (strict criterion)

> 
> tab_seed_method <- bind_rows(list_seed_method) %>%
+   select (c(ws_method_lab, adoption, group))
> 
> plot_seed_method <- tab_seed_method %>%
+   ggplot(aes(x = ws_method_lab, y = adoption, fill = ws_method_lab)) + 
+   geom_bar(stat = "identity", width = 1) +
+   coord_flip() +
+   theme(
+     legend.title = element_blank(),
+     legend.position = "none",
+     axis.title.y = element_blank(),
+     axis.text.y = element_text(size = 10),
+     axis.ticks.y = element_blank(),
+     strip.text = element_text(size = 12),
+     panel.grid = element_blank(),  # Remove background grid
+     panel.background = element_rect(fill = "gray99", color = NA)  # Lighten background to a very light grey
+   ) +
+   geom_text(aes(label = round(adoption, 0)),  # Remove % symbol from label
+             hjust = -0.1,
+             color = "black") +
+   scale_fill_manual(values = c("lightgrey",
+                                "#dbe9f6",
+                                "#b3d7e8",
+                                "#87bdd8",
+                                "#5697c6",
+                                "#336b87",
+                                "#24496e",
+                                "#16324f")) +
+   facet_wrap(~group, scales = "fixed") +
+   ylab("%") +
+   ylim(c(0, 100))
> 
> plot_seed_method
> 
> ggsave (plot_seed_method, filename = "Output/10. 3R3G_figures/plot_seed_method.png", dpi = 1024, height = 6, width = 14)
> ## Figure 38: Adoption of fertiliser use recommendations under strict and lenient criteria: (a) number of fertiliser applications, (b) amount of Nitrogen per application, and (c) both -----
> # Non-Nitrogen fertilizer: 
> 
> df %>%
+   filter(!is.na(sum_fert)) %>%
+   group_by(sum_fert == 0) %>%
+   summarise(twt = sum(weight_final_rice, na.rm = TRUE)) %>%
+   mutate(sum_twt = sum(twt, na.rm = TRUE)) %>%
+   mutate(adoption = twt * 100 / sum_twt)  #This supports numbers used in part "Willingness to change" of the Fertilizer
# A tibble: 2 × 4
  `sum_fert == 0`      twt  sum_twt adoption
  <lgl>              <dbl>    <dbl>    <dbl>
1 FALSE           6629275. 7490611.     88.5
2 TRUE             861336. 7490611.     11.5
> 
> list_2r <- c("d_1m5r_nitrogen_100kg", "d_1m5r_nitrogen_110kg", 
+              "d_1m5r_nitrogen_3app", "d_1m5r_nitrogen_2app",
+              "strict_2r", "lenient_2r")
> 
> 
> adopt_list <- list ()
> 
> for (var in list_2r){
+   adopt <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (.data[[var]]) %>%
+     summarise (twt = sum(weight_final_rice, na.rm = TRUE)) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     mutate (name = var) %>%
+     select (c(name, adoption)) 
+   
+   adopt_list[[var]] <- adopt
+ }
> 
> tab_2r_component <- bind_rows(adopt_list) %>%
+   mutate(
+     group = case_when(
+       str_detect(name, "110kg") ~ "Lenient",
+       str_detect(name, "100kg") ~ "Strict",
+       str_detect(name, "2app") ~ "Lenient",
+       str_detect(name, "3app") ~ "Strict",
+       str_detect(name, "strict") ~ "Strict",
+       str_detect(name, "lenient") ~ "Lenient"
+     ),
+     # Directly set the component levels
+     component = factor(
+       case_when(
+         str_detect(name, "app") ~ "(a) Number of applications",
+         str_detect(name, "kg") ~ "(b) Amount of Nitrogen per application",
+         TRUE ~ "(a) + (b)"
+       ),
+       levels = c("(a) Number of applications", 
+                  "(b) Amount of Nitrogen per application",
+                  "(a) + (b)")
+     )
+   ) %>%
+   select(-name) %>%
+   mutate(position = case_when(group == "Strict" ~ 0.5,
+                               group == "Lenient" ~ 1))
> 
> tab_2r_component$group <- factor(tab_2r_component$group, levels = c("Strict", "Lenient"))
> 
> 
> plot_2r_component <- plot_component_fn(tab_2r_component, footnote = "") +  # Provide an empty footnote
+   scale_fill_manual(values = c("Lenient" = "#1c4c6f", "Strict" = "#d95f02")) +  # Set colors
+   theme_minimal() +  # Use a minimal theme as a base
+   theme(
+     panel.grid = element_blank(),  # Drop all grid lines
+     legend.title = element_blank(),
+     axis.text.x = element_blank(),  # Drop x labels
+     axis.text.y = element_text(size = 12),  # Adjust y-axis text size if needed
+     axis.title.y = element_text(size = 14),  # Adjust y-axis title size
+     axis.title.x = element_blank()  # Drop x-axis title
+   ) +
+   theme(axis.ticks = element_line(colour = NA),
+         panel.grid.major = element_line(linetype = "blank"),
+         panel.grid.minor = element_line(linetype = "blank"),
+         axis.text = element_text(size = 1), 
+         plot.title = element_text(size = 14),  # Set title size to 14
+         panel.background = element_rect(fill = "gray97")) +
+   labs(y = "% adopters", x = NULL, fill = NULL, caption = NULL) +  # Set y-axis label
+   ylim(c(0, 100))  # Ensure y-axis extends from 0 to 100
Scale for fill is already present.
Adding another scale for fill, which will replace the existing scale.
Scale for y is already present.
Adding another scale for y, which will replace the existing scale.
> 
> 
> ggsave(plot_2r_component, filename = "Output/10. 3R3G_figures/Figure7.png", dpi = 1024,
+        width = 10, height = 5)
> plot_3r_component
Error: object 'plot_3r_component' not found
> df <- df %>%
+   mutate (d_1m5r_pest_timing_lenient = case_when (d_1m5r_pest_40d_sowing == 1 & d_1m5r_pest_20d_harvest == 1 ~ 1,
+                                                   d_1m5r_pest_40d_sowing != 1 | d_1m5r_pest_20d_harvest != 1 ~ 0,
+                                                   TRUE ~ NA),
+           d_1m5r_pest_timing_strict = case_when (d_1m5r_pest_40d_sowing == 1 & d_1m5r_pest_flowering == 1 ~ 1,
+                                                  d_1m5r_pest_40d_sowing != 1 | d_1m5r_pest_flowering != 1 ~ 0,
+                                                  TRUE ~ NA))
> 
> list_3r <- c("d_1m5r_pest_6app", "d_1m5r_pest_timing_lenient",
+              "d_1m5r_pest_3app", "d_1m5r_pest_timing_strict",
+              "lenient_3r", "strict_3r")
> 
> 
> adopt_list <- list ()
> 
> for (var in list_3r){
+   adopt <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (.data[[var]]) %>%
+     summarise (twt = sum(weight_final_rice, na.rm = TRUE)) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     mutate (name = var) %>%
+     select (c(name, adoption)) 
+   
+   adopt_list[[var]] <- adopt
+ }
> 
> 
> 
> tab_3r_component <- bind_rows(adopt_list) %>%
+   mutate(
+     group = case_when(
+       str_detect(name, "6app") ~ "Lenient",
+       str_detect(name, "3app") ~ "Strict",
+       str_detect(name, "strict") ~ "Strict",
+       str_detect(name, "lenient") ~ "Lenient"
+     ),
+     # Directly set the component levels
+     component = factor(
+       case_when(
+         str_detect(name, "app") ~ "(a) Number of applications",
+         str_detect(name, "3r") ~ "(a) + (b)",
+         TRUE ~ "(b) Timing of applications"
+       ),
+       levels = c("(a) Number of applications", 
+                  "(b) Timing of applications",
+                  "(a) + (b)")
+     )
+   ) %>%
+   select(-name) %>%  # Uncomment if you want to drop the 'name' column
+   mutate(position = case_when(group == "Strict" ~ 0.5,
+                               group == "Lenient" ~ 1))
> 
> tab_3r_component$group <- factor(tab_3r_component$group, levels = c("Strict", "Lenient"))
> 
> # Plot without the title
> plot_3r_component <- plot_component_fn(tab_3r_component, footnote = "") +  # Provide an empty footnote
+   scale_fill_manual(values = c("Lenient" = "#1c4c6f", "Strict" = "#d95f02")) +  # Set colors
+   theme_minimal() +  # Use a minimal theme as a base
+   theme(
+     panel.grid = element_blank(),  # Drop all grid lines
+     legend.title = element_blank(),
+     axis.text.x = element_blank(),  # Drop x labels
+     axis.text.y = element_text(size = 12),  # Adjust y-axis text size if needed
+     axis.title.y = element_text(size = 14),  # Adjust y-axis title size
+     axis.title.x = element_blank()  # Drop x-axis title
+   ) +
+   theme(axis.ticks = element_line(colour = NA),
+         panel.grid.major = element_line(linetype = "blank"),
+         panel.grid.minor = element_line(linetype = "blank"),
+         axis.text = element_text(size = 1), 
+         plot.title = element_text(size = 14),  # Set title size to 14
+         panel.background = element_rect(fill = "gray97")) +
+   labs(y = "% adopters", x = NULL, fill = NULL, caption = NULL) +  # Set y-axis label
+   ylim(c(0, 100))  # Ensure y-axis extends from 0 to 100
Scale for fill is already present.
Adding another scale for fill, which will replace the existing scale.
Scale for y is already present.
Adding another scale for y, which will replace the existing scale.
> 
> 
> ggsave(plot_3r_component, filename = "Output/Fig_40.png", dpi = 1024,
+        width = 10, height = 5)
> plot_3r_component
> curl_function ("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv'
Content type 'text/plain; charset=utf-8' length 80331 bytes (78 KB)
downloaded 78 KB

> M4B13A <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
> 
> M4B13A$SWCP <- ifelse (M4B13A$M4B13A_C2 <=3 & M4B13A$M4B13A_C3 <= 400, 1, 0); table (M4B13A$SWCP) # 830/1279 = 65% 

  0   1 
442 830 
> M4B13A$SWCP_confirmed <- ifelse (M4B13A$M4B13A_C2 <=3 & M4B13A$M4B13A_C3 <= 400 & M4B13A$M4B13A_C4 != 4 , 1, 0); table (M4B13A$SWCP_confirmed) # 357/1279 = 17%. FIltering out hhs not knowing quantity of water used

  0   1 
657 615 
> 
> 
> M4B13A <- merge (M4B13A, province, all.x =TRUE)
> 
> M4B13A <- M4B13A [M4B13A$Province_name %in% c('Tinh Dac Nong','Tinh Dak Lak','Tinh Gia Lai',
+                                               'Tinh Kon Tum','Tinh Lam Dong') ,] 
> 
> percentage_data <- M4B13A %>%
+   group_by(Province_name) %>%
+   summarize(##Lower.bound = mean(SWCP_confirmed == TRUE, na.rm = TRUE) * 100,
+     Higher.bound = mean(SWCP == TRUE, na.rm = TRUE) * 100)
> 
> percentage_data_long <- gather(percentage_data, key = "SWCP_percentage", value = "Percentage", -Province_name)
> 
> 
> 
> a <- ggplot(percentage_data_long, aes(x = Province_name, y = Percentage)) + 
+   geom_bar(stat = "identity", position = "dodge", fill = "#1c4c6f") +  # Same color for all bars
+   labs(x = " ",  # X-axis shows Province names
+        y = "Percentage of Adopters") +  # Y-axis label
+   scale_y_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100)) +
+   theme_minimal() + 
+   theme(
+     axis.text.x = element_text(size = 10),  # Adjust x-axis font size
+     panel.grid = element_blank(),  # Remove background grid
+     legend.position = "none"  # Drop the legend
+   )
> a
> percentage_data_long
# A tibble: 0 × 3
# ℹ 3 variables: Province_name <chr>, SWCP_percentage <chr>, Percentage <dbl>
> curl_function ("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv'
Content type 'text/plain; charset=utf-8' length 80331 bytes (78 KB)
downloaded 78 KB

> M4B13A <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
> 
> M4B13A$SWCP <- ifelse (M4B13A$M4B13A_C2 <=3 & M4B13A$M4B13A_C3 <= 400, 1, 0); table (M4B13A$SWCP) # 830/1279 = 65% 

  0   1 
442 830 
> M4B13A$SWCP_confirmed <- ifelse (M4B13A$M4B13A_C2 <=3 & M4B13A$M4B13A_C3 <= 400 & M4B13A$M4B13A_C4 != 4 , 1, 0); table (M4B13A$SWCP_confirmed) # 357/1279 = 17%. FIltering out hhs not knowing quantity of water used

  0   1 
657 615 
> 
> 
> M4B13A <- merge (M4B13A, province, all.x =TRUE)
> 
> M4B13A <- M4B13A [M4B13A$Province_name %in% c('Tinh Dac Nong','Tinh Dak Lak','Tinh Gia Lai',
+                                               'Tinh Kon Tum','Tinh Lam Dong') ,] 
> curl_function ("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv'
Content type 'text/plain; charset=utf-8' length 80331 bytes (78 KB)
downloaded 78 KB

> M4B13A <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
> 
> M4B13A$SWCP <- ifelse (M4B13A$M4B13A_C2 <=3 & M4B13A$M4B13A_C3 <= 400, 1, 0); table (M4B13A$SWCP) # 830/1279 = 65% 

  0   1 
442 830 
> #M4B13A$SWCP_confirmed <- ifelse (M4B13A$M4B13A_C2 <=3 & M4B13A$M4B13A_C3 <= 400 & M4B13A$M4B13A_C4 != 4 , 1, 0); table (M4B13A$SWCP_confirmed) # 357/1279 = 17%. FIltering out hhs not knowing quantity of water used
> M4B13A <- merge (M4B13A, province, all.x =TRUE)
> M4B13A <- M4B13A [M4B13A$Province_name %in% c('Tinh Dac Nong','Tinh Dak Lak','Tinh Gia Lai',
+                                               'Tinh Kon Tum','Tinh Lam Dong') ,] 
> curl_function ("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv'
Content type 'text/plain; charset=utf-8' length 80331 bytes (78 KB)
downloaded 78 KB

> M4B13A <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
> 
> M4B13A$SWCP <- ifelse (M4B13A$M4B13A_C2 <=3 & M4B13A$M4B13A_C3 <= 400, 1, 0); table (M4B13A$SWCP) # 830/1279 = 65% 

  0   1 
442 830 
> #M4B13A$SWCP_confirmed <- ifelse (M4B13A$M4B13A_C2 <=3 & M4B13A$M4B13A_C3 <= 400 & M4B13A$M4B13A_C4 != 4 , 1, 0); table (M4B13A$SWCP_confirmed) # 357/1279 = 17%. FIltering out hhs not knowing quantity of water used
> 
> 
> M4B13A <- merge (M4B13A, province, all.x =TRUE)
> M4B13A$Province_name
   [1] "Son La"   "Son La"   "Son La"   "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [10] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [19] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [28] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [37] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [46] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [55] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [64] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [73] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [82] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
  [91] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
 [100] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum" 
 [109] "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Kon Tum"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [118] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [127] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [136] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [145] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [154] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [163] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [172] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [181] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [190] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [199] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [208] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [217] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [226] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [235] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [244] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [253] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [262] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [271] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [280] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [289] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [298] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [307] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [316] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [325] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [334] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [343] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [352] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [361] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [370] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai" 
 [379] "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Gia Lai"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [388] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [397] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [406] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [415] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [424] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [433] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [442] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [451] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [460] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [469] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [478] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [487] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [496] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [505] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [514] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [523] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [532] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [541] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [550] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [559] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [568] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [577] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [586] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [595] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [604] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [613] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [622] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [631] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [640] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [649] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [658] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [667] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [676] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [685] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [694] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [703] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [712] "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak"  "Dak Lak" 
 [721] "Dak Lak"  "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [730] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [739] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [748] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [757] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [766] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [775] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [784] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [793] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [802] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [811] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [820] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [829] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [838] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [847] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [856] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [865] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [874] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [883] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [892] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [901] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [910] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [919] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [928] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [937] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [946] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [955] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [964] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [973] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [982] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
 [991] "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong" "Dak Nong"
[1000] "Dak Nong"
 [ reached getOption("max.print") -- omitted 279 entries ]
> percentage_data <- M4B13A %>%
+   group_by(Province_name) %>%
+   summarize(##Lower.bound = mean(SWCP_confirmed == TRUE, na.rm = TRUE) * 100,
+     Higher.bound = mean(SWCP == TRUE, na.rm = TRUE) * 100)
> percentage_data_long <- gather(percentage_data, key = "SWCP_percentage", value = "Percentage", -Province_name)
> View(percentage_data)
> ggplot(percentage_data_long, aes(x = Province_name, y = Percentage)) + 
+   geom_bar(stat = "identity", position = "dodge", fill = "#1c4c6f") +  # Same color for all bars
+   labs(x = " ",  # X-axis shows Province names
+        y = "Percentage of Adopters") +  # Y-axis label
+   scale_y_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100)) +
+   theme_minimal() + 
+   theme(
+     axis.text.x = element_text(size = 10),  # Adjust x-axis font size
+     panel.grid = element_blank(),  # Remove background grid
+     legend.position = "none"  # Drop the legend
+   )
Warning message:
Removed 1 rows containing missing values (`geom_bar()`). 
> M4B13A <- M4B13A [M4B13A$Province_name %in% c('Dac Nong','Dak Lak','Gia Lai',
+                                               'Kon Tum','Lam Dong') ,] 
> percentage_data <- M4B13A %>%
+   group_by(Province_name) %>%
+   summarize(##Lower.bound = mean(SWCP_confirmed == TRUE, na.rm = TRUE) * 100,
+     Higher.bound = mean(SWCP == TRUE, na.rm = TRUE) * 100)
> 
> percentage_data_long <- gather(percentage_data, key = "SWCP_percentage", value = "Percentage", -Province_name)
> 
> 
> a <- ggplot(percentage_data_long, aes(x = Province_name, y = Percentage)) + 
+   geom_bar(stat = "identity", position = "dodge", fill = "#1c4c6f") +  # Same color for all bars
+   labs(x = " ",  # X-axis shows Province names
+        y = "Percentage of Adopters") +  # Y-axis label
+   scale_y_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100)) +
+   theme_minimal() + 
+   theme(
+     axis.text.x = element_text(size = 10),  # Adjust x-axis font size
+     panel.grid = element_blank(),  # Remove background grid
+     legend.position = "none"  # Drop the legend
+   )
> a
> curl_function ("data/raw/VHLSS_2023_Household/Final/SPIA_M4B11A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Final/SPIA_M4B11A.csv'
Content type 'text/plain; charset=utf-8' length 2120705 bytes (2.0 MB)
downloaded 2.0 MB

> M4B11A <- read.csv ("data/raw/VHLSS_2023_Household/Final/SPIA_M4B11A.csv")
> M4B11A <- format_ID(M4B11A, columns = c("MATINH", "MAXA", "MADIABAN"), c(2,5,3))
> 
> curl_function ("Output/Report_weights.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/Output/Report_weights.csv'
Content type 'text/plain; charset=utf-8' length 200480 bytes (195 KB)
downloaded 195 KB

> weight <- read.csv ("Output/Report_weights.csv") 
> weight <- weight[!is.na (weight$weight_final_rice),]
> weight <- format_ID(weight, columns = c("MATINH", "MAXA", "MADIABAN"), c(2,5,3))
> 
> 
> weight$ID <- paste (weight$MATINH, weight$MAXA, weight$MADIABAN, sep="-"); M4B11A$ID <- paste (M4B11A$MATINH, M4B11A$MAXA, M4B11A$MADIABAN, sep="-")
> weight <- weight %>% distinct(ID, .keep_all = TRUE)
> 
> M4B11A <- merge (M4B11A, weight [, c(10,4)], by="ID", all.x=TRUE)
> 
> 
> 
> M4B11A <- M4B11A %>% filter(!is.na(weight_final_rice))
> survey_design <- svydesign(ids = ~1, data = M4B11A, weights = ~weight_final_rice)
> # AWD 2022
> curl_function ("data/raw/VHLSS_2022_Household/datasets/Ho_TTC_M4B11_season_edit.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Ho_TTC_M4B11_season_edit.csv'
Content type 'text/plain; charset=utf-8' length 3290049 bytes (3.1 MB)
downloaded 3.1 MB

> TTC_M4B111 <- read.csv ("data/raw/VHLSS_2022_Household/datasets/Ho_TTC_M4B11_season_edit.csv") # Crop cut module
> 
> TTC_M4B111$Onedd <- ifelse (TTC_M4B111$M4B111_C7 == 1 & TTC_M4B111$M4B111_C8 >=5, TRUE, FALSE)
> TTC_M4B111$Twodd <- ifelse (TTC_M4B111$M4B111_C7 > 1 & TTC_M4B111$M4B111_C8 >=5, TRUE, FALSE)
> 
> # HH-level | One drydown
> svy_design <- svydesign(ids = ~1, data = TTC_M4B111, weights = ~wt45)
> weighted_table <- svytable(~Onedd, design = svy_design)
> weighted_df <- as.data.frame(weighted_table)
> weighted_df$percentage <- round (prop.table(weighted_df$Freq) * 100, 1)
> weighted_df
  Onedd    Freq percentage
1 FALSE 5851406       86.4
2  TRUE  924315       13.6
> 
> # HH-level | At least two dry-downs of 5 days minimum
> svy_design <- svydesign(ids = ~1, data = TTC_M4B111, weights = ~wt45)
> weighted_table2 <- svytable(~Twodd, design = svy_design)
> weighted_df2 <- as.data.frame(weighted_table2)
> weighted_df2$percentage <- round (prop.table(weighted_df2$Freq) * 100, 1)
> weighted_df2
  Twodd    Freq percentage
1 FALSE 4651607       68.6
2  TRUE 2124746       31.4
> curl_function ("data/raw/VHLSS_2023_Household/Final/SPIA_M4B11A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Final/SPIA_M4B11A.csv'
Content type 'text/plain; charset=utf-8' length 2120705 bytes (2.0 MB)
downloaded 2.0 MB

> M4B11A <- read.csv ("data/raw/VHLSS_2023_Household/Final/SPIA_M4B11A.csv")
> M4B11A <- format_ID(M4B11A, columns = c("MATINH", "MAXA", "MADIABAN"), c(2,5,3))
> 
> curl_function ("Output/Report_weights.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/Output/Report_weights.csv'
Content type 'text/plain; charset=utf-8' length 200480 bytes (195 KB)
downloaded 195 KB

> weight <- read.csv ("Output/Report_weights.csv") 
> weight <- weight[!is.na (weight$weight_final_rice),]
> weight <- format_ID(weight, columns = c("MATINH", "MAXA", "MADIABAN"), c(2,5,3))
> 
> 
> weight$ID <- paste (weight$MATINH, weight$MAXA, weight$MADIABAN, sep="-"); M4B11A$ID <- paste (M4B11A$MATINH, M4B11A$MAXA, M4B11A$MADIABAN, sep="-")
> weight <- weight %>% distinct(ID, .keep_all = TRUE)
> 
> M4B11A <- merge (M4B11A, weight [, c(10,4)], by="ID", all.x=TRUE)
> M4B11A <- M4B11A %>% filter(!is.na(weight_final_rice))
> survey_design <- svydesign(ids = ~1, data = M4B11A, weights = ~weight_final_rice)
> 
> 
> # 1. Calculate the weighted percentage of adopters in VH23(M4B11A7_C12 == 1)
> adopter_percentage <- svymean(~I(M4B11A$M4B11A7_C12 == 1), survey_design, na.rm = TRUE) * 100
> print(adopter_percentage)
                                  mean     SE
I(M4B11A$M4B11A7_C12 == 1)FALSE 88.235 0.0037
I(M4B11A$M4B11A7_C12 == 1)TRUE  11.765 0.0037
> 
> # 2. Calculate the weighted percentage of communes with at least one adopter
> # First, create a new variable at the commune level that indicates if there is at least one adopter
> commune_adoption <- M4B11A %>%
+   group_by(ID) %>%
+   summarise(has_adopter = as.numeric(any(M4B11A7_C12 == 1)), 
+             weight_final_rice = mean(weight_final_rice, na.rm = TRUE))
> 
> # Create a new survey design object for commune-level data
> commune_design <- svydesign(ids = ~1, data = commune_adoption, weights = ~weight_final_rice)
> 
> # Calculate the weighted percentage of communes with at least one adopter
> commune_adopter_percentage <- svymean(~has_adopter, commune_design, na.rm = TRUE) * 100
> print(commune_adopter_percentage)
              mean     SE
has_adopter 24.813 0.0136
> missing_per_value <- df_23 %>%
+   group_by(awd_1drydown) %>%
+   summarise(missing_weights = sum(is.na(weight_final_rice)),
+             total_rows = n())
> 
> df_23_clean <- df_23[!is.na(df_23$weight_final_rice) & !is.na(df_23$awd_1drydown), ]
> 
> # HH-level | One drydown
> svy_design <- svydesign(ids = ~1, data = df_23_clean, weights = ~weight_final_rice)
> weighted_table <- svytable(~awd_1drydown, design = svy_design)
> weighted_df <- as.data.frame(weighted_table)
> weighted_df$percentage <- round (prop.table(weighted_df$Freq) * 100, 1)
> weighted_df
  awd_1drydown      Freq percentage
1            0 7147839.7       94.6
2            1  408689.1        5.4
> 
> # HH-level | At least two dry-downs of 5 days minimum
> svy_design <- svydesign(ids = ~1, data = df_23_clean, weights = ~weight_final_rice)
> weighted_table2 <- svytable(~awd_2drydown, design = svy_design)
> weighted_df2 <- as.data.frame(weighted_table2)
> weighted_df2$percentage <- round (prop.table(weighted_df2$Freq) * 100, 1)
> weighted_df2
  awd_2drydown      Freq percentage
1            0 7307200.4       96.7
2            1  249328.4        3.3
> 
> # EA-level | One drydown
> df_23_clean$IDCo <- paste (df_23_clean$MATINH, df_23_clean$MAHUYEN, df_23_clean$MAXA, df_23_clean$MADIABAN, sep='-') 
> 
> commune_adoption <- df_23_clean %>%
+   group_by(IDCo) %>%  # Replace `ID` with the commune-level identifier (`MATINH`)
+   summarise(
+     has_adopter = as.numeric(any(awd_1drydown == 1)),  # Check if at least one household adopted
+     weight_final_rice = mean(weight_final_rice, na.rm = TRUE)  # Compute mean weight
+   )
> 
> commune_design <- svydesign(ids = ~1, data = commune_adoption, weights = ~weight_final_rice)
> 
> commune_adopter_percentage <- svymean(~has_adopter, commune_design, na.rm = TRUE) * 100
> print(commune_adopter_percentage)
              mean     SE
has_adopter 12.568 0.0104
> # EA-level | At least two dry-downs of 5 days minimum
> df_23_clean <- df_23[!is.na(df_23$weight_final_rice) & !is.na(df_23$awd_2drydown), ]
> df_23_clean$IDCo <- paste (df_23_clean$MATINH, df_23_clean$MAHUYEN, df_23_clean$MAXA, df_23_clean$MADIABAN, sep='-') 
> 
> commune_adoption <- df_23_clean %>%
+   group_by(IDCo) %>%  # Replace `ID` with the commune-level identifier (`MATINH`)
+   summarise(
+     has_adopter = as.numeric(any(awd_2drydown == 1)),  # Check if at least one household adopted
+     weight_final_rice = mean(weight_final_rice, na.rm = TRUE)  # Compute mean weight
+   )
> 
> commune_design <- svydesign(ids = ~1, data = commune_adoption, weights = ~weight_final_rice)
> 
> commune_adopter_percentage <- svymean(~has_adopter, commune_design, na.rm = TRUE) * 100
> print(commune_adopter_percentage)
              mean     SE
has_adopter 10.386 0.0102
> # HH-level | One drydown
> svy_design <- svydesign(ids = ~1, data = df_23_clean, weights = ~weight_final_rice)
> weighted_table <- svytable(~awd_1drydown, design = svy_design)
> weighted_df <- as.data.frame(weighted_table)
> weighted_df$percentage <- round (prop.table(weighted_df$Freq) * 100, 1)
> weighted_df
  awd_1drydown      Freq percentage
1            0 7147839.7       94.6
2            1  408689.1        5.4
> # AWD 2022
> curl_function ("data/raw/VHLSS_2022_Household/datasets/Ho_TTC_M4B11_season_edit.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Ho_TTC_M4B11_season_edit.csv'
Content type 'text/plain; charset=utf-8' length 3290049 bytes (3.1 MB)
downloaded 3.1 MB

> TTC_M4B111 <- read.csv ("data/raw/VHLSS_2022_Household/datasets/Ho_TTC_M4B11_season_edit.csv") # Crop cut module
> 
> TTC_M4B111$Onedd <- ifelse (TTC_M4B111$M4B111_C7 == 1 & TTC_M4B111$M4B111_C8 >=5, TRUE, FALSE)
> TTC_M4B111$Twodd <- ifelse (TTC_M4B111$M4B111_C7 > 1 & TTC_M4B111$M4B111_C8 >=5, TRUE, FALSE)
> 
> # HH-level | One drydown
> svy_design <- svydesign(ids = ~1, data = TTC_M4B111, weights = ~wt45)
> weighted_table <- svytable(~Onedd, design = svy_design)
> weighted_df <- as.data.frame(weighted_table)
> weighted_df$percentage <- round (prop.table(weighted_df$Freq) * 100, 1)
> weighted_df
  Onedd    Freq percentage
1 FALSE 5851406       86.4
2  TRUE  924315       13.6
> 
> # HH-level | At least two dry-downs of 5 days minimum
> svy_design <- svydesign(ids = ~1, data = TTC_M4B111, weights = ~wt45)
> weighted_table2 <- svytable(~Twodd, design = svy_design)
> weighted_df2 <- as.data.frame(weighted_table2)
> weighted_df2$percentage <- round (prop.table(weighted_df2$Freq) * 100, 1)
> weighted_df2
  Twodd    Freq percentage
1 FALSE 4651607       68.6
2  TRUE 2124746       31.4
> # HH-level | One drydown
> svy_design <- svydesign(ids = ~1, data = TTC_M4B111, weights = ~wt45)
> weighted_table <- svytable(~Onedd, design = svy_design)
> weighted_df <- as.data.frame(weighted_table)
> weighted_df$percentage <- round (prop.table(weighted_df$Freq) * 100, 1)
> weighted_df
  Onedd    Freq percentage
1 FALSE 5851406       86.4
2  TRUE  924315       13.6
> 
> rm(list = ls())
> 
> 
> packages <- c("sf", "tidyverse", "haven", "foreign", "readxl", "curl", "httr", "jsonlite", "gridExtra", "reshape2")
> 
> check_and_install_package <- function(package_name) {
+   if (!requireNamespace(package_name, quietly = TRUE)) {
+     install.packages(package_name, dependencies = TRUE)
+     library(package_name, character.only = TRUE)
+   }
+ }
> 
> invisible(sapply(packages, check_and_install_package))
> 
> library(sf)
> library(tidyverse)
> library(haven)
> library(foreign)
> library(readxl)
> library(curl)
> library(httr)
> library(jsonlite)
Warning: package ‘jsonlite’ was built under R version 4.3.3
Attaching package: ‘jsonlite’

The following object is masked from ‘package:purrr’:

    flatten

The following objects are masked from ‘package:rlang’:

    flatten, unbox

> library(gridExtra)
> library(reshape2)
Warning: package ‘reshape2’ was built under R version 4.3.2
Attaching package: ‘reshape2’

The following objects are masked from ‘package:maditr’:

    dcast, melt

The following object is masked from ‘package:tidyr’:

    smiths

> 
> 
> # Access the CGIAR-SPIA repository and download VHLSS 2022 data integrations
> api_url <- "https://api.github.com/repos/CGIAR-SPIA/Vietnam-pre-report-2023/contents/datasets/"
> response <- GET(api_url)
> 
> if (status_code(response) == 200) {
+   content_list <- content(response, as = "text")
+   content_list <- fromJSON(content_list)
+   
+   # Filter out only the file names with .dta extension
+   dta_files <- content_list$name[grepl("\\.dta$", content_list$name)]
+   
+   # Create a list to store .dta file contents
+   Import <- list()
+   
+   # Loop through .dta files and import as binary
+   for (dta_file in dta_files) {
+     dta_url <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/", dta_file)
+     dta_response <- GET(dta_url)
+     
+     if (status_code(dta_response) == 200) {
+       dta_content <- content(dta_response, as = "raw")
+       Import[[dta_file]] <- haven::read_dta(dta_content)
+     } else {
+       cat("Error fetching .dta file content:", dta_file, "\n")
+     }
+   }
+   
+   # Print the list of .dta file contents
+   print(Import)
+ } else {
+   cat("Error fetching folder content\n")
+ }
$Ho_Muc4B11.dta

$Ho_Muc4B111A.dta

$Ho_Muc4B111B.dta

$Ho_Muc4B112.dta

$Ho_TTC_M4B11_season.dta

> 
> 
> # Function to curl data from GitHub----
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> 
> IDProv <- read.csv(curl("https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Provinces_IDs.csv"))
> names(IDProv)[1] = "MATINH"
> for (i in 1:5){Import[[i]] <- Import[[i]] %>% mutate(IDDIABAN=paste(MAXA,MADIABAN,sep = ""))}
> 
> 
> number <- function(i){as.numeric(as.character(unlist(i)))}
> ma=c("MATINH","MAHUYEN","MAXA","MADIABAN")
> for (j in 1:4) {for (i in 1:5) {Import[[i]][,ma[[j]]] <- number(Import[[i]][,ma[[j]]])} }
> Import <- lapply(Import, left_join,IDProv,by="MATINH")
> #Correct wrong sample ID
> Import[[5]]$M4B113_C16[Import[[5]]$IDHO == "20561005009"]="01162"
> Import[[5]]$M4B113_C16[Import[[5]]$IDHO == "20561005011"]="00934"
> Import[[5]]$M4B113_C16[Import[[5]]$IDHO == "20458007015"]="00658"
> Import[[5]]$M4B113_C16[Import[[5]]$IDHO == "20561005005"]="00710"
> Import[[5]]$M4B113_C16[Import[[5]]$IDHO == "05191009009"]="00844"
> Import[[5]]$M4B113_C16[Import[[5]]$IDHO == "13990017013"]="00780"
> Import[[5]]$M4B113_C16[Import[[5]]$IDHO == "13483003011"]="00368"
> #Drop hh with duplicated barcodes
> dup_code=c("20386004013","20386004015","21295017003","21295017009","23395004007","23539001001","23539001013","23539001015","29401018013","29497008005","30034007003","30034007009")
> Import[[5]]$M4B113_C16[Import[[5]]$IDHO %in% dup_code]=""
> #Filter test case
> ID <- Import[[1]] %>% group_by(IDHO) %>% summarise(IDHO=head(IDHO)) %>% unique()
Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in dplyr 1.1.0.
Please use `reframe()` instead.
When switching from `summarise()` to `reframe()`, remember that `reframe()` always returns an ungrouped data frame and adjust accordingly.`summarise()` has grouped output by 'IDHO'. You can override using the `.groups` argument.
> ID0 <- Import[[5]]  %>% mutate(Code_new=number(M4B113_C16)) %>% filter(is.na(Code_new)& IDHO %in% ID$IDHO|Code_new<=1250) %>% select(IDHO,M4B113_C16) %>% unique()
Warning: There was 1 warning in `mutate()`.
ℹ In argument: `Code_new = number(M4B113_C16)`.
Caused by warning in `number()`:
! NAs introduced by coercion
> Import <- lapply(Import, function(x){x %>% filter(IDHO %in% ID0$IDHO)})
> 
> QTL1 <- read.csv(curl("https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/QTL_markers.csv"))
> for (i in 2:ncol(QTL1)) {
+   c0=grep("[-]",QTL1[,i])
+   c1=grep("[+]",QTL1[,i])
+   QTL1[c0,i]=0;QTL1[c1,i]=1
+   cNA=which(!(QTL1[,i] %in% c(0,1)))
+   QTL1[cNA,i]=NA}
> QTL1[,2:ncol(QTL1)]=number(QTL1[,2:ncol(QTL1)])
> 
> recode.gene=function(y){
+   tg=rowSums(QTL1[,y],na.rm = T)*ifelse(rowSums(is.na(QTL1[,y])) == ncol(QTL1[,y]), NA, 1)
+   tg[tg>1]=1
+   return(tg)}
> attach(QTL1)
> QTL1[,2:ncol(QTL1)]=number(QTL1[,2:ncol(QTL1)])
> QTL1=QTL1[,colSums(QTL1,na.rm = T) >0]
> 
> CG= inner_join(Import[[5]] %>% mutate(Code_new=number(M4B113_C16)),QTL1,by=c("Code_new"="M4B113_C16")) %>% filter(!is.na(Code_new)&Code_new<=1250)
> 
> map <- st_read("/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp")
Reading layer `Province_with_Islands' from data source 
  `/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 65 features and 18 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 102.144 ymin: 7.180931 xmax: 117.8355 ymax: 23.39221
Geodetic CRS:  WGS 84
> 
> map$MATINH <- 0
> map$Region <- ""
> t <- rep(NA,65)
> for (i in 1:length(map$ADM1_EN)) {
+   t[[i]] <- ifelse(grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         IDProv$Province_name) %>% length()==0,0,
+                    grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         IDProv$Province_name))
+   map$MATINH[[i]] <- ifelse(t[[i]]==0,NA,IDProv$MATINH[[t[[i]]]])
+   map$Region[[i]] <- ifelse(t[[i]]==0,NA,IDProv$Region[[t[[i]]]])}
> map %>% head() %>% print(width = 120) %>% colnames()
Simple feature collection with 6 features and 20 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 104.7781 ymin: 7.180931 xmax: 117.8355 ymax: 22.7396
Geodetic CRS:  WGS 84
  Shape_Leng Shape_Area           ADM1_EN                 ADM1_VI ADM1_PCODE ADM1_REF ADM1ALT1EN ADM1ALT2EN ADM1ALT1VI
1   8.860833  0.1361665   Paracel Islands                Hoang Sa     VN2001     <NA>       <NA>       <NA>       <NA>
2  32.976015  0.7353349   Spratly Islands               Truong Sa     VN2002     <NA>       <NA>       <NA>       <NA>
3   2.900742  0.2920399          An Giang                An Giang      VN805     <NA>       <NA>       <NA>       <NA>
4   3.419187  0.1630328 Ba Ria - Vung Tau B\xe0 R?a - V?ng T\xe0u      VN717     <NA>       <NA>       <NA>       <NA>
5   4.514786  0.3387481         Bac Giang               B?c Giang      VN221     <NA>       <NA>       <NA>       <NA>
6   4.207590  0.4255983           Bac Kan                 B?c K?n      VN207     <NA>       <NA>       <NA>       <NA>
  ADM1ALT2VI  ADM0_EN  ADM0_VI ADM0_PCODE       date    validOn validTo   layer
1       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
2       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
3       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
4       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
5       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
6       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
                                                                                                                                                                                                                                                            path
1 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
2 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
3 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
4 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
5 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
6 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
                        geometry MATINH       Region
1 MULTIPOLYGON (((112.212 15....     NA         <NA>
2 MULTIPOLYGON (((114.8438 7....     NA         <NA>
3 MULTIPOLYGON (((105.1172 10...     89        6_MRD
4 MULTIPOLYGON (((106.5574 8....     77 5_South East
5 MULTIPOLYGON (((106.163 21....     24       2_NMMA
6 MULTIPOLYGON (((105.7415 22...      6       2_NMMA
 [1] "Shape_Leng" "Shape_Area" "ADM1_EN"    "ADM1_VI"    "ADM1_PCODE" "ADM1_REF"   "ADM1ALT1EN" "ADM1ALT2EN" "ADM1ALT1VI" "ADM1ALT2VI" "ADM0_EN"   
[12] "ADM0_VI"    "ADM0_PCODE" "date"       "validOn"    "validTo"    "layer"      "path"       "geometry"   "MATINH"     "Region"    
> TS <- map %>% filter(ADM1_VI=="Truong Sa") %>% st_geometry()
> cnTS = st_centroid(TS)
> TS_m = (TS-cnTS) * .25 + cnTS + c(-5,0)
> HS <- map %>% filter(ADM1_VI=="Hoang Sa") %>% st_geometry()
> cnHS = st_centroid(HS)
> HS_m =  (HS-cnHS) *.25 + cnHS + c(-2.5,0)
> modified_map <- map %>% filter(!(ADM1_VI %in% c("Truong Sa","Hoang Sa")))
> crs <- st_crs(modified_map)
> TS_map <- map %>% filter(ADM1_VI=="Truong Sa") %>% st_set_geometry(TS_m) %>% st_set_crs(crs)
> HS_map <- map %>% filter(ADM1_VI=="Hoang Sa") %>% st_set_geometry(HS_m) %>% st_set_crs(crs)
> modified_map <- rbind(modified_map,TS_map,HS_map)
> rm(TS,TS_m,TS_map,HS,HS_m,HS_map,cnHS,cnTS,crs)
> 
> #4.2 Crop germ
> #Tab1 = CG %>% group_by(MATINH) %>% summarise(across(any_of(var41),~sum(.x==1,na.rm = T)/n()))
> #Tab1[,2:ncol(Tab1)] <- Tab1[,2:ncol(Tab1)]*100
> 
> # Create dataset by province
> curl_function ("data/processed/Rice.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/Rice.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 119588 bytes (116 KB)
downloaded 116 KB

> dat <- read.csv ("data/processed/Rice.vars.VH24.csv")
> 
> # Fig A. Maps by variety 
> frequency_table <- table(dat$MATINH, dat$Correct_name.DNA2)
> frequency_table_df <- as.data.frame(frequency_table)
> frequency_table_spread <- dcast(frequency_table_df, Var1 ~ ..., value.var = "Freq")
> 
> row_sums <- rowSums(frequency_table_spread[, -1])# % Pctn by province
> FigA <- sweep(frequency_table_spread[, -1], 1, row_sums, "/") * 100
> FigA <- cbind(frequency_table_spread [, 1], FigA)
> 
> names(FigA)[1] <- 'MATINH'
> FigA$MATINH <- as.numeric (as.character(FigA$MATINH))
> FigA$Improved <- 100 - FigA$Landrace
> 
> # Fig B. IRRI-related maps - % IRR_related by province
> curl_function ("data/processed/Rice_Years.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/Rice_Years.csv'
Content type 'text/plain; charset=utf-8' length 5352 bytes
downloaded 5352 bytes

> Years <- read.csv ("data/processed/Rice_Years.csv") 
> dat <- merge (dat, Years, by.x='Correct_name.DNA2', by.y = 'Name', all.x=TRUE) 
> 
> frequency_tableB <- table(dat$MATINH, dat$IRRI_Parentage_dummy)
> 
> row_sums <- rowSums(frequency_tableB) # % Pctn by province
> FigB <- sweep(frequency_tableB, 1, row_sums, "/") * 100
> FigA <- cbind (FigA, FigB [, c(2)]) # Add column to Figure 1
> names (FigA) [32] <- 'IRRI-related Cultivars'
> 
> # Fig C. Map by mean age of release
> dat$Const <- 1 # First filter out if < 5 hhs in the province
> sum_prov <- aggregate (Const ~ MATINH, dat, sum)
> dat <- merge (dat, sum_prov, by='MATINH', all.x=TRUE)
> 
> dat$Year <- (2022 - dat$Year)
> FigC <- aggregate (Year ~ MATINH, dat [dat$Const.y >=5 ,], mean)
> 
> FigA <- merge (FigA, FigC, by = 'MATINH', all.x=TRUE)
> names (FigA)[33] <- 'Mean Age of Improved Cultivars'
> 
> # Fig D, Mean number of year the seed was recycled. 
> FigD <- aggregate (M4B113_C9 ~ MATINH, dat [dat$Correct_name.DNA2 != 'Landrace' ,], mean)
> FigA <- merge (FigA, FigD, by = 'MATINH', all.x=TRUE)
> names (FigA)[34] <- 'Mean Duration of Recycling for Improved Cultivars'
> # NOte: Too many NAs to be meaningful
> 
> # Reorder and drop varieties <5 samples nationally 
> FigA <- FigA [, - c(21,10,12,13,22,29,19,20,23)] # "Jasmine 85", "LH12", "LTH31", "RVT", "VN10", "OM576", "OM6162", "OM73417", "SH14")
> FigA <- FigA [, c(1,22,23,24,25,2:21)] # Order of maps as they'll appear
> 
> var41= names(FigA)[2:ncol(FigA)]
> 
> modified_map <- modified_map %>% left_join(FigA,by="MATINH")
> M.CG <- list()
> for(i in 1:length(var41)) {
+   M.CG[[i]] <- modified_map %>% ggplot() + aes(fill=.data[[var41[i]]])+geom_sf() + scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", na.value="#dbd0d0", space = "Lab", name="In %")  +  ggtitle(var41[[i]]) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), linewidth = 0.1, color = "black", fill = NA)+ theme(axis.text.x=element_blank(),axis.text.y=element_blank(),axis.ticks = element_blank())
+ }
> 
> 
> M.CG[[1]] <- modified_map %>% ggplot() + aes(fill=.data[[var41[1]]])+ geom_sf() + scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", na.value="#dbd0d0", space = "Lab", name="In %", limits = c(0, 100))  +  ggtitle('(a) Improved varietal adoption') +
+       theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), linewidth = 0.1, color = "black", fill = NA)+ theme(axis.text.x=element_blank(),axis.text.y=element_blank(),axis.ticks = element_blank())
> 
> M.CG[[2]]  <- modified_map %>% ggplot() + aes(fill=.data[[var41[2]]])+ geom_sf() + scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", na.value="#dbd0d0", space = "Lab", name="In %", limits = c(0, 100))  +  ggtitle('(b) IRRI-related varietal adoption') +
+     theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), linewidth = 0.1, color = "black", fill = NA)+ theme(axis.text.x=element_blank(),axis.text.y=element_blank(),axis.ticks = element_blank())
> 
> M.CG[[3]] <- modified_map %>% ggplot() + aes(fill=.data[[var41[3]]])+ geom_sf() + scale_fill_gradient2(midpoint = 10, low = "#1c4c6f", mid = "#9ecae1", high = "#f7efef", na.value="#dbd0d0", space = "Lab", name="In Years", breaks = seq(0, 20, by =5 ), limits = c(0, 20))  +  ggtitle('(c) Average age of improved varieties') +
+    theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), linewidth = 0.1, color = "black", fill = NA)+ theme(axis.text.x=element_blank(),axis.text.y=element_blank(), axis.ticks = element_blank())
> 
> 
> pdf("Figs Rice varieties.pdf")
> for (i in 1:23) { # Modify HERE 22 by the number of maps to plot
+     print(M.CG[[i]])
+ }
> dev.off()
null device 
          1 
> 
> 
> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> df_23 <- read.csv("data/processed/VH23_data.csv")
> curl_function ("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv'
Content type 'text/plain; charset=utf-8' length 1799 bytes
downloaded 1799 bytes

> Provinces_IDs <- read.csv("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv")
> 
> 
> Provinces_IDs$MATINH <- as.numeric (Provinces_IDs$MATINH); df_23$MATINH <- as.numeric (df_23$MATINH)
> df_23 <- merge (df_23, Provinces_IDs, all.x =TRUE)
> 
> df_23 <- df_23 %>%
+   filter(!is.na(awd_1drydown) & !is.na(weight_final_rice)) %>% 
+   group_by(MATINH) %>%
+   summarize(
+     Pctn.1dd = sum((awd_1drydown == 1) * weight_final_rice) / sum(weight_final_rice) * 100,
+     Pctn.2dd = sum((awd_2drydown == 1) * weight_final_rice) / sum(weight_final_rice) * 100
+ )
> 
> # Merge back with the original dataset if needed
> df_23 <- df_23 %>%
+   left_join(df_23, by = "MATINH")
> 
> 
> modified_map <- modified_map %>% left_join(df_23,by="MATINH")
> 
> AWD.p1 <- modified_map %>%
+   ggplot() + 
+   aes(fill = Pctn.1dd.x) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 15, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 30)) +  
+   ggtitle('(a) One drydown') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> 
> AWD.p2 <- modified_map %>%
+   ggplot() + 
+   aes(fill = Pctn.2dd.x) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 15, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 30)) +  
+   ggtitle('(b) At least two drydowns') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> 
> curl_function ("data/processed/VH22_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_data.csv'
Content type 'text/plain; charset=utf-8' length 2257649 bytes (2.2 MB)
downloaded 2.2 MB

> df_22 <- read.csv("data/processed/VH22_data.csv")
> 
> 
> Provinces_IDs$MATINH <- as.numeric (Provinces_IDs$MATINH); df_22$MATINH <- as.numeric (df_22$MATINH)
> df_22 <- merge (df_22, Provinces_IDs, all.x =TRUE)
> 
> df_22 <- df_22 %>%
+   filter(!is.na(Saltol) & !is.na(weight_rice_DNA)) %>% 
+   group_by(MATINH) %>%
+   summarize(
+     Pctn.saltol = sum((Saltol == 1) * weight_rice_DNA) / sum(weight_rice_DNA) * 100
+ )
> 
> # Merge back with the original dataset if needed
> df_22 <- df_22 %>%
+   left_join(df_22, by = "MATINH")
> 
> modified_map <- modified_map %>% left_join(df_22,by="MATINH")
> 
> 
> Saltol.map <- modified_map %>%
+   ggplot() + 
+   aes(fill = Pctn.saltol.x) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> Saltol.map
> AWD.p2
> 
> # Figure 35: Household adoption rates at the province-level for (a) Certified seeds, (b) 1R: Seed rate, (c) 2R: Fertilizer, (d) 3R: Pesticide 
> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> df <- read.csv("data/processed/VH23_data.csv")
> 
> 
> Provinces_IDs$MATINH <- as.numeric (Provinces_IDs$MATINH); df$MATINH <- as.numeric (df$MATINH)
> df <- merge (df, Provinces_IDs, all.x =TRUE)
> 
> df <- df %>%
+   #filter(!is.na(weight_final_rice)) %>% 
+   group_by(MATINH) %>%
+   summarize(
+    lenient_1m = sum((lenient_1m == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     lenient_1r = sum((d_1m5r_seed_120kg == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     lenient_2r = sum((lenient_2r == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     lenient_3r = sum((lenient_3r == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100
+ )
> 
> modified_map <- modified_map %>% left_join(df,by="MATINH")
> 
> OneM <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_1m) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(a) Certified seeds') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> R1 <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_1r) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(a) 1R: Seed rate') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> R2 <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_2r) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(c) 2R: Fertilizer') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> 
> R3 <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_3r) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(d) 3R: Pesticide') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> 
> OneM <- OneM + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> R1 <- R1 + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> R2 <- R2 + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> R3 <- R3 + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> 
> 
> a <- grid.arrange(OneM, R1, R2, R3, nrow = 2, ncol = 2)
> 
> ggsave (a, filename = "Output/Fig38.spatial.png", dpi = 1024)
Saving 9 x 7.2 in image
> 
> 
> 
> 
> # Figure 33: Vietnamese provinces referencing 1M5R/3R3G components in agricultural plans  ----
> # Function to plot maps: ----
> create_plot <- function(modified_map, fill_var, title) {
+   ggplot(modified_map) + 
+     aes_string(fill = paste0("factor(", fill_var, ")")) +  # Convert fill_var to factor
+     geom_sf() + 
+     scale_fill_manual(
+       values = c("0" = "#ffffff", "1" = "#1c4c6f"),  # Define colors for each factor level
+       na.value = "#dbd0d0",                          # Color for NA values
+       name = NULL,                                    # Remove legend title
+       breaks = c("1", "0"),                           # Define which levels to display
+       labels = c("Yes", "No")                         # Custom labels for the legend
+     ) +  
+     ggtitle(title) +
+     theme(
+       plot.title = element_text(size = 12, hjust = 0.5),  # Increase title size to 14
+       axis.text.x = element_blank(), 
+       axis.text.y = element_blank(),
+       legend.title = element_blank(), # Ensure legend title is removed
+       axis.ticks = element_blank() 
+     ) +
+     geom_rect(
+       aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+       linewidth = 0.1, color = "black", fill = NA
+     ) +
+     geom_rect(
+       aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+       linewidth = 0.1, color = "black", fill = NA
+     )
+ }
> curl_function ("data/processed/ag_plan_1m5r_recode.xlsx")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/ag_plan_1m5r_recode.xlsx'
Content type 'application/octet-stream' length 13602 bytes (13 KB)
downloaded 13 KB

> ag_plan <- read_excel ("data/processed/ag_plan_1m5r_recode.xlsx") #load ag_plan data
> 
> ag_plan <- ag_plan %>%
+   mutate (package = case_when (mention_1M5R == 1 | mention_3R3G == 1 ~ 1,
+                                mention_1M5R == 0 & mention_3R3G == 0 ~ 0,
+                                TRUE ~ NA))  #recode variable
> 
> 
> Provinces_IDs$MATINH <- as.numeric (Provinces_IDs$MATINH); ag_plan$MATINH <- as.numeric (ag_plan$MATINH)
> ag_plan <- merge (ag_plan, Provinces_IDs, all.x =TRUE)
> 
> 
> ag_plan <- ag_plan %>%
+   group_by(Province, Year) %>%
+   summarize (
+     MATINH = first(MATINH),
+     package_prv = case_when (mean(package) > 0 ~ 1,
+                              mean(package) == 0 ~ 0),
+     certified_prv = case_when (mean(certified) > 0 ~ 1,
+                                mean(certified) == 0 ~ 0),
+     seeding_prv = case_when (mean(seeding) > 0 ~ 1,
+                              mean(seeding) == 0 ~ 0),
+     pesticide_prv = case_when (mean(pesticide) > 0 ~ 1,
+                                mean(pesticide) == 0 ~ 0),
+     fertilizer_prv = case_when (mean(fertilizer) > 0 ~ 1,
+                                 mean(fertilizer) == 0 ~ 0)) %>%
+   mutate (n = n()) %>%
+   pivot_wider (names_from = Year, values_from = c(package_prv, certified_prv, seeding_prv, pesticide_prv, fertilizer_prv)) %>%
+   mutate (mention_package = case_when (mean(c_across(starts_with("package")), na.rm = TRUE) > 0 ~ 1, #if they mention at least once
+                                        TRUE ~ 0),
+           mention_certified = case_when (mean(c_across(starts_with("certified")), na.rm = TRUE) > 0 ~ 1,
+                                          TRUE ~ 0),
+           mention_seeding = case_when (mean(c_across(starts_with("seeding")), na.rm = TRUE) > 0 ~ 1,
+                                        TRUE ~ 0),
+           mention_pest = case_when (mean(c_across(starts_with("pest")), na.rm = TRUE) > 0 ~ 1,
+                                     TRUE ~ 0),
+           mention_fert = case_when (mean(c_across(starts_with("fert")), na.rm = TRUE) > 0 ~ 1,
+                                     TRUE ~ 0)) %>%
+   mutate (repeat_package = case_when (mean(c_across(starts_with("package")), na.rm = TRUE) > 1/n ~ 1, #if they repeatedly mention (more than once)
+                                       TRUE ~ 0),
+           repeat_certified = case_when (mean(c_across(starts_with("certified")), na.rm = TRUE) > 1/n ~ 1,
+                                         TRUE ~ 0),
+           repeat_seeding = case_when (mean(c_across(starts_with("seeding")), na.rm = TRUE) > 1/n ~ 1,
+                                       TRUE ~ 0),
+           repeat_pest = case_when (mean(c_across(starts_with("pest")), na.rm = TRUE) > 1/n ~ 1,
+                                    TRUE ~ 0),
+           repeat_fert = case_when (mean(c_across(starts_with("fert")), na.rm = TRUE) > 1/n ~ 1,
+                                    TRUE ~ 0)) 
`summarise()` has grouped output by 'Province'. You can override using the `.groups` argument.
> 
> modified_map <- modified_map %>% left_join(ag_plan,by="MATINH")
> 
> plot_info <- list(
+   list(title = "Certified seeds (Mentioned)", fill_var = "mention_certified"),
+   list(title = "Certified seeds (Repeated)", fill_var = "repeat_certified"),
+   list(title = "1R: Seed rate (Mentioned)", fill_var = "mention_seeding"),
+   list(title = "1R: Seed rate (Repeated)", fill_var = "repeat_seeding"),
+   list(title = "2R: Fertilizer (Mentioned)", fill_var = "mention_fert"),
+   list(title = "2R: Fertilizer (Repeated)", fill_var = "repeat_fert"),
+   #list(title = "3R: Pesticide (Mentioned)", fill_var = "mention_pest"),
+   #list(title = "3R: Pesticide (Repeated)", fill_var = "repeat_pest"),
+   list(title = "3R3G/1M5R (Mentioned)", fill_var = "mention_package"),
+   list(title = "3R3G/1M5R (Repeated)", fill_var = "repeat_package")
+ )
> 
> 
> 
> # Create a list of plots
> plots <- lapply(plot_info, function(info) {
+   create_plot(modified_map, info$fill_var, info$title)
+ })
> 
> # gridExtra::grid.arrange(grobs = c(plots[1], plots[2], plots[3], plots[4], ncol = 2)
> map_ag_plan <- gridExtra::arrangeGrob(grobs = plots, ncol = 2)
> 
> map_ag_plan
TableGrob (4 x 2) "arrange": 8 grobs
> 
> ggsave (plot = map_ag_plan,
+         "Output/Fig36.png", 
+         width = 6, height = 15, dpi = 1024)
> 
> 
> 
> OneM <- modified_map %>%
+   ggplot() + 
+   aes(fill = mention_certified) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(a) Certified seeds') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank(),
+         axis.ticks = element_blank())
> 
> 
> ag_plan <- ag_plan %>%
+   select (starts_with(c("mention", "repeat")))%>%
+   full_join (Provinces_IDs, by = c("Province" = "Province_name")) 
Adding missing grouping variables: `Province`
> 
> 
> 
> OneM
> curl_function ("data/processed/ag_plan_1m5r_recode.xlsx")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/ag_plan_1m5r_recode.xlsx'
Content type 'application/octet-stream' length 13602 bytes (13 KB)
downloaded 13 KB

> ag_plan <- read_excel ("data/processed/ag_plan_1m5r_recode.xlsx") #load ag_plan data
> 
> # Add MATINH to the original excel file
> 
> ag_plan <- ag_plan %>%
+   mutate (package = case_when (mention_1M5R == 1 | mention_3R3G == 1 ~ 1,
+                                mention_1M5R == 0 & mention_3R3G == 0 ~ 0,
+                                TRUE ~ NA))  #recode variable
> ag_plan$MATINH <- as.character(ag_plan$MATINH)
> 
> ag_plan <- ag_plan %>%
+   left_join (province)
Error: object 'province' not found
> ag_plan
# A tibble: 108 × 11
   MATINH Province  Crop   Year mention_1M5R mention_3R3G certified seeding fertilizer pesticide package
   <chr>  <chr>     <chr> <dbl>        <dbl>        <dbl>     <dbl>   <dbl>      <dbl>     <dbl>   <dbl>
 1 89     An Giang  W-S    2023            1            1         0       1          0         1       1
 2 89     An Giang  W-S    2022            1            1         0       1          0         1       1
 3 89     An Giang  W-S    2021            1            1         0       1          0         1       1
 4 24     Bac Giang Mua    2023            0            1         0       0          1         1       1
 5 24     Bac Giang Mua    2021            0            1         0       0          1         1       1
 6 24     Bac Giang Mua    2022            0            1         0       0          1         1       1
 7 95     Bac Lieu  S-A    2023            1            0         1       0          1         1       1
 8 95     Bac Lieu  S-A    2022            1            0         1       0          1         1       1
 9 95     Bac Lieu  W-S    2021            1            0         1       0          1         1       1
10 27     Bac Ninh  W-S    2022            0            0         0       0          1         1       0
# ℹ 98 more rows
# ℹ Use `print(n = ...)` to see more rows
> # Recode agriculture plan:----
> 
> ag_plan <- ag_plan %>%
+   group_by(Province, Year) %>%
+   summarize (
+     MATINH = first(MATINH),
+     package_prv = case_when (mean(package) > 0 ~ 1,
+                              mean(package) == 0 ~ 0),
+     certified_prv = case_when (mean(certified) > 0 ~ 1,
+                                mean(certified) == 0 ~ 0),
+     seeding_prv = case_when (mean(seeding) > 0 ~ 1,
+                              mean(seeding) == 0 ~ 0),
+     pesticide_prv = case_when (mean(pesticide) > 0 ~ 1,
+                                mean(pesticide) == 0 ~ 0),
+     fertilizer_prv = case_when (mean(fertilizer) > 0 ~ 1,
+                                 mean(fertilizer) == 0 ~ 0)) %>%
+   mutate (n = n()) %>%
+   pivot_wider (names_from = Year, values_from = c(package_prv, certified_prv, seeding_prv, pesticide_prv, fertilizer_prv)) %>%
+   mutate (mention_package = case_when (mean(c_across(starts_with("package")), na.rm = TRUE) > 0 ~ 1, #if they mention at least once
+                                        TRUE ~ 0),
+           mention_certified = case_when (mean(c_across(starts_with("certified")), na.rm = TRUE) > 0 ~ 1,
+                                          TRUE ~ 0),
+           mention_seeding = case_when (mean(c_across(starts_with("seeding")), na.rm = TRUE) > 0 ~ 1,
+                                        TRUE ~ 0),
+           mention_pest = case_when (mean(c_across(starts_with("pest")), na.rm = TRUE) > 0 ~ 1,
+                                     TRUE ~ 0),
+           mention_fert = case_when (mean(c_across(starts_with("fert")), na.rm = TRUE) > 0 ~ 1,
+                                     TRUE ~ 0)) %>%
+   mutate (repeat_package = case_when (mean(c_across(starts_with("package")), na.rm = TRUE) > 1/n ~ 1, #if they repeatedly mention (more than once)
+                                       TRUE ~ 0),
+           repeat_certified = case_when (mean(c_across(starts_with("certified")), na.rm = TRUE) > 1/n ~ 1,
+                                         TRUE ~ 0),
+           repeat_seeding = case_when (mean(c_across(starts_with("seeding")), na.rm = TRUE) > 1/n ~ 1,
+                                       TRUE ~ 0),
+           repeat_pest = case_when (mean(c_across(starts_with("pest")), na.rm = TRUE) > 1/n ~ 1,
+                                    TRUE ~ 0),
+           repeat_fert = case_when (mean(c_across(starts_with("fert")), na.rm = TRUE) > 1/n ~ 1,
+                                    TRUE ~ 0)) 
`summarise()` has grouped output by 'Province'. You can override using the `.groups` argument.
> 
> modified_map <- modified_map %>% left_join(ag_plan,by="MATINH")
Error in `sf_column %in% names(g)`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <double>.
ℹ `y$MATINH` is a <character>.
Backtrace:
 1. modified_map %>% left_join(ag_plan, by = "MATINH")
 7. dplyr:::left_join.data.frame(., ag_plan, by = "MATINH")
> rm(list = ls()) #start clean
> #Load packages
> library (this.path)
> library (tidyverse)
> library (ggplot2)
> library (gridExtra)
> library (fastDummies)
> library (ggpolypath)
> library (eulerr)
> library (readxl)
> library (stringr)
> library (flextable)
> library (sf)
> library (curl)
> library (survey)
> library (haven)
> library (httr)
> 
> 
> #Set working directory to the location of this file
> setwd(this.path::here())   
> 
> 
> #Customized function to format ID
> 
> format_ID <- function(df, columns, widths, pad_char = "0") {
+   # Loop through each column and its corresponding width
+   for (i in seq_along(columns)) {
+     column <- columns[i]
+     width <- widths[i]
+     
+     # Pad the column with the specified width and character
+     df[[column]] <- str_pad(df[[column]], width = width, pad = pad_char)
+   }
+   return(df)
+ }
> # Function to curl data from GitHub----
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> 
> 
> # Prepare layer maps ----
> # Load provincial data: 
> province <- read.csv(curl("https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Provinces_IDs.csv")) %>%
+   select (-c("CSMAP"))#load provincial data
> 
> province$Province_name <- str_remove(province$Province_name, "Tinh ")  #remove "Province" word
> province$Province_name <- str_remove(province$Province_name, "Thanh pho ")  #remove "City" word
> province$Province_name[province$Province_name=="Ba Ria Vung Tau"] <- "Ba Ria - Vung Tau" #rewording
> 
> province$MATINH <- str_pad(province$MATINH, width = 2, pad = 0) # format ID of provincial data
> 
> 
> map <- st_read("/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp")
Reading layer `Province_with_Islands' from data source 
  `/vsicurl/https://raw.githubusercontent.com/CGIAR-SPIA/Vietnam-pre-report-2023/main/datasets/Shape_file/Shape_file/Province_with_Islands.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 65 features and 18 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 102.144 ymin: 7.180931 xmax: 117.8355 ymax: 23.39221
Geodetic CRS:  WGS 84
> 
> map$MATINH <- 0
> map$Region <- ""
> t <- rep(NA,65)
> for (i in 1:length(map$ADM1_EN)) {
+   t[[i]] <- ifelse(grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         province$Province_name) %>% length()==0,0,
+                    grep(gsub('( -)','',gsub('( city)','',map$ADM1_EN[[i]])),
+                         province$Province_name))
+   map$MATINH[[i]] <- ifelse(t[[i]]==0,NA,province$MATINH[[t[[i]]]])
+   map$Region[[i]] <- ifelse(t[[i]]==0,NA,province$Region[[t[[i]]]])}
> map %>% head() %>% print(width = 120) %>% colnames()
Simple feature collection with 6 features and 20 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 104.7781 ymin: 7.180931 xmax: 117.8355 ymax: 22.7396
Geodetic CRS:  WGS 84
  Shape_Leng Shape_Area           ADM1_EN                 ADM1_VI ADM1_PCODE ADM1_REF ADM1ALT1EN ADM1ALT2EN ADM1ALT1VI
1   8.860833  0.1361665   Paracel Islands                Hoang Sa     VN2001     <NA>       <NA>       <NA>       <NA>
2  32.976015  0.7353349   Spratly Islands               Truong Sa     VN2002     <NA>       <NA>       <NA>       <NA>
3   2.900742  0.2920399          An Giang                An Giang      VN805     <NA>       <NA>       <NA>       <NA>
4   3.419187  0.1630328 Ba Ria - Vung Tau B\xe0 R?a - V?ng T\xe0u      VN717     <NA>       <NA>       <NA>       <NA>
5   4.514786  0.3387481         Bac Giang               B?c Giang      VN221     <NA>       <NA>       <NA>       <NA>
6   4.207590  0.4255983           Bac Kan                 B?c K?n      VN207     <NA>       <NA>       <NA>       <NA>
  ADM1ALT2VI  ADM0_EN  ADM0_VI ADM0_PCODE       date    validOn validTo   layer
1       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
2       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
3       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
4       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
5       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
6       <NA> Viet Nam Vi?t Nam         VN 2019-10-01 2020-01-03    <NA> Clipped
                                                                                                                                                                                                                                                            path
1 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
2 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
3 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
4 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
5 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
6 MultiPolygon?crs=EPSG:4326&field=Shape_Leng:double(18,11)&field=Shape_Area:double(18,11)&field=ADM1_EN:string(50,0)&field=ADM1_VI:string(50,0)&field=ADM1_PCODE:string(50,0)&field=ADM1_REF:string(50,0)&field=ADM1ALT1EN:string(50,0)&field=ADM1ALT2EN:string
                        geometry MATINH Region
1 MULTIPOLYGON (((112.212 15....   <NA>   <NA>
2 MULTIPOLYGON (((114.8438 7....   <NA>   <NA>
3 MULTIPOLYGON (((105.1172 10...     89  6_MRD
4 MULTIPOLYGON (((106.5574 8....   <NA>   <NA>
5 MULTIPOLYGON (((106.163 21....     24 2_NMMA
6 MULTIPOLYGON (((105.7415 22...     06 2_NMMA
 [1] "Shape_Leng" "Shape_Area" "ADM1_EN"    "ADM1_VI"    "ADM1_PCODE" "ADM1_REF"   "ADM1ALT1EN"
 [8] "ADM1ALT2EN" "ADM1ALT1VI" "ADM1ALT2VI" "ADM0_EN"    "ADM0_VI"    "ADM0_PCODE" "date"      
[15] "validOn"    "validTo"    "layer"      "path"       "geometry"   "MATINH"     "Region"    
> TS <- map %>% filter(ADM1_VI=="Truong Sa") %>% st_geometry()
> cnTS = st_centroid(TS)
> TS_m = (TS-cnTS) * .25 + cnTS + c(-5,0)
> HS <- map %>% filter(ADM1_VI=="Hoang Sa") %>% st_geometry()
> cnHS = st_centroid(HS)
> HS_m =  (HS-cnHS) *.25 + cnHS + c(-2.5,0)
> modified_map <- map %>% filter(!(ADM1_VI %in% c("Truong Sa","Hoang Sa")))
> crs <- st_crs(modified_map)
> TS_map <- map %>% filter(ADM1_VI=="Truong Sa") %>% st_set_geometry(TS_m) %>% st_set_crs(crs)
> HS_map <- map %>% filter(ADM1_VI=="Hoang Sa") %>% st_set_geometry(HS_m) %>% st_set_crs(crs)
> modified_map <- rbind(modified_map,TS_map,HS_map)
> rm(TS,TS_m,TS_map,HS,HS_m,HS_map,cnHS,cnTS,crs, map, i, t)
> 
> 
> 
> 
> # Function to plot maps: ----
> create_plot <- function(modified_map, fill_var, title) {
+   ggplot(modified_map) + 
+     aes_string(fill = paste0("factor(", fill_var, ")")) +  # Convert fill_var to factor
+     geom_sf() + 
+     scale_fill_manual(
+       values = c("0" = "#ffffff", "1" = "#1c4c6f"),  # Define colors for each factor level
+       na.value = "#dbd0d0",                          # Color for NA values
+       name = NULL,                                    # Remove legend title
+       breaks = c("1", "0"),                           # Define which levels to display
+       labels = c("Yes", "No")                         # Custom labels for the legend
+     ) +  
+     ggtitle(title) +
+     theme(
+       plot.title = element_text(size = 12, hjust = 0.5),  # Increase title size to 14
+       axis.text.x = element_blank(), 
+       axis.text.y = element_blank(),
+       legend.title = element_blank(), # Ensure legend title is removed
+       axis.ticks = element_blank() 
+     ) +
+     geom_rect(
+       aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+       linewidth = 0.1, color = "black", fill = NA
+     ) +
+     geom_rect(
+       aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+       linewidth = 0.1, color = "black", fill = NA
+     )
+ }
> 
> 
> # Function to plot by component:----
> plot_component_fn <- function(data, footnote) {
+   plot <- data %>%
+     ggplot() +
+     geom_bar(aes(x=position, y=adoption, fill = group), stat="identity", alpha=0.8) +
+     labs (caption = footnote,
+           fill = "") +
+     xlab("") +
+     ylab ("(%)") +
+     theme (axis.text.y = element_text (size = 12),
+            legend.text = element_text(size = 12),
+            legend.title = element_text(size = 15),
+            title = element_text (size = 14),
+            axis.text.x = element_blank (),
+            axis.ticks.x = element_blank(),
+            legend.position = "bottom",
+            strip.text = element_text (size = 12),
+            plot.caption = element_text (hjust = 0)) +
+     scale_y_continuous(breaks = seq(0, 100, 20),
+                        expand = c(0,1)) +
+     scale_fill_manual (labels = c("Strict", "Lenient"),
+                        values = c("#006D5B", "#FFBF00")) +
+     facet_wrap(~component, scales = "fixed") 
+   
+   return (plot)
+ }
> 
> 
> # Load and merge data----
> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> 
> df_23 <- read.csv ("data/processed/VH23_data.csv") %>%
+   select (-c (starts_with("mech"), starts_with("straw"), Genotype_rec,
+               Genotype, CMD, DMC, CIAT.related, starts_with("Strain"), SWCP, SWCP_confirmed, weight_cass, 
+               weight_gift, weight_coffee)) %>%
+   distinct() 
> 
> df_23 <- format_ID(df_23, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), widths = c(2, 3, 5, 3, 3) ) #format admin-ID
> 
> df_23$IDHO <- paste0 (df_23$MAXA, df_23$MADIABAN, df_23$HOSO) #unique household ID
> 
> 
> 
> 
> curl_function("data/raw/VHLSS_2023_Household/Final/Final_1M5R/final_1m5r.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Final/Final_1M5R/final_1m5r.csv'
Content type 'text/plain; charset=utf-8' length 6039601 bytes (5.8 MB)
downloaded 5.8 MB

> df_1m5r <- read.csv ("data/raw/VHLSS_2023_Household/Final/Final_1M5R/final_1m5r.csv") %>%
+   select (c(MATINH:IDHO, rice_name, ws_rate_ha, seed_rate_duration, pre_rate_ha, KYDIEUTRA, 
+             ws_method, n_app_fert, sum_fert, 
+             d_1m5r_nitrogen_2app, d_1m5r_nitrogen_110kg, d_1m5r_nitrogen_3app, d_1m5r_nitrogen_100kg,
+             n_app_drug, n_app_insect_fungi, starts_with("n_drug_mix_"),
+             d_1m5r_pest_3app, d_1m5r_pest_6app, d_1m5r_pest_40d_sowing, 
+             d_1m5r_pest_flowering, d_1m5r_pest_20d_harvest,
+             starts_with ("purpose")))
> 
> df_1m5r <- format_ID(df_1m5r, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), widths = c(2, 3, 5, 3, 3) ) 
> 
> df_1m5r$IDHO <- paste0 (df_1m5r$MAXA, df_1m5r$MADIABAN, df_1m5r$HOSO)
> 
> dup_id <- df_1m5r[which(duplicated(df_1m5r$IDHO)),]$IDHO
> 
> df_1m5r <- df_1m5r %>%
+   filter (!IDHO %in% dup_id | (IDHO %in% dup_id & KYDIEUTRA == 4)) #there are some households surveyed in two quarters, but quarter 4 seems to be purposefully collected, so we keep values of Q4
> 
> 
> df <- left_join (df_23, df_1m5r) #join two dataset
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN, HOSO, IDHO)`
> 
> 
> df <- df %>%
+   rename (c("strict_1m" = "d_1m5r_certified",
+             "strict_1r" = "d_1m5r_seed_100kg",
+             "lenient_1r" = "d_1m5r_seed_120kg"))  #rename vars
> 
> df$MATINH <- str_pad(df$MATINH, width = 2, pad = 0) # format ID of newly merged data
> 
> 
> df <- left_join (df, province, by = "MATINH") %>%
+   relocate (c("Province_name", "Region"), .after = MATINH) #join two datasets
> 
> 
> 
> # Figure 34: Vietnamese provinces referencing 1M5R/3R3G components in provincial agriculture plans  ----
> curl_function ("data/processed/ag_plan_1m5r_recode.xlsx")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/ag_plan_1m5r_recode.xlsx'
Content type 'application/octet-stream' length 13602 bytes (13 KB)
downloaded 13 KB

> ag_plan <- read_excel ("data/processed/ag_plan_1m5r_recode.xlsx") #load ag_plan data
> 
> # Add MATINH to the original excel file
> 
> ag_plan <- ag_plan %>%
+   mutate (package = case_when (mention_1M5R == 1 | mention_3R3G == 1 ~ 1,
+                                mention_1M5R == 0 & mention_3R3G == 0 ~ 0,
+                                TRUE ~ NA))  #recode variable
> ag_plan$MATINH <- as.character(ag_plan$MATINH)
> 
> ag_plan <- ag_plan %>%
+   left_join (province)
Joining with `by = join_by(MATINH)`
> 
> 
> 
> 
> ag_plan <- ag_plan %>%
+   group_by(Province, Year) %>%
+   summarize (
+     MATINH = first(MATINH),
+     package_prv = case_when (mean(package) > 0 ~ 1,
+                              mean(package) == 0 ~ 0),
+     certified_prv = case_when (mean(certified) > 0 ~ 1,
+                                mean(certified) == 0 ~ 0),
+     seeding_prv = case_when (mean(seeding) > 0 ~ 1,
+                              mean(seeding) == 0 ~ 0),
+     pesticide_prv = case_when (mean(pesticide) > 0 ~ 1,
+                                mean(pesticide) == 0 ~ 0),
+     fertilizer_prv = case_when (mean(fertilizer) > 0 ~ 1,
+                                 mean(fertilizer) == 0 ~ 0)) %>%
+   mutate (n = n()) %>%
+   pivot_wider (names_from = Year, values_from = c(package_prv, certified_prv, seeding_prv, pesticide_prv, fertilizer_prv)) %>%
+   mutate (mention_package = case_when (mean(c_across(starts_with("package")), na.rm = TRUE) > 0 ~ 1, #if they mention at least once
+                                        TRUE ~ 0),
+           mention_certified = case_when (mean(c_across(starts_with("certified")), na.rm = TRUE) > 0 ~ 1,
+                                          TRUE ~ 0),
+           mention_seeding = case_when (mean(c_across(starts_with("seeding")), na.rm = TRUE) > 0 ~ 1,
+                                        TRUE ~ 0),
+           mention_pest = case_when (mean(c_across(starts_with("pest")), na.rm = TRUE) > 0 ~ 1,
+                                     TRUE ~ 0),
+           mention_fert = case_when (mean(c_across(starts_with("fert")), na.rm = TRUE) > 0 ~ 1,
+                                     TRUE ~ 0)) %>%
+   mutate (repeat_package = case_when (mean(c_across(starts_with("package")), na.rm = TRUE) > 1/n ~ 1, #if they repeatedly mention (more than once)
+                                       TRUE ~ 0),
+           repeat_certified = case_when (mean(c_across(starts_with("certified")), na.rm = TRUE) > 1/n ~ 1,
+                                         TRUE ~ 0),
+           repeat_seeding = case_when (mean(c_across(starts_with("seeding")), na.rm = TRUE) > 1/n ~ 1,
+                                       TRUE ~ 0),
+           repeat_pest = case_when (mean(c_across(starts_with("pest")), na.rm = TRUE) > 1/n ~ 1,
+                                    TRUE ~ 0),
+           repeat_fert = case_when (mean(c_across(starts_with("fert")), na.rm = TRUE) > 1/n ~ 1,
+                                    TRUE ~ 0)) 
`summarise()` has grouped output by 'Province'. You can override using the `.groups` argument.
> 
> modified_map <- modified_map %>% left_join(ag_plan,by="MATINH")
> 
> 
> 
> 
> plot_info <- list(
+   list(title = "Certified seeds (Mentioned)", fill_var = "mention_certified"),
+   list(title = "Certified seeds (Repeated)", fill_var = "repeat_certified"),
+   list(title = "1R: Seed rate (Mentioned)", fill_var = "mention_seeding"),
+   list(title = "1R: Seed rate (Repeated)", fill_var = "repeat_seeding"),
+   list(title = "2R: Fertilizer (Mentioned)", fill_var = "mention_fert"),
+   list(title = "2R: Fertilizer (Repeated)", fill_var = "repeat_fert"),
+   #list(title = "3R: Pesticide (Mentioned)", fill_var = "mention_pest"),
+   #list(title = "3R: Pesticide (Repeated)", fill_var = "repeat_pest"),
+   list(title = "3R3G/1M5R (Mentioned)", fill_var = "mention_package"),
+   list(title = "3R3G/1M5R (Repeated)", fill_var = "repeat_package")
+ )
> 
> 
> plots <- lapply(plot_info, function(info) {
+   create_plot(modified_map, info$fill_var, info$title) +
+     theme(axis.ticks = element_blank()) # Remove axis ticks
+ })
> 
> # gridExtra::grid.arrange(grobs = c(plots[1], plots[2], plots[3], plots[4], ncol = 2)
> map_ag_plan <- gridExtra::arrangeGrob(grobs = plots, ncol = 2)
> 
> ggsave (plot = map_ag_plan,
+         "Output/Fig_34.png", 
+         width = 6, height = 15, dpi = 1024)
> map_ag_plan
TableGrob (4 x 2) "arrange": 8 grobs
  z     cells    name           grob
1 1 (1-1,1-1) arrange gtable[layout]
2 2 (1-1,2-2) arrange gtable[layout]
3 3 (2-2,1-1) arrange gtable[layout]
4 4 (2-2,2-2) arrange gtable[layout]
5 5 (3-3,1-1) arrange gtable[layout]
6 6 (3-3,2-2) arrange gtable[layout]
7 7 (4-4,1-1) arrange gtable[layout]
8 8 (4-4,2-2) arrange gtable[layout]
> map_ag_plan
TableGrob (4 x 2) "arrange": 8 grobs
  z     cells    name           grob
1 1 (1-1,1-1) arrange gtable[layout]
2 2 (1-1,2-2) arrange gtable[layout]
3 3 (2-2,1-1) arrange gtable[layout]
4 4 (2-2,2-2) arrange gtable[layout]
5 5 (3-3,1-1) arrange gtable[layout]
6 6 (3-3,2-2) arrange gtable[layout]
7 7 (4-4,1-1) arrange gtable[layout]
8 8 (4-4,2-2) arrange gtable[layout]
> ## Figure 35: Adoption Rates of 1M5R/3R3G Practices by lenient and strict Criteria in Viet Nam in 2023  ----
> 
> var_lenient <- c("lenient_1m", "lenient_1r", "lenient_2r", "lenient_3r")
> var_strict <- c("strict_1m", "strict_1r", "strict_2r", "strict_3r")
> 
> adopt_all <- list()
> 
> calculate_adoption <- function(var_set, criterion_type) {
+   adopt_list <- list()
+   for (var in var_set) {
+     adopt <- df %>%
+       filter(!is.na(.data[[var]])) %>%
+       group_by(.data[[var]]) %>%
+       summarise(twt = sum(weight_final_rice, na.rm = TRUE)) %>%
+       mutate(sum_twt = sum(twt, na.rm = TRUE)) %>%
+       mutate(adoption = twt * 100 / sum_twt) %>%
+       filter(.data[[var]] == 1) %>%
+       select(-c(sum_twt, twt)) %>%
+       mutate(group = var, criterion = criterion_type)
+     adopt_list[[var]] <- adopt
+   }
+   return(adopt_list)
+ }
> 
> adopt_lenient <- calculate_adoption(var_lenient, "Lenient")
> adopt_strict <- calculate_adoption(var_strict, "Strict")
> 
> adopt_all <- c(adopt_lenient, adopt_strict)
> tab_adopt <- bind_rows(adopt_all) %>%
+   select(group, adoption, criterion)
> 
> tab_adopt$group <- factor(tab_adopt$group, 
+                           levels = c("lenient_1m", "lenient_1r", "lenient_2r", "lenient_3r",
+                                      "strict_1m", "strict_1r", "strict_2r", "strict_3r"),
+                           labels = c("Certified seeds", "1R: Seed rate", "2R: Fertilizer", "3R: Pesticide",
+                                      "Certified seeds", "1R: Seed rate", "2R: Fertilizer", "3R: Pesticide"))
> 
> # Plot with separate bars for lenient and strict criteria
> plot <- tab_adopt %>%
+   ggplot(aes(x = group, y = adoption, fill = criterion)) +
+   geom_bar(stat = "identity", position = position_dodge(width = 0.9), alpha = 0.8) +  # Increase dodge width to avoid overlap
+   labs(x = " ", y = "Percentage of Adopters", fill = "Criteria") +
+   scale_y_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100)) +
+   theme_minimal() +
+   theme(
+     axis.text.x = element_text(size = 10),  # Keep x-axis labels horizontal
+     axis.text.y = element_text(size = 12),  # Adjust y-axis font size
+     panel.grid = element_blank(),  # Remove background grid
+     legend.position = "top"  # Position legend at the top
+   ) +
+   scale_fill_manual(values = c("Lenient" = "#1c4c6f", "Strict" = "#d95f02"))  # Different colors for criteria
> 
> ggsave (plot, filename = "Output/Fig_35.png", width = 10, height = 6, dpi = 1024)
> 
> 
> ## Figure 36: Household adoption rates at the province-level for (a) Certified seeds, (b) 1R: Seed rate, (c) 2R: Fertilizer, (d) 3R: Pesticide----
> 
> tab_fig4 <- df %>%
+   #filter(!is.na(weight_final_rice)) %>% 
+   group_by(MATINH) %>%
+   summarize(
+     lenient_1m = sum((lenient_1m == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     lenient_1r = sum((lenient_1r == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     lenient_2r = sum((lenient_2r == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100,
+     lenient_3r = sum((lenient_3r == 1) * weight_final_rice, na.rm = TRUE) / sum(weight_final_rice, na.rm = TRUE) * 100
+   )
> 
> 
> modified_map <- modified_map %>% left_join(tab_fig4,by="MATINH")
> 
> OneM <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_1m) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(a) Certified seeds') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank())
> 
> R1 <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_1r) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(a) 1R: Seed rate') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank())
> 
> R2 <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_2r) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(c) 2R: Fertilizer') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank())
> 
> 
> R3 <- modified_map %>%
+   ggplot() + 
+   aes(fill = lenient_3r) + 
+   geom_sf() + 
+   scale_fill_gradient2(midpoint = 50, low = "#f7efef", mid = "#9ecae1", high = "#1c4c6f", 
+                        na.value = "#dbd0d0", space = "Lab", name = "In %", limits = c(0, 100)) +  
+   ggtitle('(d) 3R: Pesticide') +
+   theme(plot.title = element_text(size = 8, hjust = 0.5)) +
+   geom_rect(aes(xmin = 107.5, xmax = 110, ymin = 8.5, ymax = 10), 
+             linewidth = 0.1, color = "black", fill = NA) +
+   geom_rect(aes(xmin = 109.3, xmax = 110, ymin = 16.1, ymax = 16.7), 
+             linewidth = 0.1, color = "black", fill = NA) + 
+   theme(axis.text.x = element_blank(), 
+         axis.text.y = element_blank())
> 
> 
> OneM <- OneM + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> R1 <- R1 + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> R2 <- R2 + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> R3 <- R3 + theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
> 
> 
> a <- grid.arrange(OneM, R1, R2, R3, nrow = 2, ncol = 2)
> 
> ggsave (a, filename = "Output/Fig_36.png", 
+         height = 15, width = 8, 
+         dpi = 1024)
> rm(list = ls()) #start clean
> library (tidyverse)
> library (flextable)
> library (ggplot2)
> library (officer)
> library (httr)
> # Function to curl data from GitHub----
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> 
> 
> 
> ## VH22----
> curl_function ("data/processed/VH22_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_data.csv'
Content type 'text/plain; charset=utf-8' length 2257649 bytes (2.2 MB)
downloaded 2.2 MB

> df_22 <- read.csv ("data/processed/VH22_data.csv") %>%
+   mutate (THUNHAP_mil = THUNHAP/1000,
+           TONGCHITIEU_mil = TONGCHITIEU/1000,
+           land_area_ha = land_area_sum / 10000) %>%
+   mutate (panel = 2022)
> df_22 <- format_ID (df_22, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), 
+                     widths = c (2, 3, 5, 3, 3)) %>%
+   mutate (IDHO = paste0 (MAXA, MADIABAN, HOSO))
Error in format_ID(df_22, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN",  : 
  could not find function "format_ID"
> # Function to format ID values
> format_ID <- function(df, columns, widths, pad_char = "0") {
+   # Loop through each column and its corresponding width
+   for (i in seq_along(columns)) {
+     column <- columns[i]
+     width <- widths[i]
+     
+     # Pad the column with the specified width and character
+     df[[column]] <- str_pad(df[[column]], width = width, pad = pad_char)
+   }
+   return(df)
+ }
> 
> # Define columns and widths
> columns <- c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO")
> widths <- c(2, 3, 5, 3, 3)
> 
> # Function to curl data from GitHub----
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> 
> 
> 
> ## VH22----
> curl_function ("data/processed/VH22_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_data.csv'
Content type 'text/plain; charset=utf-8' length 2257649 bytes (2.2 MB)
downloaded 2.2 MB

> df_22 <- read.csv ("data/processed/VH22_data.csv") %>%
+   mutate (THUNHAP_mil = THUNHAP/1000,
+           TONGCHITIEU_mil = TONGCHITIEU/1000,
+           land_area_ha = land_area_sum / 10000) %>%
+   mutate (panel = 2022)
> df_22 <- format_ID (df_22, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), 
+                     widths = c (2, 3, 5, 3, 3)) %>%
+   mutate (IDHO = paste0 (MAXA, MADIABAN, HOSO))
> 
> 
> ## VH23----
> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> df_23 <- read.csv ("data/processed/VH23_data.csv") %>%
+   mutate (THUNHAP_mil = THUNHAP/1000,
+           TONGCHITIEU_mil = TONGCHITIEU/1000,
+           land_area_ha = land_area_sum / 10000) %>%
+   mutate (panel = 2023)
> df_23 <- format_ID (df_23, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), 
+                     widths = c (2, 3, 5, 3, 3)) %>%
+   mutate (IDHO = paste0 (MAXA, MADIABAN, HOSO))
> 
> 
> ## Combine df----
> df <- full_join (df_22, df_23)
Joining with `by = join_by(MATINH, IDHO, MAHUYEN, MAXA, MADIABAN, HOSO, mech_combine_harvester,
mech_straw_baler, mech_seed_blower, panel, mean_csmap, csmap_final, weight_final_rice, THUNHAP,
TONGCHITIEU, ethnic, rls_head, age, edu_grade, n_member, male, female, internet, land_area_sum,
Quintiles, Bottom_20, Bottom_40, THUNHAP_mil, TONGCHITIEU_mil, land_area_ha)`
> 
> 
> 
> ## List of results by year----
> var_group_stat <- c ("ethnic",
+                      "age",
+                      "edu_grade",
+                      "n_member",
+                      "female",
+                      "internet",
+                      "THUNHAP_mil",
+                      "TONGCHITIEU_mil",
+                      "land_area_ha",
+                      "poor_commune",
+                      "main_str_asphalt",
+                      "dist_market_wholesale",
+                      "dist_ext_center")
> 
> 
> 
> 
> var_name_group_stat <- c ("% of household head is an ethnic minority", 
+                           "Age of household head (in years)",
+                           "Household head's highest completed grade",
+                           "Household size",
+                           "% of households with female head",
+                           "% of households with internet access",
+                           "Annual income (in million VND)",
+                           "Annual consumption (in million VND)",
+                           "Total agricultural land managed or used by the household (in ha)",
+                           "Commune is labeled as poor (%)",
+                           "Main road is asphalt (%)", 
+                           "Distance to wholesale market (in km)",
+                           "Distance to extension center (in km)")
> 
> result <- list () #to store result
> 
> 
> for (i in (1:length(var_group_stat))) {
+   stat <- df %>%
+     group_by (panel) %>%
+     summarise (mean = mean (.data[[var_group_stat[i]]], na.rm = TRUE),
+                sd = sd (.data[[var_group_stat[i]]], na.rm = TRUE)) %>%
+     mutate (name = var_group_stat[[i]])
+   result[[i]] <- stat
+ }
>   
> result  #list of mean and sd by year
[[1]]
# A tibble: 2 × 4
  panel  mean    sd name  
  <dbl> <dbl> <dbl> <chr> 
1  2022 0.358 0.480 ethnic
2  2023 0.352 0.478 ethnic

[[2]]
# A tibble: 2 × 4
  panel  mean    sd name 
  <dbl> <dbl> <dbl> <chr>
1  2022  52.0  12.3 age  
2  2023  52.5  11.9 age  

[[3]]
# A tibble: 2 × 4
  panel  mean    sd name     
  <dbl> <dbl> <dbl> <chr>    
1  2022  7.60  3.05 edu_grade
2  2023  7.66  3.05 edu_grade

[[4]]
# A tibble: 2 × 4
  panel  mean    sd name    
  <dbl> <dbl> <dbl> <chr>   
1  2022  4.00  1.67 n_member
2  2023  3.99  1.67 n_member

[[5]]
# A tibble: 2 × 4
  panel  mean    sd name  
  <dbl> <dbl> <dbl> <chr> 
1  2022 0.162 0.369 female
2  2023 0.165 0.371 female

[[6]]
# A tibble: 2 × 4
  panel  mean    sd name    
  <dbl> <dbl> <dbl> <chr>   
1  2022 0.621 0.485 internet
2  2023 0.689 0.463 internet

[[7]]
# A tibble: 2 × 4
  panel  mean    sd name       
  <dbl> <dbl> <dbl> <chr>      
1  2022  144.  128. THUNHAP_mil
2  2023  157.  132. THUNHAP_mil

[[8]]
# A tibble: 2 × 4
  panel  mean    sd name           
  <dbl> <dbl> <dbl> <chr>          
1  2022  92.2  229. TONGCHITIEU_mil
2  2023  59.3  177. TONGCHITIEU_mil

[[9]]
# A tibble: 2 × 4
  panel  mean    sd name        
  <dbl> <dbl> <dbl> <chr>       
1  2022 0.967  1.59 land_area_ha
2  2023 1.05   1.81 land_area_ha

[[10]]
# A tibble: 2 × 4
  panel    mean     sd name        
  <dbl>   <dbl>  <dbl> <chr>       
1  2022   0.246  0.431 poor_commune
2  2023 NaN     NA     poor_commune

[[11]]
# A tibble: 2 × 4
  panel    mean     sd name            
  <dbl>   <dbl>  <dbl> <chr>           
1  2022   0.784  0.412 main_str_asphalt
2  2023 NaN     NA     main_str_asphalt

[[12]]
# A tibble: 2 × 4
  panel  mean    sd name                 
  <dbl> <dbl> <dbl> <chr>                
1  2022  14.3  24.5 dist_market_wholesale
2  2023 NaN    NA   dist_market_wholesale

[[13]]
# A tibble: 2 × 4
  panel  mean    sd name           
  <dbl> <dbl> <dbl> <chr>          
1  2022  12.6  11.8 dist_ext_center
2  2023 NaN    NA   dist_ext_center

> 
> 
> 
> ## Edit result table----
> tab_result <- bind_rows(result) %>%
+   pivot_wider (names_from = "panel", values_from = c(mean, sd)) %>%
+   relocate (ends_with("2023"), .after = ends_with("2022")) #combine result to dataframe
> 
> var_pct <- c("ethnic", "female", "internet", "poor_commune", "main_str_asphalt") #list of variables in %
> 
> col_pct <- tab_result %>%
+   select (starts_with(c("mean", "sd"))) %>%
+   colnames()  
> 
> for (col in col_pct){ #convert vars to % 
+     tab_result[[col]][tab_result$name %in% var_pct] <- tab_result[[col]][tab_result$name %in% var_pct] * 100
+ }
> 
> tab_result <- tab_result %>%
+   mutate (Variable = var_name_group_stat) %>%
+   mutate (order = case_when (name == "female" ~ 1,
+                              name == "ethnic" ~ 2,
+                              name == "internet" ~ 3,
+                              name == "land_area_ha" ~ 4,
+                              name == "age" ~ 5,
+                              name == "edu_grade" ~ 6,
+                              name == "n_member" ~ 7,
+                              name == "THUNHAP_mil" ~ 8,
+                              name == "TONGCHITIEU_mil" ~ 9,
+                              name == "poor_commune" ~ 10,
+                              name == "main_str_asphalt" ~ 11,
+                              name == "dist_market_wholesale" ~ 12,
+                              name == "dist_ext_center" ~ 13)) %>%
+   arrange(order) %>%
+   relocate (Variable, .before = everything ()) %>%
+   select (-c(order, name))
>   
> 
> ft_result <- flextable(tab_result) %>%
+   delete_part (part = "header") %>%
+   add_header_row(values = c("Variable", "Mean", "SD", "Mean", "SD")) %>%
+   add_header_row(values = c("", "VHLSS 2022", "VHLSS 2023"), 
+                  colwidths = c(1,2,2)) %>%
+   align(align = "center", part = "all", j = 2:5) %>%
+   colformat_double(j = c(2:5),digits = 2, na_str = "-", nan_str = "-") %>%
+   bold(i = 1, part = "header") %>%
+   bold (i = 2, part = "header") %>% 
+   theme_vanilla() %>%
+   autofit()
> 
> 
> ft_result
> save_as_docx (ft_result, 
+               path = "Output/Tab5.docx",
+               pr_section = prop_section(page_size(orient = "landscape")))
> library (tidyverse)
> library (dplyr)
> library (fastDummies)
> library (flextable)
> library (officer)
> library (haven)
> library (readxl)
> format_ID <- function(df, columns, widths, pad_char = "0") {
+   # Loop through each column and its corresponding width
+   for (i in seq_along(columns)) {
+     column <- columns[i]
+     width <- widths[i]
+     
+     # Pad the column with the specified width and character
+     df[[column]] <- str_pad(df[[column]], width = width, pad = pad_char)
+   }
+   return(df)
+ }
> 
> 
> # Define columns and widths
> columns <- c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO")
> widths <- c(2, 3, 5, 3, 3)
> 
> 
> # Function to curl data ----
> 
> 
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> 
> 
> # Create output folders----
> 
> output_dir <- "Output"
> 
> # Check if the directory exists, if not, create it
> if (!dir.exists(output_dir)) {
+   dir.create(output_dir, recursive = TRUE)
+ }
> 
> 
> # Load data----
> curl_function ("data/processed/VH22_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_data.csv'
Content type 'text/plain; charset=utf-8' length 2257649 bytes (2.2 MB)
downloaded 2.2 MB

> df_22 <- read.csv ("data/processed/VH22_data.csv")
> df_22 <- format_ID(df_22, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), widths = c(2, 3, 5, 3, 3) )
> df_22$IDHO <- paste0 (df_22$MAXA, df_22$MADIABAN, df_22$HOSO)
> 
> 
> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> df_23 <- read.csv ("data/processed/VH23_data.csv")
> df_23 <- format_ID(df_23, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), widths = c(2, 3, 5, 3, 3) )
> df_23$IDHO <- paste0 (df_23$MAXA, df_23$MADIABAN, df_23$HOSO)
> 
> df_23 <- df_23 %>%
+   rename ("mech_row_seeder" = "mech_row_drum_seeder")
> 
> df <- full_join (df_22, df_23)
Joining with `by = join_by(MATINH, IDHO, MAHUYEN, MAXA, MADIABAN, HOSO, mech_combine_harvester,
mech_straw_baler, mech_row_seeder, mech_seed_blower, panel, mean_csmap, csmap_final, weight_final_rice,
THUNHAP, TONGCHITIEU, ethnic, rls_head, age, edu_grade, n_member, male, female, internet, land_area_sum,
Quintiles, Bottom_20, Bottom_40)`
> 
> 
> 
> # PFES data:
> curl_function ("data/raw/VHLSS_2024_Commune/SPIA_ThongTinXa_Q1-3.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2024_Commune/SPIA_ThongTinXa_Q1-3.csv'
Content type 'text/plain; charset=utf-8' length 266833 bytes (260 KB)
downloaded 260 KB

> 
> 
> pfes <- read.csv ("data/raw/VHLSS_2024_Commune/SPIA_ThongTinXa_Q1-3.csv") %>%
+   select (c(MATINH, MAHUYEN, MAXA, M43_C1)) %>%
+   mutate (pfes_dummy = case_when (M43_C1 == 1 ~ 1,
+                                   TRUE ~ 0)) %>%
+   select (-M43_C1) %>%
+   mutate (panel = 2024)
> 
> pfes <- format_ID(pfes, columns = c("MATINH", "MAHUYEN", "MAXA"), widths = c(2,3,5))
> 
> 
> curl_function ("data/raw/Weight/Census_household_communelevel_clean.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/Census_household_communelevel_clean.csv'
Content type 'text/plain; charset=utf-8' length 665472 bytes (649 KB)
downloaded 649 KB

> n_hh_pop <- read.csv ("data/raw/Weight/Census_household_communelevel_clean.csv") %>%
+   select (c(MATINH, MAXA, n_hh)) %>%
+   rename (n_hh_pop = n_hh) 
> #merge by Commune ID (MAXA) because of some administrative change 
> # (486 missing if merge by prov, dist, comm ID --> 470 missing if merge by prov and comm ID)
> 
> n_hh_pop <- format_ID(n_hh_pop, columns = c("MATINH", "MAXA"), widths = c(2, 5))
> 
> pfes_joined <- pfes %>%
+   left_join (n_hh_pop)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> # Merge to one dataset:
> 
> df <- df%>%
+   full_join (pfes_joined)  
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, panel)`
> 
> #Prep data:
> df <- df %>%
+   mutate (CMD_edited = case_when(CMD == TRUE ~ 1,
+                                  CMD == FALSE ~ 0,
+                                  TRUE ~ NA),
+           DMC_edited = case_when (DMC == TRUE ~ 1,
+                                   DMC == FALSE ~ 0,
+                                   TRUE ~ NA)) %>%
+   mutate (lenient_3r3g = case_when (d_1m5r_seed_120kg == 1 & lenient_2r == 1 & lenient_3r == 1 ~ 1,
+                                     d_1m5r_seed_120kg != 1 | lenient_2r != 1 | lenient_3r != 1 ~ 0,
+                                     TRUE ~ NA),
+           strict_3r3g = case_when (d_1m5r_seed_100kg == 1 & strict_2r == 1 & strict_3r ==1 ~ 1,
+                                    d_1m5r_seed_100kg != 1 | strict_2r != 1 | strict_3r !=1 ~ 0)) %>%
+   mutate (ID_EA = paste0 (MAXA, MADIABAN)) 
> 
> 
> 
> # Adoption table ----
> 
> ## Adoption at HH level and reach of adoption ----
> 
> 
> ### Rice ----
> 
> var_list <- c(
+   "csmap_final",
+   "mech_mini_combiner", "mech_combine_harvester", "mech_straw_baler",
+   "mech_laser_level", "mech_row_seeder", "mech_seed_blower",
+   
+   "lenient_1m", "d_1m5r_certified", 
+   "d_1m5r_seed_120kg", "d_1m5r_seed_100kg", 
+   "lenient_2r", "strict_2r",
+   "lenient_3r", "strict_3r",
+   "lenient_3r3g", "strict_3r3g",
+   "awd_1drydown",
+   "awd_2drydown",
+   "straw_rm_livestock", "straw_rm_mushroom", 
+   "straw_rm_compost")
> 
> 
> 
> 
> 
> 
> 
> # % OF SURVEYED HOUSEHOLDS THAT ADOPT INNOVATION: 
> 
> adoption_hh <- list()  #prep result list
> 
> for (var in var_list){
+   adopt <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (panel, .data[[var]]) %>%
+     summarise (twt = sum(weight_final_rice, na.rm = TRUE)) %>%
+     group_by (panel) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt))
+   
+   adoption_hh[[var]] <- adopt
+   
+ }
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
> 
> 
> adoption_hh
$csmap_final
# A tibble: 2 × 4
# Groups:   panel [2]
  panel csmap_final     twt adoption
  <dbl>       <int>   <dbl>    <dbl>
1  2022           1  87346.     24.4
2  2023           1 171561.     27.4

$mech_mini_combiner
# A tibble: 1 × 4
# Groups:   panel [1]
  panel mech_mini_combiner     twt adoption
  <dbl>              <int>   <dbl>    <dbl>
1  2022                  1 800280.     10.3

$mech_combine_harvester
# A tibble: 2 × 4
# Groups:   panel [2]
  panel mech_combine_harvester      twt adoption
  <dbl>                  <int>    <dbl>    <dbl>
1  2022                      1 5091704.     65.4
2  2023                      1 5621020.     74.0

$mech_straw_baler
# A tibble: 2 × 4
# Groups:   panel [2]
  panel mech_straw_baler     twt adoption
  <dbl>            <int>   <dbl>    <dbl>
1  2022                1 207567.     2.67
2  2023                1 303644.     4.00

$mech_laser_level
# A tibble: 1 × 4
# Groups:   panel [1]
  panel mech_laser_level     twt adoption
  <dbl>            <int>   <dbl>    <dbl>
1  2023                1 409359.     5.39

$mech_row_seeder
# A tibble: 2 × 4
# Groups:   panel [2]
  panel mech_row_seeder     twt adoption
  <dbl>           <int>   <dbl>    <dbl>
1  2022               1 344464.     4.43
2  2023               1 127955.     1.68

$mech_seed_blower
# A tibble: 2 × 4
# Groups:   panel [2]
  panel mech_seed_blower     twt adoption
  <dbl>            <int>   <dbl>    <dbl>
1  2022                1 182016.     2.34
2  2023                1 289120.     3.81

$lenient_1m
# A tibble: 1 × 4
# Groups:   panel [1]
  panel lenient_1m      twt adoption
  <dbl>      <int>    <dbl>    <dbl>
1  2023          1 6777472.     89.2

$d_1m5r_certified
# A tibble: 1 × 4
# Groups:   panel [1]
  panel d_1m5r_certified      twt adoption
  <dbl>            <int>    <dbl>    <dbl>
1  2023                1 5591764.     73.6

$d_1m5r_seed_120kg
# A tibble: 1 × 4
# Groups:   panel [1]
  panel d_1m5r_seed_120kg      twt adoption
  <dbl>             <int>    <dbl>    <dbl>
1  2023                 1 5428344.     75.8

$d_1m5r_seed_100kg
# A tibble: 1 × 4
# Groups:   panel [1]
  panel d_1m5r_seed_100kg      twt adoption
  <dbl>             <int>    <dbl>    <dbl>
1  2023                 1 5081131.     71.0

$lenient_2r
# A tibble: 1 × 4
# Groups:   panel [1]
  panel lenient_2r      twt adoption
  <dbl>      <int>    <dbl>    <dbl>
1  2023          1 4718949.     62.8

$strict_2r
# A tibble: 1 × 4
# Groups:   panel [1]
  panel strict_2r      twt adoption
  <dbl>     <int>    <dbl>    <dbl>
1  2023         1 2904259.     38.5

$lenient_3r
# A tibble: 1 × 4
# Groups:   panel [1]
  panel lenient_3r      twt adoption
  <dbl>      <int>    <dbl>    <dbl>
1  2023          1 1611585.     21.4

$strict_3r
# A tibble: 1 × 4
# Groups:   panel [1]
  panel strict_3r      twt adoption
  <dbl>     <int>    <dbl>    <dbl>
1  2023         1 1340450.     17.8

$lenient_3r3g
# A tibble: 1 × 4
# Groups:   panel [1]
  panel lenient_3r3g     twt adoption
  <dbl>        <dbl>   <dbl>    <dbl>
1  2023            1 787775.     10.5

$strict_3r3g
# A tibble: 1 × 4
# Groups:   panel [1]
  panel strict_3r3g     twt adoption
  <dbl>       <dbl>   <dbl>    <dbl>
1  2023           1 356602.     4.76

$awd_1drydown
# A tibble: 1 × 4
# Groups:   panel [1]
  panel awd_1drydown     twt adoption
  <dbl>        <int>   <dbl>    <dbl>
1  2023            1 408689.     5.41

$awd_2drydown
# A tibble: 1 × 4
# Groups:   panel [1]
  panel awd_2drydown     twt adoption
  <dbl>        <int>   <dbl>    <dbl>
1  2023            1 249328.     3.30

$straw_rm_livestock
# A tibble: 1 × 4
# Groups:   panel [1]
  panel straw_rm_livestock      twt adoption
  <dbl>              <int>    <dbl>    <dbl>
1  2022                  1 1162775.     50.8

$straw_rm_mushroom
# A tibble: 1 × 4
# Groups:   panel [1]
  panel straw_rm_mushroom   twt adoption
  <dbl>             <int> <dbl>    <dbl>
1  2022                 1 4949.    0.216

$straw_rm_compost
# A tibble: 1 × 4
# Groups:   panel [1]
  panel straw_rm_compost     twt adoption
  <dbl>            <int>   <dbl>    <dbl>
1  2022                1 191688.     8.38

> 
> 
> 
> ### Rice DNA reach: ----
> 
> 
> dna_list <- c("Sub1", "Saltol", "IRRI_Parentage_edited")
> 
> reach_dna <- list()
> 
> for (var in dna_list) {
+   
+   adopt_dna <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (panel, .data[[var]]) %>%
+     summarise (twt = sum(weight_rice_DNA, na.rm = TRUE)) %>%
+     group_by (panel) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt))
+   
+   
+   reach_dna[[var]] <- adopt_dna
+ }
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
> 
> reach_dna
$Sub1
# A tibble: 1 × 4
# Groups:   panel [1]
  panel  Sub1     twt adoption
  <dbl> <int>   <dbl>    <dbl>
1  2022     1 114906.     1.49

$Saltol
# A tibble: 1 × 4
# Groups:   panel [1]
  panel Saltol      twt adoption
  <dbl>  <int>    <dbl>    <dbl>
1  2022      1 2354528.     30.8

$IRRI_Parentage_edited
# A tibble: 1 × 4
# Groups:   panel [1]
  panel IRRI_Parentage_edited      twt adoption
  <dbl>                 <int>    <dbl>    <dbl>
1  2022                     1 1926902.     25.0

> 
> 
> 
> 
> ### Cassava reach ----
> 
> 
> cassava_list <- c("CMD_edited", "DMC_edited", "CIAT.related")
> 
> reach_cassava <- list()
> 
> for (var in cassava_list) {
+   
+   adopt_cassava <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (panel, .data[[var]]) %>%
+     summarise (twt = sum(weight_cass, na.rm = TRUE)) %>%
+     group_by (panel) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt))
+   
+   
+   reach_cassava[[var]] <- adopt_cassava
+ }
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
> 
> reach_cassava
$CMD_edited
# A tibble: 1 × 4
# Groups:   panel [1]
  panel CMD_edited     twt adoption
  <dbl>      <dbl>   <dbl>    <dbl>
1  2023          1 147892.     29.7

$DMC_edited
# A tibble: 1 × 4
# Groups:   panel [1]
  panel DMC_edited     twt adoption
  <dbl>      <dbl>   <dbl>    <dbl>
1  2023          1 398237.     80.0

$CIAT.related
# A tibble: 1 × 4
# Groups:   panel [1]
  panel CIAT.related     twt adoption
  <dbl>        <int>   <dbl>    <dbl>
1  2023            1 270869.     54.4

> 
> 
> 
> 
> 
> ### Tilapia reach ----
> 
> tilapia_list <- c("StrainB_edited")
> 
> reach_tilapia <- list()
> 
> for (var in tilapia_list) {
+   
+   adopt_tilapia <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (panel, .data[[var]]) %>%
+     summarise (twt = sum(weight_gift, na.rm = TRUE)) %>%
+     group_by (panel) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt))
+   
+   
+   reach_tilapia[[var]] <- adopt_tilapia
+ }
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
> 
> reach_tilapia
$StrainB_edited
# A tibble: 1 × 4
# Groups:   panel [1]
  panel StrainB_edited   twt adoption
  <dbl>          <int> <int>    <dbl>
1  2023              1 27291     29.7

> 
> 
> 
> 
> ### Coffee reach ----
> 
> 
> coffee_list <- c("SWCP")
> 
> reach_coffee <- list()
> 
> for (var in coffee_list) {
+   
+   adopt_coffee <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (panel, .data[[var]]) %>%
+     summarise (twt = sum(weight_coffee, na.rm = TRUE)) %>%
+     group_by (panel) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt))
+   
+   
+   reach_coffee[[var]] <- adopt_coffee
+ }
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
> 
> reach_coffee
$SWCP
# A tibble: 1 × 4
# Groups:   panel [1]
  panel  SWCP     twt adoption
  <dbl> <int>   <dbl>    <dbl>
1  2023     1 491393.     62.6

> 
> 
> 
> ### PFES reach----
> 
> reach_pfes <- df %>%
+   filter (!is.na(pfes_dummy)) %>%
+   group_by (panel, pfes_dummy) %>%
+   summarise (twt = sum (n_hh_pop, na.rm = TRUE)) %>%
+   group_by (panel) %>%
+   mutate (sum_twt = sum(twt, na.rm = TRUE)) %>%
+   filter (pfes_dummy == 1) %>%
+   select (-c(sum_twt))
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
> 
> 
> 
> ### CSMAPs----
> 
> 
> 
> ## Adoption at EA level:----
> all_var <-  c(
+   "csmap_final",
+   "mech_mini_combiner", "mech_combine_harvester", "mech_straw_baler",
+   "mech_laser_level", "mech_row_seeder", "mech_seed_blower",
+   
+   "lenient_1m", "d_1m5r_certified", 
+   "d_1m5r_seed_120kg", "d_1m5r_seed_100kg", 
+   "lenient_2r", "strict_2r", 
+   "lenient_3r", "strict_3r", 
+   "lenient_3r3g", "strict_3r3g",
+   "awd_1drydown",
+   "awd_2drydown",
+                         
+   "straw_rm_livestock", "straw_rm_mushroom", 
+   "straw_rm_compost",
+   "Sub1", "Saltol", "IRRI_Parentage_edited",
+   "CMD_edited", "DMC_edited", "CIAT.related",
+   "StrainB_edited",
+   "SWCP",
+   "pfes_dummy")
> 
> adopt_EA <- list ()
> 
> for (var in all_var){
+   adopt_ea <- df %>%
+     filter(!is.na(.data[[var]])) %>%
+     group_by(ID_EA, panel) %>%
+     dplyr::summarize(mean_adopt_EA = mean(.data[[var]], na.rm = TRUE)) %>%
+     group_by (panel) %>%
+     mutate (adopt_EA = case_when (mean_adopt_EA > 0 ~ 1,
+                                   mean_adopt_EA == 0 ~ 0,
+                                   TRUE ~ NA)) %>%
+     group_by (panel, adopt_EA) %>%
+     summarise (n_EA = n()) %>%
+     mutate (pct_adopt_EA = n_EA * 100 / sum(n_EA)) %>%
+     filter (adopt_EA == 1 ) %>%
+     select (c(panel, pct_adopt_EA))
+   
+   adopt_EA[[var]] <- adopt_ea
+   
+ }
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'ID_EA'. You can override using the `.groups` argument.
`summarise()` has grouped output by 'panel'. You can override using the `.groups` argument.
> 
> adopt_EA
$csmap_final
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         43.2
2  2023         49.6

$mech_mini_combiner
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         26.9

$mech_combine_harvester
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         75.8
2  2023         81.7

$mech_straw_baler
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         8.29
2  2023        12.8 

$mech_laser_level
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         13.9

$mech_row_seeder
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022        13.0 
2  2023         5.56

$mech_seed_blower
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         5.75
2  2023         8.46

$lenient_1m
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         95.1

$d_1m5r_certified
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         86.2

$d_1m5r_seed_120kg
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         76.8

$d_1m5r_seed_100kg
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         70.6

$lenient_2r
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         87.7

$strict_2r
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         71.5

$lenient_3r
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         46.7

$strict_3r
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         40.8

$lenient_3r3g
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         27.4

$strict_3r3g
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         14.9

$awd_1drydown
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         14.7

$awd_2drydown
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         11.5

$straw_rm_livestock
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         61.9

$straw_rm_mushroom
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022        0.820

$straw_rm_compost
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         18.0

$Sub1
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         3.21

$Saltol
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         49.2

$IRRI_Parentage_edited
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         41.4

$CMD_edited
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023           50

$DMC_edited
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         87.9

$CIAT.related
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         62.5

$StrainB_edited
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         77.2

$SWCP
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         78.9

$pfes_dummy
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2024         11.6

> 
> 
> 
> 
> ## Unlist all results: ----
> 
> library (plyr)
Warning: package ‘plyr’ was built under R version 4.3.2
--------------------------------------------------------------------------------------------------------
You have loaded plyr after dplyr - this is likely to cause problems.
If you need functions from both plyr and dplyr, please load plyr first, then dplyr:
library(plyr); library(dplyr)
--------------------------------------------------------------------------------------------------------

Attaching package: ‘plyr’

The following object is masked from ‘package:this.path’:

    here

The following object is masked from ‘package:purrr’:

    compact

The following object is masked from ‘package:maditr’:

    take

The following objects are masked from ‘package:dplyr’:

    arrange, count, desc, failwith, id, mutate, rename, summarise, summarize

> 
> tab_adoption_rice <- ldply (adoption_hh) %>%
+   select (c(".id", "panel", "twt", "adoption"))
> 
> tab_adoption_dna <- ldply (reach_dna) %>%
+   select (c(".id", "twt", "adoption")) %>%
+   mutate ("panel" = 2022)
> 
> 
> tab_adoption_cassava <- ldply (reach_cassava) %>%
+   select (c(".id", "twt", "adoption")) %>%
+   mutate ("panel" = 2023)
> 
> tab_adoption_tilapia <- ldply (reach_tilapia) %>%
+   select (c(".id", "twt", "adoption")) %>%
+   mutate ("panel" = 2023)
> 
> tab_adoption_coffee <- ldply (reach_coffee) %>%
+   select (c(".id", "twt", "adoption")) %>%
+   mutate ("panel" = 2023)
> 
> tab_adoption_ea <- ldply (adopt_EA)
> 
> 
> 
> detach("package:plyr", unload = TRUE)
Warning: ‘plyr’ namespace cannot be unloaded:
  namespace ‘plyr’ is imported by ‘rapportools’, ‘reshape2’ so cannot be unloaded
> 
> 
> 
> ## Edit column names of result table: ----
> 
> tab_adoption_rice <- tab_adoption_rice %>%
+   rename (reach = twt,
+           adopt_hh = adoption) %>%
+   pivot_wider(names_from = panel, values_from = c(adopt_hh, reach))
> 
> 
> tab_adoption_dna <- tab_adoption_dna %>%
+   rename (reach = twt,
+           adopt_hh = adoption) %>%
+   pivot_wider(names_from = panel, values_from = c(adopt_hh, reach))
> 
> 
> tab_adoption_cassava <- tab_adoption_cassava %>%
+   rename (reach = twt,
+           adopt_hh = adoption) %>%
+   pivot_wider(names_from = panel, values_from = c(adopt_hh, reach))
> 
> 
> tab_adoption_tilapia <- tab_adoption_tilapia %>%
+   rename (reach = twt,
+           adopt_hh = adoption) %>%
+   pivot_wider(names_from = panel, values_from = c(adopt_hh, reach))
> 
> tab_adoption_coffee <- tab_adoption_coffee %>%
+   rename (reach = twt,
+           adopt_hh = adoption) %>%
+   pivot_wider(names_from = panel, values_from = c(adopt_hh, reach))
> 
> 
> tab_adoption_ea <- tab_adoption_ea %>%
+   rename (adopt_EA = pct_adopt_EA) %>%
+   pivot_wider(names_glue = "{.value}_{panel}", names_from = panel, values_from = adopt_EA)
> 
> 
> tab_adoption_pfes <- reach_pfes %>%
+   rename (c("reach" = "twt",
+             ".id" = "pfes_dummy")) %>%
+   pivot_wider(names_glue = "{.value}_{panel}", names_from = panel, values_from = reach)
> tab_adoption_pfes[1] <- "pfes_dummy"
> 
> 
> 
> 
> result <- full_join (tab_adoption_rice, tab_adoption_dna) %>%
+   full_join (tab_adoption_cassava) %>%
+   full_join (tab_adoption_tilapia) %>%
+   full_join (tab_adoption_coffee) %>%
+   full_join (tab_adoption_pfes) %>%
+   full_join (tab_adoption_ea) %>%
+   rename (var = ".id") %>%
+   relocate (c("adopt_EA_2022", "adopt_hh_2022", "reach_2022", "adopt_EA_2023", "adopt_hh_2023", "reach_2023", "adopt_EA_2024", "reach_2024"),
+             .after = "var")
Joining with `by = join_by(.id, adopt_hh_2022, reach_2022)`
Joining with `by = join_by(.id, adopt_hh_2023, reach_2023)`
Joining with `by = join_by(.id, adopt_hh_2023, reach_2023)`
Joining with `by = join_by(.id, adopt_hh_2023, reach_2023)`
Joining with `by = join_by(.id)`
Joining with `by = join_by(.id)`
> 
> 
> 
> 
> 
> 
> ## Final results output: ----
> 
> 
> result <- result %>%
+   mutate(
+     order = case_when(
+       var == "CMD_edited" ~ 0.95,
+       var == "DMC_edited" ~ 0.96,  #reorder CMD and DMC
+       var == "CIAT.related" ~ 0.94,
+       var == "StrainB_edited" ~ 0.4,
+       var == "Sub1" ~ 0.99,
+       var == "Saltol" ~ 0.98,
+       var == "IRRI_Parentage_edited" ~ 0.97,
+       var == "mech_laser_level" ~ 2,
+       var == "mech_row_seeder" ~ 20.1,
+       var == "mech_seed_blower" ~ 2.9,
+       var == "mech_combine_harvester" ~ 2.2,
+       var == "mech_mini_combiner" ~ 2.3,
+       var == "mech_straw_baler" ~ 2.4,
+       var == "pfes_dummy" ~ 1.9,
+       var == "csmap_final" ~ 1.8,
+       TRUE ~ as.numeric(row_number())
+     )
+   ) %>%
+   arrange(order) %>%
+   select(-order)
> 
> 
> 
> 
> 
> 
> 
> 
> # Print Table 7----
> 
> var_name <- c(
+   "Genetically Improved Farmed Tilapia (GIFT)-derived strains",
+   "CGIAR-related Cassava Varieties",
+   "Cassava Mosaic Disease (CMD)-resistant Cassava Varieties",
+   "High-starch cassava varieties (DM QTL)",
+   "CGIAR-related Rice Varieties",
+   "Salt-tolerant Rice Varieties (STRVs)",
+   "Submergence-tolerant Rice Varieties",
+   "Climate-Smart Mapping and Adaptation Planning (CS-MAP)",
+   "Payment for Forest Environmental Services (PFES)",
+   "Laser Land Levelling (LLL)",
+   "Combine Harvester (CHB)",
+   "Mini-Combine Harvester (MCHB)",
+   "Rice Straw Baler", 
+   "Household used seed blower",
+   "Lenient 1M (Households combined certified seeds and own seeds)",
+   "Strict 1M (Households used totally certified seeds)", 
+   "Lenient 1R (Households adopted maximum 120kg/ha seed rates)",
+   "Strict 1R (Households adopted maximum 100kg/ha seed rates)",
+   "Lenient 2R (Maximum 110kg/ha of Nitrogen and minimum 2 applications)",
+   "Strict 2R (Maximum 100kg/ha of Nitrogen and minimum 3 applications)",
+   "Lenient 3R (Maximum 6 applications and not within 20 days before harvest)",
+   "Strict 3R (Maximum 3 applications of chemicals, not within 40 days after sowing, not after flowering)",
+   "Three Reductions, Three Gains (3R3G) and One Must Do, Five Reductions (1M5R)",
+   "Strict 3R3G (Households adopted all the three reductions, using strict criteria)",
+   "Alternate Wetting and Drying (AWD)",
+   "At least 2 dry-downs, all between reproductive stage, each enduring at least 5 days",
+   "Households used removed straws to feed for livestock",
+   "Drum Seeder",
+   "Off-field Straw Management Practices",
+   "Households used removed straws for compost", 
+   "Sustainable Water Use for Coffee Production"
+   )
> 
> 
> 
> table7_print <- result %>%
+   mutate (var_name = var_name) %>%
+   relocate (var_name, .before = everything()) %>%
+   select (-var) 
> 
> 
> var_remove <- c("High-starch cassava varieties (DM QTL)",
+                 "Households used removed straws to feed for livestock",
+                 "Households used removed straws for compost",
+                 "Household used seed blower",
+                 "Lenient 1M (Households combined certified seeds and own seeds)",
+                 "Strict 1M (Households used totally certified seeds)",
+                 "Lenient 1R (Households adopted maximum 120kg/ha seed rates)",
+                 "Strict 1R (Households adopted maximum 100kg/ha seed rates)",
+                 "Lenient 2R (Maximum 110kg/ha of Nitrogen and minimum 2 applications)",
+                 "Strict 2R (Maximum 100kg/ha of Nitrogen and minimum 3 applications)",
+                 "Lenient 3R (Maximum 6 applications and not within 20 days before harvest)",
+                 "Strict 3R (Maximum 3 applications of chemicals, not within 40 days after sowing, not after flowering)",
+                 "At least 2 dry-downs, all between reproductive stage, each enduring at least 5 days",
+                 "Strict 3R3G (Households adopted all the three reductions, using strict criteria)")
> 
> 
> 
> table7_print <- table7_print %>%
+   filter (!var_name %in% var_remove)
> 
> 
> table7_print <- table7_print %>%
+   mutate (group_name = c(
+     rep ("Aquaculture and Capture Fisheries", 1),
+     rep ("Breeding Innovations", 5),
+     "Climate Change Adaptation Options",
+     "Environmental Conservation",
+     rep ("Mechanization", 4),
+     rep("Sustainable Intensification Practices", 5))) %>%
+   relocate (group_name, .before = "var_name") 
> 
> 
> 
> 
> 
> 
> ft <- flextable(table7_print) %>%
+   delete_part(part = "header") %>%
+   add_header_row(values = c("", "", 
+                             "%EA", "%HH", "Estimated number of hh", 
+                             "%EA", "%HH", "Estimated number of hh",
+                             "%EA", "Estimated number of hh")) %>%
+   add_header_row(values = c("Core Domain", "Innovation", "2022", "2023", "2024"), 
+                  colwidths = c(1, 1, 3, 3, 2)) %>%
+   align(align = "center", part = "header") %>%
+   # align(align = "center", part = "header", i = 1) %>%
+   merge_v(j = 1) %>% 
+   valign(j = 1, valign = "top") %>% 
+   colformat_double(j = c(3,4,6,7,9),digits = 1) %>%
+   # colformat_double(j = 7:8, digits = 1) %>%
+   colformat_double(j = 5, digits = 0) %>%
+   colformat_double(j = 8, digits=  0) %>%
+   bold(i = 1, part = "header") %>%
+   bold (i = 2, part = "header") %>% 
+   border_inner_h(part = "body", fp_border(width = 0.5)) %>%
+   border_inner_h(part = "header", fp_border(width = 0.5)) %>%
+   border(i = 1, border.top = fp_border(width = 2)) %>%
+   bold(j = 1, part = "body") %>%
+   footnote (part = "header", i = 1, j = 9, value = as_paragraph("Preliminary data - from the first three quarters of VHLSS 2024"), ref_symbols = "1") %>%
+   autofit()
> 
> ft
> 
> print(ft)
> 
> 
> save_as_html (ft,
+               path = "Output/Tab7.html",
+               page_size(orient = "landscape"))
> 
> 
> save_as_docx(ft, 
+              path = "Output/Tab7.docx",
+              pr_section = prop_section(page_size = page_size(orient = "landscape")))
> adopt_EA
$csmap_final
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         43.2
2  2023         49.6

$mech_mini_combiner
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         26.9

$mech_combine_harvester
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         75.8
2  2023         81.7

$mech_straw_baler
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         8.29
2  2023        12.8 

$mech_laser_level
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         13.9

$mech_row_seeder
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022        13.0 
2  2023         5.56

$mech_seed_blower
# A tibble: 2 × 2
# Groups:   panel [2]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         5.75
2  2023         8.46

$lenient_1m
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         95.1

$d_1m5r_certified
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         86.2

$d_1m5r_seed_120kg
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         76.8

$d_1m5r_seed_100kg
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         70.6

$lenient_2r
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         87.7

$strict_2r
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         71.5

$lenient_3r
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         46.7

$strict_3r
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         40.8

$lenient_3r3g
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         27.4

$strict_3r3g
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         14.9

$awd_1drydown
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         14.7

$awd_2drydown
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         11.5

$straw_rm_livestock
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         61.9

$straw_rm_mushroom
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022        0.820

$straw_rm_compost
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         18.0

$Sub1
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         3.21

$Saltol
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         49.2

$IRRI_Parentage_edited
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2022         41.4

$CMD_edited
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023           50

$DMC_edited
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         87.9

$CIAT.related
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         62.5

$StrainB_edited
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         77.2

$SWCP
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2023         78.9

$pfes_dummy
# A tibble: 1 × 2
# Groups:   panel [1]
  panel pct_adopt_EA
  <dbl>        <dbl>
1  2024         11.6

> rm (list = ls()) #start clean
> options(scipen = 999) #display of number
> # Load packages ----
> library(rvest)
Warning: package ‘rvest’ was built under R version 4.3.2

Attaching package: ‘rvest’

The following object is masked from ‘package:readr’:

    guess_encoding

> library(iai)
Warning: package ‘iai’ was built under R version 4.3.3

Attaching package: ‘iai’

The following objects are masked from ‘package:jsonlite’:

    read_json, write_json

The following object is masked from ‘package:e1071’:

    impute

The following object is masked from ‘package:stats’:

    predict

The following objects are masked from ‘package:base’:

    apply, transform

> library(jtools)
> library(tidyverse)
> library(summarytools)
> library(broom)
Warning: package ‘broom’ was built under R version 4.3.3
> library(dplyr)
> library(estimatr)  
> library(ggplot2)
> library (curl)
> library(patchwork)
Warning: package ‘patchwork’ was built under R version 4.3.3

Attaching package: ‘patchwork’

The following object is masked from ‘package:cowplot’:

    align_plots

> library (flextable)
> library (officer)
> library (httr)
> 
> 
> # Function to curl data from GitHub----
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> 
> 
> # Create output folders----
> 
> output_dir <- "Output/Multivariate_analysis"
> 
> # Check if the directory exists, if not, create it
> if (!dir.exists(output_dir)) {
+   dir.create(output_dir, recursive = TRUE)
+ }
> 
> 
> 
> # Load and prepare data ----
> # Province ID: 
> curl_function ("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv'
Content type 'text/plain; charset=utf-8' length 1799 bytes
downloaded 1799 bytes

> Provinces_IDs <- read.csv("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv") 
> Provinces_IDs$MATINH <- as.numeric (Provinces_IDs$MATINH)
> 
> # VH22: 
> curl_function ("data/processed/VH22_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_data.csv'
Content type 'text/plain; charset=utf-8' length 2257649 bytes (2.2 MB)
downloaded 2.2 MB

> df_22 <- read.csv("data/processed/VH22_data.csv")
> df_22$MATINH <- as.numeric (df_22$MATINH)
> df_22 <- df_22 %>%
+   left_join (Provinces_IDs)
Joining with `by = join_by(MATINH)`
> 
> # Adjust variables
> df_22$THUNHAP_mil <- scale(df_22$THUNHAP) # df_22$THUNHAP / 1000 #convert unit from in thousand dong to in million dong
> df_22$land_area_ha <- df_22$land_area_sum / 10000 #convert from sq meters to hectare
> df_22$age <- scale (df_22$age) # Scale age
> df_22$edu_grade <- scale (df_22$edu_grade) 
> df_22$n_member <- scale (df_22$n_member) 
> 
>  
> 
> # Calculate the 99th percentiles
> thunhap_99 <- quantile(df_22$THUNHAP_mil, 0.99, na.rm = TRUE)
> land_area_sum_99 <- quantile(df_22$land_area_ha, 0.99, na.rm = TRUE)
> df_22$THUNHAP_mil <- ifelse(df_22$THUNHAP_mil > thunhap_99, NA, df_22$THUNHAP_mil)
> df_22$land_area_ha <- ifelse(df_22$land_area_ha > land_area_sum_99, NA, df_22$land_area_ha)
> df_22$EA_ID <- paste (df_22$MATINH, df_22$MAHUYEN, df_22$MAXA, df_22$MADIABAN, sep='-')
> 
> 
> #VH23:
> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> df_23 <- read.csv("data/processed/VH23_data.csv")
> df_23$MATINH <- as.numeric (df_23$MATINH)
> 
> df_23 <- df_23 %>%
+   left_join (Provinces_IDs)
Joining with `by = join_by(MATINH)`
> 
> df_23$THUNHAP_mil <- scale (df_23$THUNHAP)
> df_23$land_area_ha <- df_23$land_area_sum / 10000
> df_23$age <- scale (df_23$age)
> df_23$edu_grade <- scale (df_23$edu_grade) 
> df_23$n_member <- scale (df_23$n_member) 
> 
> df_23$ThreeR <- ifelse(
+   df_23$d_1m5r_seed_120kg == 1 & df_23$lenient_2r == 1 & df_23$lenient_3r == 1, 1, 
+   ifelse(is.na(df_23$d_1m5r_seed_120kg) & is.na(df_23$lenient_2r) & is.na(df_23$lenient_3r), NA, 0 ))
> 
> 
> 
> # Calculate the 99th percentiles
> thunhap_99 <- quantile(df_23$THUNHAP_mil, 0.99, na.rm = TRUE)
> land_area_sum_99 <- quantile(df_23$land_area_ha, 0.99, na.rm = TRUE)
> df_23$THUNHAP_mil <- ifelse(df_23$THUNHAP_mil > thunhap_99, NA, df_23$THUNHAP_mil)
> df_23$land_area_ha <- ifelse(df_23$land_area_ha > land_area_sum_99, NA, df_23$land_area_ha)
> 
> df_23$EA_ID <- paste (df_23$MATINH, df_23$MAHUYEN, df_23$MAXA, df_23$MADIABAN, sep='-')
> 
> 
> 
> 
> # Dependent vars:
> All_22 <- c("mech_row_seeder", "mech_mini_combiner", "mech_combine_harvester", "mech_straw_baler", 
+          "IRRI_Parentage_edited", "Saltol")
> 
> 
> # Define the list of predictor variables
> corr_list_22 <- c("female", "ethnic", "internet", "land_area_ha","main_str_asphalt",
+                "age", "edu_grade", "n_member", "THUNHAP_mil") 
> 
> 
> All_23 <- c(
+   "mech_row_drum_seeder", 
+   # "mech_seed_blower", 
+   "mech_combine_harvester", "mech_straw_baler", "mech_laser_level",
+   'CIAT.related', "DMC", "CMD",  
+   "StrainB_edited", 
+   "SWCP", 
+   "d_1m5r_seed_100kg", "d_1m5r_seed_120kg", "strict_2r", "lenient_2r", "strict_3r", "lenient_3r", "ThreeR",
+   "awd_1drydown", "awd_2drydown", "csmap_final"
+ )
> 
> 
> corr_list_23 <- c("female", "ethnic", "internet", "land_area_ha",
+                   "age", "edu_grade", "n_member", "THUNHAP_mil") 
> 
> 
> dependent_23_noCSMAP <- c(
+   "mech_row_drum_seeder", 
+   # "mech_seed_blower", 
+   "mech_combine_harvester", "mech_straw_baler", "mech_laser_level",
+   'CIAT.related', "DMC", "CMD",  
+   "StrainB_edited", 
+   "SWCP", 
+   "d_1m5r_seed_100kg", "d_1m5r_seed_120kg", "strict_2r", "lenient_2r", "strict_3r", "lenient_3r", "ThreeR",
+   "awd_1drydown", "awd_2drydown")
> 
> 
> # Function for SE cluster----
> 
> cluster_fn <- function (dataset, dependent_set, corr_list, fixed_effect = TRUE){
+   ols_models <- list()
+   model_results <- list()
+   
+   for (dependent_var in dependent_set) {
+     # Select appropriate weight variable
+     if (dependent_var %in% c("CIAT.related", "DMC", "CMD")) {
+       weight_var <- "weight_cass"
+     }else if (dependent_var %in% c ("StrainB_edited")) {
+       weight_var <- "weight_gift"
+     }else if (dependent_var %in% c("IRRI_Parentage_edited", "Saltol")) {
+       weight_var <- "weight_rice_DNA"
+     }else {
+       weight_var <- "weight_final_rice"
+     }
+     
+     
+     if (fixed_effect) {
+       formula <- as.formula(paste(dependent_var, "~", paste(c(corr_list, "Region"), collapse = " + ")))
+     } else {
+       formula <- as.formula(paste(dependent_var, "~", paste(corr_list, collapse = " + ")))
+     }
+     
+     # Run weighted OLS model with clustered standard errors
+     model <- lm_robust(formula, data = dataset, weights = .data[[weight_var]], clusters = EA_ID)
+     ols_models[[dependent_var]] <- model
+     
+     # Store tidy model result
+     model_results[[dependent_var]] <- tidy(model, conf.int = TRUE)
+     model_results[[dependent_var]]$dependent_var <- dependent_var
+   }
+   
+   
+   # Combine all results into a single data frame
+   result <- do.call(rbind, model_results)
+   
+   # Remove the intercept row
+   result <- result %>% filter(term != "(Intercept)")
+   
+   # Rename Innovations
+   result <- result %>%
+     mutate(dependent_var = case_when(
+       dependent_var == 'IRRI_Parentage_edited' ~ "CGIAR-related Rice Varieties",
+       dependent_var == 'mech_mini_combiner' ~ "Mini-Combine Harvester (MCHB)",
+       dependent_var == 'mech_combine_harvester' ~ "Combine Harvester (CHB)",
+       dependent_var == 'mech_straw_baler' ~ "Rice Straw Baler", 
+       dependent_var == 'mech_row_seeder' ~ 'Drum Seeder',
+       dependent_var == "mech_row_drum_seeder" ~ "Drum Seeder",
+       dependent_var == 'Saltol' ~ "Salt-tolerant Rice Varieties (STRVs)",
+       dependent_var == "CIAT.related" ~ "CGIAR-related Cassava Varieties",
+       dependent_var == "DMC" ~ "Cassava DMC QTL", 
+       dependent_var == "CMD" ~ "Cassava Mosaic Disease (CMD)-resistant Cassava Varieties",
+       dependent_var == "SWCP" ~ "Sustainable Water for Coffee Production", 
+       dependent_var == "d_1m5r_seed_100kg" ~ "1R: Seed rate (strict)", 
+       dependent_var == "mech_seed_blower" ~ "Seed blower", 
+       dependent_var == "mech_laser_level" ~ "Laser Land Levelling (LLL)",
+       dependent_var == "StrainB_edited" ~ "Genetically Improved Farmed Tilapia (GIFT)-derived strains",
+       dependent_var == "d_1m5r_seed_120kg" ~ "1R: Seed rate (lenient)", 
+       dependent_var == "strict_2r" ~ "2R: Nitrogen use (strict)", 
+       dependent_var == "lenient_2r" ~ "2R: Nitrogen use (lenient)", 
+       dependent_var == "strict_3r" ~ "3R: Pesticide use (strict)", 
+       dependent_var == "lenient_3r" ~ "3R: Pesticide use (lenient)", 
+       dependent_var == "ThreeR" ~ "Three Reductions, Three Gains (3R3G) and One Must Do, Five Reductions (1M5R - lenient)", 
+       dependent_var == "awd_1drydown" ~ "Alternate Wetting and Drying (1 dry-down)", 
+       dependent_var == "awd_2drydown" ~ "Alternate Wetting and Drying (At least 2 dry-downs)", 
+       dependent_var == "csmap_final" ~ "Climate-Smart Mapping and Adaptation Planning (CS-MAP)",
+       TRUE ~ dependent_var
+     ))
+   
+   # Rename correlates
+   result <- result %>%
+     mutate(term = case_when(
+       term == 'ethnic' ~ 'HH head from an ethnic minority',
+       term == 'female' ~ 'HH head is female',
+       term == 'age' ~ 'Age of hh head (std)',
+       term == 'edu_grade' ~ 'HH head highest completed grade (std)',
+       term == 'n_member' ~ 'Household size (std)',
+       term == 'THUNHAP_mil' ~ 'Annual income (std)',
+       term == 'TONGCHITIEU' ~ 'Annual consumption (in million VND)',
+       term == 'land_area_ha' ~ 'Total agricultural land (in ha)',
+       term == 'internet' ~ 'Household has internet',
+       term == 'poor_commune' ~ 'Poor Commune label',
+       term == 'main_str_asphalt' ~ 'Main road is asphalt',
+       term == 'dist_market_wholesale' ~ 'Distance to wholesale market (km)',
+       term == 'dummy_protection_staff' ~ 'Commune has extension agent',
+       term == 'Bottom_20' ~ '% of households in bottom 20% of annual consumption',
+       term == 'Bottom_40' ~ '% of households in bottom 40% of annual consumption',
+       TRUE ~ term
+     ),
+     significance = case_when(
+       p.value < 0.001 ~ "***",
+       p.value < 0.05 ~ "**",
+       p.value < 0.1 ~ "*",
+       TRUE ~ ""
+     )
+     )
+ }
> 
> 
> # Function to plot SE cluster ----
> 
> plot_cluster_fn <- function (dataset, name){
+   
+   Plots_list <- list()
+   
+   for (innovation in unique(dataset[["dependent_var"]])) {
+     
+     # Subset data for each innovation
+     data_subset <- dataset %>% 
+       filter(dependent_var == innovation)
+     
+     p <- ggplot(data_subset, aes(x = estimate, y = term)) +
+     geom_point(size = 5, color = "#2c3e50") +
+     geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.4, color = "#34495e", size = 1) +
+     theme_minimal(base_size = 15) +
+     theme(
+       panel.background = element_blank(),
+       plot.background = element_blank(),
+       axis.title.x = element_blank(),  # Remove x-axis title
+       axis.title.y = element_blank(),  # Remove y-axis title
+       axis.text.y = element_text(size = 14),
+       axis.text.x = element_text(size = 14),
+       plot.title = element_blank(),  # Remove plot title
+       panel.grid.major.y = element_line(color = "grey85", linetype = "dashed"),
+       panel.grid.minor.y = element_blank(),
+       panel.grid.minor.x = element_blank(),
+       axis.ticks.y = element_blank(),  # Drop y-axis ticks
+       axis.ticks.length = unit(0.2, "cm"),
+       plot.margin = unit(c(0.5, 1, 0.5, 1), "cm"),
+       strip.text = element_text(size = 14)
+     ) +
+     labs(x = "Coefficient", y = " ") +
+     geom_vline(xintercept = 0, linetype = "dashed", color = "black", size = 1) +
+     scale_y_discrete(limits = rev(levels(factor(data_subset$term))), expand = c(0, 0.1))  # Reverse order of y-axis labels
+ 
+   # Store the plot in the list
+   Plots_list[[innovation]] <- p
+   }
+ 
+   # Save the plots
+   for (innovation in names(Plots_list)) {
+     # Generate file name with special case for 3R3G and 1M5R
+     file_name <- if (innovation == "Three Reductions, Three Gains (3R3G) and One Must Do, Five Reductions (1M5R - lenient)") {
+       paste0("C:/Users/BThanh/SPIA Dropbox/SPIA General/SPIA 2019-2024/5. OBJ.3-Data collection/Country teams/Vietnam/Report 2024/Reproducible Scripts/Output/Multivariate_analysis/", 
+              name, "_Adopters of_3R3G, 1M5R.png")
+     } else {
+       paste0("C:/Users/BThanh/SPIA Dropbox/SPIA General/SPIA 2019-2024/5. OBJ.3-Data collection/Country teams/Vietnam/Report 2024/Reproducible Scripts/Output/Multivariate_analysis/", 
+              name, "_Adopters of_", gsub("[/:?*<>|]", "-", innovation), ".png")
+     }
+   
+     # Save the plot
+     png(file_name, width = 1080, height = 600)
+     print(Plots_list[[innovation]])  # Print the plot to save
+     dev.off()  # Close the device
+   }
+   return (Plots_list)
+ }
> 
> # Function to convert results to publishable format----
> publish_fn <- function (result_table){
+   results_Overview <- result_table %>%
+     mutate(estimate = round(estimate, 2),  # Round estimates to two decimal places
+            estimate_significance = paste(estimate, significance)) %>%
+     select(term, dependent_var, estimate_significance) %>%
+     spread(key = term, value = estimate_significance)  # Make terms as columns
+   
+   table (results_Overview$dependent_var)
+   
+   
+   results_Overview <- results_Overview %>%
+     mutate(dependent_var = case_when(
+       dependent_var == "IRRI-related rice varietie" ~ "CGIAR-related Rice Varieties",
+       dependent_var == "CIAT-related cassava varieties" ~ "CGIAR-related Cassava Varieties",
+       TRUE ~ dependent_var  # Keep original if no match is found
+     ))
+   
+   if ("Main road is asphalt" %in% names(results_Overview)) {
+     results_Overview <- results_Overview %>%
+       relocate(c(
+         "Age of hh head (std)",
+         "Annual income (std)",
+         "HH head from an ethnic minority",
+         "HH head highest completed grade (std)",
+         "HH head is female",
+         "Household has internet",
+         "Household size (std)",
+         "Main road is asphalt",
+         "Total agricultural land (in ha)"
+       ), .after = "dependent_var")
+   } else {
+     results_Overview <- results_Overview %>%
+       relocate(c(
+         "Age of hh head (std)",
+         "Annual income (std)",
+         "HH head from an ethnic minority",
+         "HH head highest completed grade (std)",
+         "HH head is female",
+         "Household has internet",
+         "Household size (std)",
+         "Total agricultural land (in ha)"
+       ), .after = "dependent_var")
+   }
+   
+   results_Overview <- results_Overview %>%
+     rename("Innovation" = "dependent_var")
+   
+   return (results_Overview)
+ }
> 
> 
> # 1. OLS with SE clustered at the EA level ----
> ## VH22: SE clustering----
> VH22_OLS_Results <- cluster_fn (df_22, All_22, corr_list_22, fixed_effect = FALSE) %>%
+   select (-outcome)
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
> 
> rownames(VH22_OLS_Results) <- NULL
> 
> write.csv(VH22_OLS_Results, "Output/Multivariate_analysis/VH22_OLS_Results.csv",
+           row.names = FALSE)
> 
> 
> ## VH23: SE clustering----
> VH23_OLS_Results <- cluster_fn (df_23, All_23, corr_list_23, fixed_effect = FALSE) 
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
> 
> 
> VH23_OLS_Results <- VH23_OLS_Results %>%
+   select (-outcome)
> 
> rownames(VH23_OLS_Results) <- NULL
> 
> 
> write.csv (VH23_OLS_Results, "Output/Multivariate_Analysis/VH23_OLS_Results.csv",
+            row.names = FALSE)
> 
> 
> 
> # 2. Add regional fixed effects----
> ## VH22: Fixed effect ----
> VH22_fixed <- cluster_fn (df_22, All_22, corr_list_22, fixed_effect = TRUE)  %>%
+   select (-outcome)
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
> 
> rownames(VH22_fixed) <- NULL
> 
> write.csv(VH22_fixed, "Output/Multivariate_analysis/VH22_OLS_Results_FE.csv",
+           row.names = FALSE)
> 
> 
> ## VH23: Fixed effect ----
> VH23_fixed <- cluster_fn (df_23, dependent_23_noCSMAP, corr_list_23, fixed_effect = TRUE) %>%
+   select (-outcome)
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
Warning in eval(quote({ :
  Some observations have missingness in the weights variable(s) but not in the outcome or covariates. These observations have been dropped.
> 
> rownames(VH23_fixed) <- NULL
> 
> write.csv(VH23_fixed, "Output/Multivariate_analysis/VH23_OLS_Results_FE.csv",
+           row.names = FALSE)
> 
> # 3. Convert the table into publishable formats ----
> 
> ## Not having regional fixed effects ----
> 
> table_22_non_fixed <- publish_fn (VH22_OLS_Results) %>%
+   filter (Innovation %in% c("CGIAR-related Rice Varieties", "Salt-tolerant Rice Varieties (STRVs)"))
> 
> order_dependent <- c("Genetically Improved Farmed Tilapia (GIFT)-derived strains",
+                      "CGIAR-related Cassava Varieties",
+                      "Cassava Mosaic Disease (CMD)-resistant Cassava Varieties",
+                      "Laser Land Levelling (LLL)",
+                      # "Seed blower",
+                      "Combine Harvester (CHB)",
+                      "Rice Straw Baler",
+                      "1R: Seed rate (lenient)",
+                      "2R: Nitrogen use (lenient)",
+                      "3R: Pesticide use (lenient)",
+                      "Three Reductions, Three Gains (3R3G) and One Must Do, Five Reductions (1M5R - lenient)",
+                      "Alternate Wetting and Drying (1 dry-down)",
+                      'Drum Seeder',
+                      "Climate-Smart Mapping and Adaptation Planning (CS-MAP)",
+                      "CS-MAPs")
> 
> table_23_non_fixed <- publish_fn (VH23_OLS_Results) %>%
+   filter (Innovation %in% order_dependent) %>%
+   mutate (order_var = factor (Innovation, levels = order_dependent)) %>%
+   arrange (order_var) %>%
+   select (-order_var)
> 
> 
> prep_table <- full_join (table_22_non_fixed, table_23_non_fixed)
Joining with `by = join_by(Innovation, `Age of hh head (std)`, `Annual income (std)`, `HH head from an
ethnic minority`, `HH head highest completed grade (std)`, `HH head is female`, `Household has internet`,
`Household size (std)`, `Total agricultural land (in ha)`)`
> 
> sub_title1 <- c("Aqualculture and Capture fisheries", rep (NA, 9))
> sub_title2 <- c("Breeding Innovations", rep (NA, 9))
> sub_title3 <- c("Climate Change Adaptation Options", rep (NA, 9))
> sub_title4 <- c("Mechanisation", rep (NA, 9))
> sub_title5 <- c("Sustainable Intensification Practices", rep (NA, 9))
> 
> prep_table <- prep_table %>%
+   rbind(sub_title1, sub_title2, sub_title3, sub_title4, sub_title5)
>   
> prep_table_order <- prep_table %>%
+   mutate (order = case_when(Innovation == "Aqualculture and Capture fisheries" ~ 0.1,
+                             Innovation == "Genetically Improved Farmed Tilapia (GIFT)-derived strains" ~ 0.2,
+                             Innovation == "Breeding Innovations" ~ 0.3,
+                             Innovation == "Climate Change Adaptation Options" ~ 5.1,
+                             Innovation == "Climate-Smart Mapping and Adaptation Planning (CS-MAP)" ~ 5.2,
+                             Innovation == "Mechanisation" ~ 4.3,
+                             Innovation == "Sustainable Intensification Practices" ~ 8.1,
+                             TRUE ~ row_number())) %>%
+   arrange(order) %>%
+   select (-order)
> 
> write.csv (prep_table_order, "Output/Multivariate_analysis/Tab9.csv",
+            row.names = FALSE)  #write as csv file
> 
> table9_no_region_effect <- flextable (prep_table_order) %>% 
+   align(align = "center", part = "header") %>%
+   align(align = "center", part = "body", j = 2:9) %>%
+   colformat_double(j = c(2:9), digits = 2, na_str = "NA") %>%
+   merge_at(part = "body", i = 1) %>%
+   merge_at(part = "body", i = 3) %>%
+   # merge_at(part = "body", i = 6) %>%
+   merge_at(part = "body", i = 7) %>%
+   merge_at(part = "body", i = 9) %>%
+   merge_at(part = "body", i = 14) %>%
+   bold(part = "body", i = c(1,3,7, 9,14)) %>%
+   # colformat_char(j=3) %>%
+   bold(i = 1, part = "header") %>%
+   border_inner_h(part = "body", fp_border(width = 0.5)) %>%
+   border_inner_h(part = "header", fp_border(width = 0.5)) %>%
+   border(i = 1, border.top = fp_border(width = 2)) %>%
+   autofit()
> 
> 
> table9_no_region_effect
> 
> save_as_html(table9_no_region_effect, path = "Output/Multivariate_analysis/Tab9.html" )
> 
> 
> 
> ## With regional fixed effects ----
> table_22_fixed <- publish_fn (VH22_fixed) %>%
+   filter (Innovation %in% c("CGIAR-related Rice Varieties", "Salt-tolerant Rice Varieties (STRVs)"))
> 
> order_dependent <- c("Genetically Improved Farmed Tilapia (GIFT)-derived strains",
+                      "CGIAR-related Cassava Varieties",
+                      "Cassava Mosaic Disease (CMD)-resistant Cassava Varieties",
+                      "Laser Land Levelling (LLL)",
+                       # "Seed blower",
+                      "Combine Harvester (CHB)",
+                      "Rice Straw Baler",
+                      "1R: Seed rate (lenient)",
+                      "2R: Nitrogen use (lenient)",
+                      "3R: Pesticide use (lenient)",
+                      "Three Reductions, Three Gains (3R3G) and One Must Do, Five Reductions (1M5R - lenient)",
+                      "Alternate Wetting and Drying (1 dry-down)",
+                      "Drum Seeder",
+                      "Sustainable Water for Coffee Production",
+                      "Climate-Smart Mapping and Adaptation Planning (CS-MAP)")
> 
> table_23_fixed <- publish_fn (VH23_fixed) %>%
+   filter (Innovation %in% order_dependent) %>%
+   mutate (order_var = factor (Innovation, levels = order_dependent)) %>%
+   arrange (order_var) %>%
+   select (-order_var)
> 
> 
> prep_table_fixed <- full_join (table_22_fixed, table_23_fixed)
Joining with `by = join_by(Innovation, `Age of hh head (std)`, `Annual income (std)`, `HH head from an
ethnic minority`, `HH head highest completed grade (std)`, `HH head is female`, `Household has internet`,
`Household size (std)`, `Total agricultural land (in ha)`, Region2_NMMA, Region3_NCCCA, `Region4_Central
Highlands`, `Region5_South East`, Region6_MRD)`
> 
> sub_title1 <- c("Aqualculture and Capture fisheries", rep (NA, 14))
> sub_title2 <- c("Breeding Innovations", rep (NA, 14))
> sub_title3 <- c("Climate Change Adaptation Options", rep (NA, 14))
> sub_title4 <- c("Mechanisation", rep (NA, 14))
> sub_title5 <- c("Sustainable Intensification Practices", rep (NA, 14))
> 
> prep_table_fixed <- prep_table_fixed %>%
+   rbind(sub_title1, sub_title2, sub_title3, sub_title4, sub_title5)
> 
> prep_table_order_fixed <- prep_table_fixed %>%
+   mutate (order = case_when(Innovation == "Aqualculture and Capture fisheries" ~ 0.1,
+                             Innovation == "Genetically Improved Farmed Tilapia (GIFT)-derived strains" ~ 0.2,
+                             Innovation == "Breeding Innovations" ~ 0.3,
+                             Innovation == "Climate Change Adaptation Options" ~ 5.1,
+                             Innovation == "Climate-Smart Mapping and Adaptation Planning (CS-MAP)" ~ 5.2,
+                             Innovation == "Mechanisation" ~ 5.3,
+                             Innovation == "Sustainable Intensification Practices" ~ 8.1,
+                             TRUE ~ row_number())) %>%
+   arrange(order) %>%
+   select (-order) %>%
+   filter (Innovation != "Climate Change Adaptation Options")
> 
> colnames(prep_table_order_fixed)[colnames(prep_table_order_fixed)=="Region2_NMMA"] <- "Region: NMMA"
> colnames(prep_table_order_fixed)[colnames(prep_table_order_fixed)=="Region3_NCCCA"] <- "Region: NCCCA"
> colnames(prep_table_order_fixed)[colnames(prep_table_order_fixed)=="Region4_Central Highlands"] <- "Region: Central Highlands"
> colnames(prep_table_order_fixed)[colnames(prep_table_order_fixed)=="Region5_South East"] <- "Region: South East"
> colnames(prep_table_order_fixed)[colnames(prep_table_order_fixed)=="Region6_MRD"] <- "Region: MRD"
> 
> View(prep_table_order_fixed)
> 
> write.csv (prep_table_order_fixed, "Output/Multivariate_analysis/Appendix.C.csv",
+            row.names = FALSE)  #write as csv file
> 
> table9_fixed <- flextable (prep_table_order_fixed) %>% 
+   align(align = "center", part = "header") %>%
+   align(align = "center", part = "body", j = 2:15) %>%
+   colformat_double(j = c(2:15), digits = 2, na_str = "NA") %>%
+   merge_at(part = "body", i = 1) %>%
+   merge_at(part = "body", i = 3) %>%
+   # merge_at(part = "body", i = 6) %>%
+   merge_at(part = "body", i = 8) %>%
+   merge_at(part = "body", i = 12) %>%
+   bold(part = "body", i = c(1,3,8,12)) %>%
+   # colformat_char(j=3) %>%
+   bold(i = 1, part = "header") %>%
+   border_inner_h(part = "body", fp_border(width = 0.5)) %>%
+   border_inner_h(part = "header", fp_border(width = 0.5)) %>%
+   border(i = 1, border.top = fp_border(width = 2)) %>%
+   autofit()
> 
> 
> table9_fixed
> 
> save_as_html(table9_fixed, path = "Output/Multivariate_analysis/Appendix.C.html" )
> 
> 
> 
> 
> 
> # 4. Plotting----
> ## VH22: Plotting----
> plot_cluster_fn(VH22_OLS_Results, "VH22") #plot and save figures
Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
ℹ Please use `linewidth` instead.
This warning is displayed once every 8 hours.
Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.
Warning in png(file_name, width = 1080, height = 600) :
  unable to open file 'C:/Users/BThanh/SPIA Dropbox/SPIA General/SPIA 2019-2024/5. OBJ.3-Data collection/Country teams/Vietnam/Report 2024/Reproducible Scripts/Output/Multivariate_analysis/VH22_Adopters of_Drum Seeder.png' for writing
Warning in png(file_name, width = 1080, height = 600) :
  opening device failed
Error in png(file_name, width = 1080, height = 600) : 
  unable to start png() device
Called from: png(file_name, width = 1080, height = 600)
Browse[1]> 
> rm (list = ls()) #start clean
> rm (list = ls()) #start clean
> format_ID <- function(df, columns, widths, pad_char = "0") {
+   # Loop through each column and its corresponding width
+   for (i in seq_along(columns)) {
+     column <- columns[i]
+     width <- widths[i]
+     
+     # Pad the column with the specified width and character
+     df[[column]] <- str_pad(df[[column]], width = width, pad = pad_char)
+   }
+   return(df)
+ }
> 
> 
> # Define columns and widths
> columns <- c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO")
> widths <- c(2, 3, 5, 3, 3)
> 
> 
> 
> 
> # Curl data from GitHub ----
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> 
> 
> # Load data----
> curl_function ("data/processed/VH22_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH22_data.csv'
Content type 'text/plain; charset=utf-8' length 2257649 bytes (2.2 MB)
downloaded 2.2 MB

> df_22 <- read.csv ("data/processed/VH22_data.csv")
> df_22 <- format_ID(df_22, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), widths = c(2, 3, 5, 3, 3) )
> df_22$IDHO <- paste0 (df_22$MAXA, df_22$MADIABAN, df_22$HOSO)
> 
> 
> 
> 
> curl_function ("data/processed/VH23_data.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/VH23_data.csv'
Content type 'text/plain; charset=utf-8' length 2319044 bytes (2.2 MB)
downloaded 2.2 MB

> df_23 <- read.csv ("data/processed/VH23_data.csv")
> df_23 <- format_ID(df_23, columns = c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO"), widths = c(2, 3, 5, 3, 3) )
> df_23$IDHO <- paste0 (df_23$MAXA, df_23$MADIABAN, df_23$HOSO)
> 
> 
> df_23 <- df_23 %>%
+   rename ("mech_row_seeder" = "mech_row_drum_seeder")
> 
> df <- full_join (df_22, df_23)
Joining with `by = join_by(MATINH, IDHO, MAHUYEN, MAXA, MADIABAN, HOSO, mech_combine_harvester,
mech_straw_baler, mech_row_seeder, mech_seed_blower, panel, mean_csmap, csmap_final, weight_final_rice,
THUNHAP, TONGCHITIEU, ethnic, rls_head, age, edu_grade, n_member, male, female, internet, land_area_sum,
Quintiles, Bottom_20, Bottom_40)`
> 
> 
> 
> # PFES data:
> curl_function ("data/raw/VHLSS_2024_Commune/SPIA_ThongTinXa_Q1-3.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2024_Commune/SPIA_ThongTinXa_Q1-3.csv'
Content type 'text/plain; charset=utf-8' length 266833 bytes (260 KB)
downloaded 260 KB

> pfes <- read.csv ("data/raw/VHLSS_2024_Commune/SPIA_ThongTinXa_Q1-3.csv") %>%
+   select (c(MATINH, MAHUYEN, MAXA, M43_C1))
> 
> 
> pfes <- pfes %>%
+   mutate (pfes_dummy = case_when (M43_C1 == 1 ~ 1,
+                                   TRUE ~ 0)) %>%
+   select (-M43_C1) %>%
+   mutate (panel = 2024)
> pfes <- format_ID(pfes, columns = c("MATINH", "MAHUYEN", "MAXA"), widths = c(2,3,5))
> 
> 
> curl_function ("data/raw/Weight/Census_household_communelevel_clean.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/Census_household_communelevel_clean.csv'
Content type 'text/plain; charset=utf-8' length 665472 bytes (649 KB)
downloaded 649 KB

> n_hh_pop <- read.csv ("data/raw/Weight/Census_household_communelevel_clean.csv") %>%
+   select (c(MATINH, MAXA, n_hh)) %>%
+   rename (n_hh_pop = n_hh) 
> #merge by Commune ID (MAXA) because of some administrative change 
> # (486 missing if merge by prov, dist, comm ID --> 470 missing if merge by prov and comm ID)
> 
> n_hh_pop <- format_ID(n_hh_pop, columns = c("MATINH", "MAXA"), widths = c(2, 5))
> 
> pfes_joined <- pfes %>%
+   left_join (n_hh_pop)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> # Merge datasets
> 
> df <- df%>%
+   full_join (pfes_joined)  
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, panel)`
> 
> 
> 
> # Categorize provinces into region
> curl_function ("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv'
Content type 'text/plain; charset=utf-8' length 1799 bytes
downloaded 1799 bytes

> prov <- read.csv ("data/raw/VHLSS_2022_Household/datasets/Provinces_IDs.csv") %>%
+   select (c(MATINH, Region, Province_name)) 
> prov <- format_ID (prov, c("MATINH"), c(2))
> df <- df %>%
+   left_join (prov)%>%
+   relocate (c("Region", "Province_name"), .before = "MATINH") 
Joining with `by = join_by(MATINH)`
> 
> 
> #Prep data:
> df <- df %>%
+   mutate (CMD_edited = case_when(CMD == TRUE ~ 1,
+                                  CMD == FALSE ~ 0,
+                                  TRUE ~ NA),
+           DMC_edited = case_when (DMC == TRUE ~ 1,
+                                   DMC == FALSE ~ 0,
+                                   TRUE ~ NA)) %>%
+   mutate (lenient_3r3g = case_when (d_1m5r_seed_120kg == 1 & lenient_2r == 1 & lenient_3r == 1 ~ 1,
+                                     d_1m5r_seed_120kg != 1 | lenient_2r != 1 | lenient_3r != 1 ~ 0,
+                                     TRUE ~ NA),
+           strict_3r3g = case_when (d_1m5r_seed_100kg == 1 & strict_2r == 1 & strict_3r ==1 ~ 1,
+                                    d_1m5r_seed_100kg != 1 | strict_2r != 1 | strict_3r !=1 ~ 0)) %>%
+   mutate (ID_EA = paste0 (MAXA, MADIABAN)) 
> 
> 
> 
> 
> 
> 
> # Adoption table ----
> 
> ## Adoption at HH level and reach of adoption ----
> 
> 
> ### Rice ----
> 
> var_list <- c(
+   "csmap_final",
+   "mech_mini_combiner", "mech_combine_harvester", "mech_straw_baler",
+   "mech_laser_level", "mech_row_seeder", "mech_seed_blower",
+   
+   "lenient_1m", "d_1m5r_certified", 
+   "d_1m5r_seed_120kg", "d_1m5r_seed_100kg", 
+   "lenient_2r", "strict_2r",
+   "lenient_3r", "strict_3r",
+   "lenient_3r3g", "strict_3r3g",
+   "awd_1drydown",
+   "awd_2drydown",
+   "straw_rm_livestock", "straw_rm_mushroom", 
+   "straw_rm_compost")
> 
> 
> 
> 
> 
> 
> 
> # % OF SURVEYED HOUSEHOLDS THAT ADOPT INNOVATION: 
> 
> adoption_hh <- list()  #prep result list
> 
> for (var in var_list){
+   adopt <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (.data[[var]]) %>%
+     filter (panel == max(panel, na.rm = TRUE)) %>%
+     ungroup () %>%
+     mutate (n_obs = n(),
+             panel = panel) %>%
+     group_by (n_obs, panel, .data[[var]], Region) %>%
+     summarise (twt = sum(weight_final_rice, na.rm = TRUE),
+                n_adopt = n()) %>%
+     group_by (Region) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE),
+             n_sub = sum (n_adopt)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt, twt))
+   
+   adoption_hh[[var]] <- adopt
+   
+ }
`summarise()` has grouped output by 'n_obs', 'panel', 'csmap_final'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'mech_mini_combiner'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'mech_combine_harvester'. You can override using
the `.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'mech_straw_baler'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'mech_laser_level'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'mech_row_seeder'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'mech_seed_blower'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'lenient_1m'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'd_1m5r_certified'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'd_1m5r_seed_120kg'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'd_1m5r_seed_100kg'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'lenient_2r'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'strict_2r'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'lenient_3r'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'strict_3r'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'lenient_3r3g'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'strict_3r3g'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'awd_1drydown'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'awd_2drydown'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'straw_rm_livestock'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'straw_rm_mushroom'. You can override using the
`.groups` argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'straw_rm_compost'. You can override using the
`.groups` argument.
> 
> 
> adoption_hh
$csmap_final
# A tibble: 1 × 7
# Groups:   Region [1]
  n_obs panel csmap_final Region n_adopt n_sub adoption
  <int> <dbl>       <int> <chr>    <int> <int>    <dbl>
1  1069  2023           1 6_MRD      275  1069     27.4

$mech_mini_combiner
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel mech_mini_combiner Region              n_adopt n_sub adoption
  <int> <dbl>              <int> <chr>                 <int> <int>    <dbl>
1 13986  2022                  1 1_RRD                   225  2732     8.39
2 13986  2022                  1 2_NMMA                  715  5087    13.1 
3 13986  2022                  1 3_NCCCA                 473  3725    13.3 
4 13986  2022                  1 4_Central Highlands     154   763    13.3 
5 13986  2022                  1 5_South East              5   107     4.93
6 13986  2022                  1 6_MRD                    22  1572     1.23

$mech_combine_harvester
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel mech_combine_harvester Region              n_adopt n_sub adoption
  <int> <dbl>                  <int> <chr>                 <int> <int>    <dbl>
1 13217  2023                      1 1_RRD                  2424  2519     96.5
2 13217  2023                      1 2_NMMA                 1453  4712     35.6
3 13217  2023                      1 3_NCCCA                2854  3597     80.6
4 13217  2023                      1 4_Central Highlands     351   777     49.8
5 13217  2023                      1 5_South East             77    80     97.7
6 13217  2023                      1 6_MRD                  1454  1532     93.0

$mech_straw_baler
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel mech_straw_baler Region              n_adopt n_sub adoption
  <int> <dbl>            <int> <chr>                 <int> <int>    <dbl>
1 13217  2023                1 1_RRD                    64  2519     2.16
2 13217  2023                1 2_NMMA                   96  4712     2.16
3 13217  2023                1 3_NCCCA                 228  3597     6.46
4 13217  2023                1 4_Central Highlands      23   777     3.13
5 13217  2023                1 5_South East             10    80    11.0 
6 13217  2023                1 6_MRD                    75  1532     5.70

$mech_laser_level
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel mech_laser_level Region              n_adopt n_sub adoption
  <int> <dbl>            <int> <chr>                 <int> <int>    <dbl>
1 13217  2023                1 1_RRD                   173  2519     7.86
2 13217  2023                1 2_NMMA                   72  4712     2.02
3 13217  2023                1 3_NCCCA                 156  3597     5.56
4 13217  2023                1 4_Central Highlands      65   777    12.1 
5 13217  2023                1 5_South East              6    80    11.5 
6 13217  2023                1 6_MRD                    69  1532     3.62

$mech_row_seeder
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel mech_row_seeder Region              n_adopt n_sub adoption
  <int> <dbl>           <int> <chr>                 <int> <int>    <dbl>
1 13217  2023               1 1_RRD                    28  2519   1.04  
2 13217  2023               1 2_NMMA                    2  4712   0.0494
3 13217  2023               1 3_NCCCA                  17  3597   0.512 
4 13217  2023               1 4_Central Highlands       3   777   0.226 
5 13217  2023               1 5_South East              1    80   0.779 
6 13217  2023               1 6_MRD                   150  1532   9.99  

$mech_seed_blower
# A tibble: 5 × 7
# Groups:   Region [5]
  n_obs panel mech_seed_blower Region       n_adopt n_sub adoption
  <int> <dbl>            <int> <chr>          <int> <int>    <dbl>
1 13217  2023                1 1_RRD             15  2519    0.354
2 13217  2023                1 2_NMMA             2  4712    0.122
3 13217  2023                1 3_NCCCA           17  3597    0.690
4 13217  2023                1 5_South East      36    80   48.9  
5 13217  2023                1 6_MRD            367  1532   25.9  

$lenient_1m
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel lenient_1m Region              n_adopt n_sub adoption
  <int> <dbl>      <int> <chr>                 <int> <int>    <dbl>
1 13216  2023          1 1_RRD                  2409  2519     95.7
2 13216  2023          1 2_NMMA                 3946  4711     87.2
3 13216  2023          1 3_NCCCA                3120  3597     89.6
4 13216  2023          1 4_Central Highlands     562   777     72.4
5 13216  2023          1 5_South East             74    80     92.6
6 13216  2023          1 6_MRD                  1291  1532     83.6

$d_1m5r_certified
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel d_1m5r_certified Region              n_adopt n_sub adoption
  <int> <dbl>            <int> <chr>                 <int> <int>    <dbl>
1 13216  2023                1 1_RRD                  2141  2519     84.4
2 13216  2023                1 2_NMMA                 3009  4711     70.2
3 13216  2023                1 3_NCCCA                2434  3597     73.2
4 13216  2023                1 4_Central Highlands     324   777     44.3
5 13216  2023                1 5_South East             74    80     92.6
6 13216  2023                1 6_MRD                  1016  1532     66.8

$d_1m5r_seed_120kg
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel d_1m5r_seed_120kg Region              n_adopt n_sub adoption
  <int> <dbl>             <int> <chr>                 <int> <int>    <dbl>
1 12244  2023                 1 1_RRD                  2403  2459     97.6
2 12244  2023                 1 2_NMMA                 3794  4139     93.0
3 12244  2023                 1 3_NCCCA                2284  3485     71.0
4 12244  2023                 1 4_Central Highlands     160   677     19.5
5 12244  2023                 1 5_South East             19    66     26.2
6 12244  2023                 1 6_MRD                   371  1418     22.9

$d_1m5r_seed_100kg
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel d_1m5r_seed_100kg Region              n_adopt n_sub adoption
  <int> <dbl>             <int> <chr>                 <int> <int>    <dbl>
1 12244  2023                 1 1_RRD                  2396  2459     97.3
2 12244  2023                 1 2_NMMA                 3677  4139     89.6
3 12244  2023                 1 3_NCCCA                1914  3485     63.2
4 12244  2023                 1 4_Central Highlands     120   677     15.2
5 12244  2023                 1 5_South East             10    66     13.2
6 12244  2023                 1 6_MRD                   209  1418     12.4

$lenient_2r
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel lenient_2r Region              n_adopt n_sub adoption
  <int> <dbl>      <int> <chr>                 <int> <int>    <dbl>
1 13061  2023          1 1_RRD                  1660  2507     67.6
2 13061  2023          1 2_NMMA                 2591  4670     57.0
3 13061  2023          1 3_NCCCA                2268  3544     61.5
4 13061  2023          1 4_Central Highlands     409   771     52.9
5 13061  2023          1 5_South East             37    77     39.0
6 13061  2023          1 6_MRD                  1083  1492     71.8

$strict_2r
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel strict_2r Region              n_adopt n_sub adoption
  <int> <dbl>     <int> <chr>                 <int> <int>    <dbl>
1 13113  2023         1 1_RRD                  1027  2512     41.3
2 13113  2023         1 2_NMMA                 1414  4707     30.4
3 13113  2023         1 3_NCCCA                1498  3548     37.3
4 13113  2023         1 4_Central Highlands     266   775     32.7
5 13113  2023         1 5_South East             26    78     25.6
6 13113  2023         1 6_MRD                   815  1493     54.8

$lenient_3r
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel lenient_3r Region              n_adopt n_sub adoption
  <int> <dbl>      <int> <chr>                 <int> <int>    <dbl>
1 13087  2023          1 1_RRD                   452  2512    17.9 
2 13087  2023          1 2_NMMA                 1307  4651    24.6 
3 13087  2023          1 3_NCCCA                 939  3540    25.3 
4 13087  2023          1 4_Central Highlands     214   772    24.4 
5 13087  2023          1 5_South East              6    80     6.52
6 13087  2023          1 6_MRD                   185  1532    13.0 

$strict_3r
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel strict_3r Region              n_adopt n_sub adoption
  <int> <dbl>     <int> <chr>                 <int> <int>    <dbl>
1 13088  2023         1 1_RRD                   315  2512    12.3 
2 13088  2023         1 2_NMMA                 1180  4651    21.3 
3 13088  2023         1 3_NCCCA                 816  3540    22.1 
4 13088  2023         1 4_Central Highlands     197   772    22.0 
5 13088  2023         1 5_South East              3    80     4.35
6 13088  2023         1 6_MRD                   170  1533    11.7 

$lenient_3r3g
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel lenient_3r3g Region              n_adopt n_sub adoption
  <int> <dbl>        <dbl> <chr>                 <int> <int>    <dbl>
1 12925  2023            1 1_RRD                   304  2510   12.4  
2 12925  2023            1 2_NMMA                  592  4508   12.1  
3 12925  2023            1 3_NCCCA                 428  3560   11.6  
4 12925  2023            1 4_Central Highlands      44   760    4.70 
5 12925  2023            1 5_South East              1    79    0.313
6 12925  2023            1 6_MRD                    52  1508    3.11 

$strict_3r3g
# A tibble: 5 × 7
# Groups:   Region [5]
  n_obs panel strict_3r3g Region              n_adopt n_sub adoption
  <int> <dbl>       <dbl> <chr>                 <int> <int>    <dbl>
1 12971  2023           1 1_RRD                   125  2515     4.81
2 12971  2023           1 2_NMMA                  297  4529     6.01
3 12971  2023           1 3_NCCCA                 179  3577     4.96
4 12971  2023           1 4_Central Highlands      38   760     3.92
5 12971  2023           1 6_MRD                    31  1509     2.17

$awd_1drydown
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel awd_1drydown Region              n_adopt n_sub adoption
  <int> <dbl>        <int> <chr>                 <int> <int>    <dbl>
1 13112  2023            1 1_RRD                   174  2519     8.24
2 13112  2023            1 2_NMMA                  306  4618     6.42
3 13112  2023            1 3_NCCCA                  88  3597     2.88
4 13112  2023            1 4_Central Highlands      40   765     4.80
5 13112  2023            1 5_South East              5    80     2.80
6 13112  2023            1 6_MRD                    38  1533     3.62

$awd_2drydown
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel awd_2drydown Region              n_adopt n_sub adoption
  <int> <dbl>        <int> <chr>                 <int> <int>    <dbl>
1 13112  2023            1 1_RRD                    74  2519    3.99 
2 13112  2023            1 2_NMMA                  141  4618    2.98 
3 13112  2023            1 3_NCCCA                  57  3597    1.69 
4 13112  2023            1 4_Central Highlands      28   765    4.47 
5 13112  2023            1 5_South East              1    80    0.779
6 13112  2023            1 6_MRD                    87  1533    6.11 

$straw_rm_livestock
# A tibble: 6 × 7
# Groups:   Region [6]
  n_obs panel straw_rm_livestock Region              n_adopt n_sub adoption
  <int> <dbl>              <int> <chr>                 <int> <int>    <dbl>
1  4313  2022                  1 1_RRD                    59   207     17.4
2  4313  2022                  1 2_NMMA                  900  1357     60.6
3  4313  2022                  1 3_NCCCA                1136  1865     60.5
4  4313  2022                  1 4_Central Highlands     168   344     46.0
5  4313  2022                  1 5_South East              9    44     19.3
6  4313  2022                  1 6_MRD                   150   496     29.0

$straw_rm_mushroom
# A tibble: 3 × 7
# Groups:   Region [3]
  n_obs panel straw_rm_mushroom Region  n_adopt n_sub adoption
  <int> <dbl>             <int> <chr>     <int> <int>    <dbl>
1  4313  2022                 1 2_NMMA        1  1357   0.0979
2  4313  2022                 1 3_NCCCA       4  1865   0.162 
3  4313  2022                 1 6_MRD         6   496   0.872 

$straw_rm_compost
# A tibble: 5 × 7
# Groups:   Region [5]
  n_obs panel straw_rm_compost Region              n_adopt n_sub adoption
  <int> <dbl>            <int> <chr>                 <int> <int>    <dbl>
1  4313  2022                1 1_RRD                    10   207     3.38
2  4313  2022                1 2_NMMA                  150  1357    13.2 
3  4313  2022                1 3_NCCCA                 124  1865     7.79
4  4313  2022                1 4_Central Highlands      56   344    15.7 
5  4313  2022                1 6_MRD                    20   496     3.69

> 
> 
> 
> ### Rice DNA reach: ----
> 
> 
> dna_list <- c("Sub1", "Saltol", "IRRI_Parentage_edited")
> 
> reach_dna <- list()
> 
> for (var in dna_list) {
+   
+   adopt_dna <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (.data[[var]]) %>%
+     filter (panel == max(panel, na.rm = TRUE)) %>%
+     ungroup () %>%
+     mutate (n_obs = n(),
+             panel = panel) %>%
+     group_by (n_obs, panel, .data[[var]], Region) %>%
+     summarise (twt = sum(weight_rice_DNA, na.rm = TRUE),
+                n_adopt = n()) %>%
+     group_by (Region) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE),
+             n_sub = sum (n_adopt)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt))
+   
+   
+   reach_dna[[var]] <- adopt_dna
+ }
`summarise()` has grouped output by 'n_obs', 'panel', 'Sub1'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Saltol'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'IRRI_Parentage_edited'. You can override using the
`.groups` argument.
> 
> reach_dna
$Sub1
# A tibble: 3 × 8
# Groups:   Region [3]
  n_obs panel  Sub1 Region                 twt n_adopt n_sub adoption
  <int> <dbl> <int> <chr>                <dbl>   <int> <int>    <dbl>
1   766  2022     1 3_NCCCA             39691.       4   171     2.34
2   766  2022     1 4_Central Highlands 12534.       5    44     4.45
3   766  2022     1 6_MRD               62682.       4   194     3.14

$Saltol
# A tibble: 6 × 8
# Groups:   Region [6]
  n_obs panel Saltol Region                  twt n_adopt n_sub adoption
  <int> <dbl>  <int> <chr>                 <dbl>   <int> <int>    <dbl>
1   760  2022      1 1_RRD               577114.      43   155     26.3
2   760  2022      1 2_NMMA              371153.      46   178     26.6
3   760  2022      1 3_NCCCA             381212.      35   169     22.6
4   760  2022      1 4_Central Highlands  35067.       5    43     12.7
5   760  2022      1 5_South East         84012.      14    24     54.6
6   760  2022      1 6_MRD               905970.      84   191     46.5

$IRRI_Parentage_edited
# A tibble: 6 × 8
# Groups:   Region [6]
  n_obs panel IRRI_Parentage_edited Region                   twt n_adopt n_sub adoption
  <int> <dbl>                 <int> <chr>                  <dbl>   <int> <int>    <dbl>
1   766  2022                     1 1_RRD                235905.      15   155    10.8 
2   766  2022                     1 2_NMMA               214248.      27   178    15.4 
3   766  2022                     1 3_NCCCA               66962.       9   171     3.94
4   766  2022                     1 4_Central Highlands  127260.      17    44    45.2 
5   766  2022                     1 5_South East         102132.      14    24    66.3 
6   766  2022                     1 6_MRD               1180395.     118   194    59.1 

> 
> 
> 
> 
> ### Cassava reach ----
> 
> 
> cassava_list <- c("CMD_edited", "DMC_edited", "CIAT.related")
> 
> reach_cassava <- list()
> 
> for (var in cassava_list) {
+   
+   adopt_cassava <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     filter (!is.na(Region)) %>%
+     group_by (.data[[var]]) %>%
+     filter (panel == max(panel, na.rm = TRUE)) %>%
+     ungroup () %>%
+     mutate (n_obs = n(),
+             panel = panel) %>%
+     group_by (n_obs, panel, .data[[var]], Region) %>%
+     summarise (twt = sum(weight_cass, na.rm = TRUE),
+                n_adopt = n()) %>%
+     group_by (Region) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE),
+             n_sub = sum (n_adopt)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt))
+   
+   
+   reach_cassava[[var]] <- adopt_cassava
+ }
`summarise()` has grouped output by 'n_obs', 'panel', 'CMD_edited'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'DMC_edited'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'CIAT.related'. You can override using the
`.groups` argument.
> 
> reach_cassava
$CMD_edited
# A tibble: 6 × 8
# Groups:   Region [6]
  n_obs panel CMD_edited Region                 twt n_adopt n_sub adoption
  <int> <dbl>      <dbl> <chr>                <dbl>   <int> <int>    <dbl>
1   738  2023          1 1_RRD                6921.      16    21     95.1
2   738  2023          1 2_NMMA              70094.     126   324     36.6
3   738  2023          1 3_NCCCA             51435.      72   228     30.5
4   738  2023          1 4_Central Highlands 17114.      26   148     14.3
5   738  2023          1 5_South East         1800.       1    12     17.9
6   738  2023          1 6_MRD                 529.       5     5    100  

$DMC_edited
# A tibble: 6 × 8
# Groups:   Region [6]
  n_obs panel DMC_edited Region                  twt n_adopt n_sub adoption
  <int> <dbl>      <dbl> <chr>                 <dbl>   <int> <int>    <dbl>
1   738  2023          1 1_RRD                 7081.      18    21     97.3
2   738  2023          1 2_NMMA              126855.     213   324     66.3
3   738  2023          1 3_NCCCA             149165.     214   228     88.4
4   738  2023          1 4_Central Highlands 104561.     130   148     87.1
5   738  2023          1 5_South East         10046.      12    12    100  
6   738  2023          1 6_MRD                  529.       5     5    100  

$CIAT.related
# A tibble: 4 × 8
# Groups:   Region [4]
  n_obs panel CIAT.related Region                  twt n_adopt n_sub adoption
  <int> <dbl>        <int> <chr>                 <dbl>   <int> <int>    <dbl>
1   738  2023            1 2_NMMA               65261.     101   324     34.1
2   738  2023            1 3_NCCCA             115560.     152   228     68.5
3   738  2023            1 4_Central Highlands  82208.     109   148     68.5
4   738  2023            1 5_South East          7839.      10    12     78.0

> 
> 
> 
> 
> 
> ### Tilapia reach ----
> 
> tilapia_list <- c("StrainB_edited")
> 
> reach_tilapia <- list()
> 
> for (var in tilapia_list) {
+   
+   adopt_tilapia <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     filter (!is.na(Region)) %>%
+     group_by (.data[[var]]) %>%
+     filter (panel == max(panel, na.rm = TRUE)) %>%
+     ungroup () %>%
+     mutate (n_obs = n(),
+             panel = panel) %>%
+     group_by (n_obs, panel, .data[[var]], Region) %>%
+     summarise (twt = sum(weight_gift, na.rm = TRUE),
+                n_adopt = n()) %>%
+     group_by (Region) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE),
+             n_sub = sum (n_adopt)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt, twt))
+   
+   
+   reach_tilapia[[var]] <- adopt_tilapia
+ }
`summarise()` has grouped output by 'n_obs', 'panel', 'StrainB_edited'. You can override using the
`.groups` argument.
> 
> reach_tilapia
$StrainB_edited
# A tibble: 5 × 7
# Groups:   Region [5]
  n_obs panel StrainB_edited Region              n_adopt n_sub adoption
  <int> <dbl>          <int> <chr>                 <int> <int>    <dbl>
1   194  2023              1 1_RRD                     2     2   100   
2   194  2023              1 2_NMMA                   36    40    90.2 
3   194  2023              1 3_NCCCA                   6     6   100   
4   194  2023              1 4_Central Highlands       5     5   100   
5   194  2023              1 6_MRD                    10   141     7.30

> 
> 
> 
> 
> ### Coffee reach ----
> 
> 
> coffee_list <- c("SWCP")
> 
> reach_coffee <- list()
> 
> for (var in coffee_list) {
+   
+   adopt_coffee <- df %>%
+     filter (!is.na(.data[[var]])) %>%
+     group_by (.data[[var]]) %>%
+     filter (panel == max(panel, na.rm = TRUE)) %>%
+     ungroup () %>%
+     mutate (n_obs = n(),
+             panel = panel) %>%
+     group_by (n_obs, panel, .data[[var]], Region) %>%
+     summarise (twt = sum(weight_coffee, na.rm = TRUE),
+                n_adopt = n()) %>%
+     group_by (Region) %>%
+     mutate (sum_twt = sum(twt, na.rm = TRUE),
+             n_sub = sum (n_adopt)) %>%
+     mutate (adoption = twt * 100 / sum_twt) %>%
+     filter (.data[[var]] == 1) %>%
+     select (-c(sum_twt))
+   
+   
+   reach_coffee[[var]] <- adopt_coffee
+ }
`summarise()` has grouped output by 'n_obs', 'panel', 'SWCP'. You can override using the `.groups`
argument.
> 
> reach_coffee
$SWCP
# A tibble: 1 × 8
# Groups:   Region [1]
  n_obs panel  SWCP Region                  twt n_adopt n_sub adoption
  <int> <dbl> <int> <chr>                 <dbl>   <int> <int>    <dbl>
1  1272  2023     1 4_Central Highlands 491393.     830  1270     63.2

> 
> 
> 
> 
> ### CSMAPs----
> 
> 
> 
> ## Adoption at EA level:----
> all_var <-  c(
+   "csmap_final",
+   "mech_mini_combiner", "mech_combine_harvester", "mech_straw_baler",
+   "mech_laser_level", "mech_row_seeder", "mech_seed_blower",
+   
+   "lenient_1m", "d_1m5r_certified", 
+   "d_1m5r_seed_120kg", "d_1m5r_seed_100kg", 
+   "lenient_2r", "strict_2r", 
+   "lenient_3r", "strict_3r", 
+   "lenient_3r3g", "strict_3r3g",
+   "awd_1drydown",
+   "awd_2drydown",
+                         
+   "straw_rm_livestock", "straw_rm_mushroom", 
+   "straw_rm_compost",
+   "Sub1", "Saltol", "IRRI_Parentage_edited",
+   "CMD_edited", "DMC_edited", "CIAT.related",
+   "StrainB_edited",
+   "SWCP",
+   "pfes_dummy")
> 
> adopt_EA <- list ()
> 
> for (var in all_var){
+   adopt_ea <- df %>%
+     filter (!is.na(Region)) %>%
+     filter(!is.na(.data[[var]])) %>%
+     group_by (.data[[var]]) %>%
+     filter (panel == max(panel, na.rm = TRUE)) %>%
+     ungroup () %>%
+     mutate (n_obs = n(),
+             panel = panel) %>%
+     group_by (n_obs, panel, Region, ID_EA) %>%
+     dplyr::summarize(mean_adopt_EA = mean(.data[[var]], na.rm = TRUE)) %>%
+     # group_by (panel) %>%
+     mutate (adopt_EA = case_when (mean_adopt_EA > 0 ~ 1,
+                                   mean_adopt_EA == 0 ~ 0,
+                                   TRUE ~ NA)) %>%
+     group_by (n_obs, panel, Region, adopt_EA) %>%
+     summarise (n_EA = n()) %>%
+     mutate (pct_adopt_EA = n_EA * 100 / sum(n_EA)) %>%
+     filter (adopt_EA == 1 ) %>%
+     select (c(n_obs, panel, Region, pct_adopt_EA))
+   
+   adopt_EA[[var]] <- adopt_ea
+   
+ }
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
`summarise()` has grouped output by 'n_obs', 'panel', 'Region'. You can override using the `.groups`
argument.
> 
> adopt_EA
$csmap_final
# A tibble: 1 × 4
# Groups:   n_obs, panel, Region [1]
  n_obs panel Region pct_adopt_EA
  <int> <dbl> <chr>         <dbl>
1  1069  2023 6_MRD          49.6

$mech_mini_combiner
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13986  2022 1_RRD                      24.5 
2 13986  2022 2_NMMA                     40.2 
3 13986  2022 3_NCCCA                    27.4 
4 13986  2022 4_Central Highlands        45.4 
5 13986  2022 5_South East               12.1 
6 13986  2022 6_MRD                       5.18

$mech_combine_harvester
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13217  2023 1_RRD                       98.5
2 13217  2023 2_NMMA                      51.7
3 13217  2023 3_NCCCA                     89.2
4 13217  2023 4_Central Highlands         67.3
5 13217  2023 5_South East                96  
6 13217  2023 6_MRD                       96.2

$mech_straw_baler
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13217  2023 1_RRD                       8.73
2 13217  2023 2_NMMA                      9.83
3 13217  2023 3_NCCCA                    18.1 
4 13217  2023 4_Central Highlands        10.2 
5 13217  2023 5_South East               24   
6 13217  2023 6_MRD                      14.3 

$mech_laser_level
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13217  2023 1_RRD                      18.2 
2 13217  2023 2_NMMA                      8.95
3 13217  2023 3_NCCCA                    15.9 
4 13217  2023 4_Central Highlands        20.4 
5 13217  2023 5_South East               12   
6 13217  2023 6_MRD                      10.8 

$mech_row_seeder
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13217  2023 1_RRD                      3.74 
2 13217  2023 2_NMMA                     0.218
3 13217  2023 3_NCCCA                    1.51 
4 13217  2023 4_Central Highlands        3.06 
5 13217  2023 5_South East               4    
6 13217  2023 6_MRD                     22.5  

$mech_seed_blower
# A tibble: 5 × 4
# Groups:   n_obs, panel, Region [5]
  n_obs panel Region       pct_adopt_EA
  <int> <dbl> <chr>               <dbl>
1 13217  2023 1_RRD               0.748
2 13217  2023 2_NMMA              0.218
3 13217  2023 3_NCCCA             1.29 
4 13217  2023 5_South East       36    
5 13217  2023 6_MRD              41.3  

$lenient_1m
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13216  2023 1_RRD                       98.5
2 13216  2023 2_NMMA                      96.9
3 13216  2023 3_NCCCA                     93.8
4 13216  2023 4_Central Highlands         86.7
5 13216  2023 5_South East                92  
6 13216  2023 6_MRD                       93.0

$d_1m5r_certified
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13216  2023 1_RRD                       94.5
2 13216  2023 2_NMMA                      86.0
3 13216  2023 3_NCCCA                     85.2
4 13216  2023 4_Central Highlands         69.4
5 13216  2023 5_South East                92  
6 13216  2023 6_MRD                       81.9

$d_1m5r_seed_120kg
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 12244  2023 1_RRD                       99.5
2 12244  2023 2_NMMA                      96.8
3 12244  2023 3_NCCCA                     72.1
4 12244  2023 4_Central Highlands         37.2
5 12244  2023 5_South East                42.1
6 12244  2023 6_MRD                       40.1

$d_1m5r_seed_100kg
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 12244  2023 1_RRD                       99.5
2 12244  2023 2_NMMA                      96.1
3 12244  2023 3_NCCCA                     62.9
4 12244  2023 4_Central Highlands         30.9
5 12244  2023 5_South East                21.1
6 12244  2023 6_MRD                       24.0

$lenient_2r
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13061  2023 1_RRD                       88.2
2 13061  2023 2_NMMA                      86.7
3 13061  2023 3_NCCCA                     89.0
4 13061  2023 4_Central Highlands         83.7
5 13061  2023 5_South East                65.4
6 13061  2023 6_MRD                       89.5

$strict_2r
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13113  2023 1_RRD                       68.6
2 13113  2023 2_NMMA                      67.9
3 13113  2023 3_NCCCA                     73.8
4 13113  2023 4_Central Highlands         67.7
5 13113  2023 5_South East                46.2
6 13113  2023 6_MRD                       80.6

$lenient_3r
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13087  2023 1_RRD                       36.8
2 13087  2023 2_NMMA                      65.9
3 13087  2023 3_NCCCA                     53.2
4 13087  2023 4_Central Highlands         52.5
5 13087  2023 5_South East                15.4
6 13087  2023 6_MRD                       22.4

$strict_3r
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13088  2023 1_RRD                       26.6
2 13088  2023 2_NMMA                      60.7
3 13088  2023 3_NCCCA                     48.5
4 13088  2023 4_Central Highlands         45.5
5 13088  2023 5_South East                11.5
6 13088  2023 6_MRD                       19.6

$lenient_3r3g
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 12925  2023 1_RRD                      26.4 
2 12925  2023 2_NMMA                     41.2 
3 12925  2023 3_NCCCA                    31.5 
4 12925  2023 4_Central Highlands        13.1 
5 12925  2023 5_South East                3.85
6 12925  2023 6_MRD                       9.52

$strict_3r3g
# A tibble: 5 × 4
# Groups:   n_obs, panel, Region [5]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 12971  2023 1_RRD                      12.0 
2 12971  2023 2_NMMA                     23.4 
3 12971  2023 3_NCCCA                    17.4 
4 12971  2023 4_Central Highlands        10.1 
5 12971  2023 6_MRD                       5.40

$awd_1drydown
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13112  2023 1_RRD                      12.0 
2 13112  2023 2_NMMA                     23.4 
3 13112  2023 3_NCCCA                    11.8 
4 13112  2023 4_Central Highlands        19.2 
5 13112  2023 5_South East               16   
6 13112  2023 6_MRD                       8.52

$awd_2drydown
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1 13112  2023 1_RRD                       9.98
2 13112  2023 2_NMMA                     14.2 
3 13112  2023 3_NCCCA                     7.31
4 13112  2023 4_Central Highlands        12.1 
5 13112  2023 5_South East                4   
6 13112  2023 6_MRD                      16.1 

$straw_rm_livestock
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1  4313  2022 1_RRD                       36.2
2  4313  2022 2_NMMA                      75.9
3  4313  2022 3_NCCCA                     72.3
4  4313  2022 4_Central Highlands         66.2
5  4313  2022 5_South East                27.3
6  4313  2022 6_MRD                       35.1

$straw_rm_mushroom
# A tibble: 3 × 4
# Groups:   n_obs, panel, Region [3]
  n_obs panel Region  pct_adopt_EA
  <int> <dbl> <chr>          <dbl>
1  4313  2022 2_NMMA         0.370
2  4313  2022 3_NCCCA        0.282
3  4313  2022 6_MRD          3.51 

$straw_rm_compost
# A tibble: 5 × 4
# Groups:   n_obs, panel, Region [5]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1  4313  2022 1_RRD                       8.51
2  4313  2022 2_NMMA                     23.0 
3  4313  2022 3_NCCCA                    21.5 
4  4313  2022 4_Central Highlands        26.2 
5  4313  2022 6_MRD                       7.60

$Sub1
# A tibble: 3 × 4
# Groups:   n_obs, panel, Region [3]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1   766  2022 3_NCCCA                     6.67
2   766  2022 4_Central Highlands        18.2 
3   766  2022 6_MRD                       3.66

$Saltol
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1   760  2022 1_RRD                       40.4
2   760  2022 2_NMMA                      47.8
3   760  2022 3_NCCCA                     46.7
4   760  2022 4_Central Highlands         27.3
5   760  2022 5_South East                62.5
6   760  2022 6_MRD                       59.3

$IRRI_Parentage_edited
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1   766  2022 1_RRD                       15.8
2   766  2022 2_NMMA                      32.6
3   766  2022 3_NCCCA                     13.3
4   766  2022 4_Central Highlands         63.6
5   766  2022 5_South East                75  
6   766  2022 6_MRD                       73.2

$CMD_edited
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1   738  2023 1_RRD                       83.3
2   738  2023 2_NMMA                      55.6
3   738  2023 3_NCCCA                     50.7
4   738  2023 4_Central Highlands         32.5
5   738  2023 5_South East                12.5
6   738  2023 6_MRD                      100  

$DMC_edited
# A tibble: 6 × 4
# Groups:   n_obs, panel, Region [6]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1   738  2023 1_RRD                       83.3
2   738  2023 2_NMMA                      79.8
3   738  2023 3_NCCCA                     96  
4   738  2023 4_Central Highlands         90  
5   738  2023 5_South East               100  
6   738  2023 6_MRD                      100  

$CIAT.related
# A tibble: 4 × 4
# Groups:   n_obs, panel, Region [4]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1   738  2023 2_NMMA                      45.5
2   738  2023 3_NCCCA                     74.7
3   738  2023 4_Central Highlands         90  
4   738  2023 5_South East                87.5

$StrainB_edited
# A tibble: 5 × 4
# Groups:   n_obs, panel, Region [5]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1   194  2023 1_RRD                      100  
2   194  2023 2_NMMA                     100  
3   194  2023 3_NCCCA                    100  
4   194  2023 4_Central Highlands        100  
5   194  2023 6_MRD                       31.6

$SWCP
# A tibble: 1 × 4
# Groups:   n_obs, panel, Region [1]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1  1272  2023 4_Central Highlands         79.4

$pfes_dummy
# A tibble: 4 × 4
# Groups:   n_obs, panel, Region [4]
  n_obs panel Region              pct_adopt_EA
  <int> <dbl> <chr>                      <dbl>
1  1539  2024 2_NMMA                    38.4  
2  1539  2024 3_NCCCA                    8.63 
3  1539  2024 4_Central Highlands       23.6  
4  1539  2024 5_South East               0.870

> 
> 
> 
> 
> ## Unlist all results: ----
> 
> library (plyr)
Warning: package ‘plyr’ was built under R version 4.3.2
--------------------------------------------------------------------------------------------------------
You have loaded plyr after dplyr - this is likely to cause problems.
If you need functions from both plyr and dplyr, please load plyr first, then dplyr:
library(plyr); library(dplyr)
--------------------------------------------------------------------------------------------------------

Attaching package: ‘plyr’

The following object is masked from ‘package:this.path’:

    here

The following object is masked from ‘package:purrr’:

    compact

The following object is masked from ‘package:maditr’:

    take

The following objects are masked from ‘package:dplyr’:

    arrange, count, desc, failwith, id, mutate, rename, summarise, summarize

> 
> tab_adoption_rice <- ldply (adoption_hh) %>%
+   select (c(".id", "Region", "adoption", "n_obs", "panel", "n_sub"))
> 
> tab_adoption_dna <- ldply (reach_dna) %>%
+   select (c(".id", "Region", "adoption", "n_obs", "panel", "n_sub")) 
> 
> 
> tab_adoption_cassava <- ldply (reach_cassava) %>%
+   select (c(".id", "Region", "adoption", "n_obs", "panel", "n_sub")) 
> 
> tab_adoption_tilapia <- ldply (reach_tilapia) %>%
+   select (c(".id", "Region", "adoption", "n_obs", "panel", "n_sub"))
> 
> tab_adoption_coffee <- ldply (reach_coffee) %>%
+   select (c(".id", "Region", "adoption", "n_obs", "panel", "n_sub")) 
> 
> tab_adoption_ea <- ldply (adopt_EA)
> 
> 
> 
> detach("package:plyr", unload = TRUE)
Warning: ‘plyr’ namespace cannot be unloaded:
  namespace ‘plyr’ is imported by ‘rapportools’, ‘reshape2’ so cannot be unloaded
> 
> 
> 
> ## Edit column names of result table: ----
> tab_combine <- full_join (tab_adoption_rice, tab_adoption_dna) %>%
+   full_join (tab_adoption_cassava) %>%
+   full_join (tab_adoption_tilapia) %>%
+   full_join (tab_adoption_coffee) %>%
+   # full_join (tab_adoption_pfes) %>%
+   full_join (tab_adoption_ea) %>%
+   rename (var = ".id") %>%
+   mutate (adopt_hh = case_when (n_sub >= 30 ~ adoption,
+                                 n_sub <30 ~ NA)) %>%
+   mutate (adopt_EA = case_when (var == "pfes_dummy" ~ pct_adopt_EA,
+                                 (n_sub >= 30 & var != "pfes_dummy") ~ pct_adopt_EA,
+                                 (n_sub < 30 & var != "pfes_dummy") ~ NA)) %>%
+   select (-c(adoption, n_sub, pct_adopt_EA)) %>%
+   pivot_wider(names_glue = "{.value}_{Region}", names_from = "Region", values_from = c(adopt_hh, adopt_EA)) %>% 
+   relocate (starts_with("adopt_hh"), .after = starts_with("adopt_EA")) %>%
+   relocate (c(ends_with(c("RRD", "NMMA", "NCCCA", "Central Highlands", "South East", "MRD"))),
+             .after = "var") %>%
+   relocate (c("n_obs", "panel"), .after = "var") 
Joining with `by = join_by(.id, Region, adoption, n_obs, panel, n_sub)`
Joining with `by = join_by(.id, Region, adoption, n_obs, panel, n_sub)`
Joining with `by = join_by(.id, Region, adoption, n_obs, panel, n_sub)`
Joining with `by = join_by(.id, Region, adoption, n_obs, panel, n_sub)`
Joining with `by = join_by(.id, Region, n_obs, panel)`
> 
> 
> 
> 
> 
> 
> ## Final results output: ----
> 
> 
> result <- tab_combine %>%
+   mutate(
+     order = case_when(
+       var == "CMD_edited" ~ 0.95,
+       var == "DMC_edited" ~ 0.96,  #reorder CMD and DMC
+       var == "CIAT.related" ~ 0.94,
+       var == "StrainB_edited" ~ 0.4,
+       var == "Sub1" ~ 0.99,
+       var == "Saltol" ~ 0.98,
+       var == "IRRI_Parentage_edited" ~ 0.97,
+       var == "mech_laser_level" ~ 2,
+       var == "mech_row_seeder" ~ 20.1,
+       var == "mech_seed_blower" ~ 2.9,
+       var == "mech_combine_harvester" ~ 2.2,
+       var == "mech_mini_combiner" ~ 2.3,
+       var == "mech_straw_baler" ~ 2.4,
+       var == "pfes_dummy" ~ 1.9,
+       var == "csmap_final" ~ 1.8,
+       TRUE ~ as.numeric(row_number())
+     )
+   ) %>%
+   arrange(order) %>%
+   select(-order)
> 
> 
> 
> 
> 
> 
> 
> 
> # Print Table 8----
> 
> var_name <- c(
+   "Genetically Improved Farmed Tilapia (GIFT)-derived strains",
+   "CGIAR-related Cassava Varieties",
+   "Cassava Mosaic Disease (CMD)-resistant Cassava Varieties",
+   "High-starch cassava varieties (DM QTL)",
+   "CGIAR-related Rice Varieties",
+   "Salt-tolerant Rice Varieties (STRVs)",
+   "Submergence-tolerant Rice Varieties",
+   "Climate-Smart Mapping and Adaptation Planning (CS-MAP)",
+   "Payment for Forest Environmental Services (PFES)",
+   "Laser Land Levelling (LLL)",
+   "Combine Harvester (CHB)",
+   "Mini-Combine Harvester (MCHB)",
+   "Rice Straw Baler", 
+   "Household used seed blower",
+   "Lenient 1M (Households combined certified seeds and own seeds)",
+   "Strict 1M (Households used totally certified seeds)", 
+   "Lenient 1R (Households adopted maximum 120kg/ha seed rates)",
+   "Strict 1R (Households adopted maximum 100kg/ha seed rates)",
+   "Lenient 2R (Maximum 110kg/ha of Nitrogen and minimum 2 applications)",
+   "Strict 2R (Maximum 100kg/ha of Nitrogen and minimum 3 applications)",
+   "Lenient 3R (Maximum 6 applications and not within 20 days before harvest)",
+   "Strict 3R (Maximum 3 applications of chemicals, not within 40 days after sowing, not after flowering)",
+   "Three Reductions, Three Gains (3R3G) and One Must Do, Five Reductions (1M5R)",
+   "Strict 3R3G (Households adopted all the three reductions, using strict criteria)",
+   "Alternate Wetting and Drying (AWD)",
+   "At least 2 dry-downs, all between reproductive stage, each enduring at least 5 days",
+   "Households used removed straws to feed for livestock",
+   "Drum Seeder",
+   "Off-field Straw Management Practices",
+   "Households used removed straws for compost", 
+   "Sustainable Water Use for Coffee Production"
+ )
> 
> 
> 
> table6_print <- result %>%
+   mutate (var_name = var_name) %>%
+   relocate (var_name, .before = everything())
> 
> table6_print$panel <- as.character(table6_print$panel)
> 
> var_remove <- c("High-starch cassava varieties (DM QTL)",
+                 "Households used removed straws to feed for livestock",
+                 "Households used removed straws for compost",
+                 "Household used seed blower",
+                 "Lenient 1M (Households combined certified seeds and own seeds)",
+                 "Strict 1M (Households used totally certified seeds)",
+                 "Lenient 1R (Households adopted maximum 120kg/ha seed rates)",
+                 "Strict 1R (Households adopted maximum 100kg/ha seed rates)",
+                 "Lenient 2R (Maximum 110kg/ha of Nitrogen and minimum 2 applications)",
+                 "Strict 2R (Maximum 100kg/ha of Nitrogen and minimum 3 applications)",
+                 "Lenient 3R (Maximum 6 applications and not within 20 days before harvest)",
+                 "Strict 3R (Maximum 3 applications of chemicals, not within 40 days after sowing, not after flowering)",
+                 "At least 2 dry-downs, all between reproductive stage, each enduring at least 5 days",
+                 "Strict 3R3G (Households adopted all the three reductions, using strict criteria)")
> 
> 
> table6_print <- table6_print %>%
+   filter (!var_name %in% var_remove)
> 
> 
> 
> 
> ft <- table6_print %>%
+   select (-var) %>%
+   flextable () %>%
+   delete_part(part = "header") %>%
+   add_header_row(values = c("", "N", "Panel",
+                             rep (c("%EA", "%HH"), 6))) %>%
+   add_header_row(values = c("", "", "",
+                             "Red River Delta", 
+                             "Northern Midlands and Mountains", 
+                             "Northern and Central Coast", 
+                             "Central Highlands",
+                             "Southeast",
+                             "Mekong River Delta"), 
+                  colwidths = c(1,1,1,2,2,2,2,2,2)) %>%
+   merge_v(part = "header", j = 1:3) %>%
+   align(align = "center", part = "header", j = 1:15) %>%
+   align(align = "center", part = "body", j = 2:15) %>%
+   colformat_double(j = 4:15, digits = 1, na_str = "-") %>%
+   colformat_char(j=3) %>%
+   bold(i = 1, part = "header") %>%
+   bold (i = 2, part = "header") %>% 
+   border_inner_h(part = "body", fp_border(width = 0.5)) %>%
+   border_inner_h(part = "header", fp_border(width = 0.5)) %>%
+   vline (j = c(3,5,7,9,11,13), border = fp_border(width = 0.5), part = "body") %>%
+   border(i = 1, border.top = fp_border(width = 2)) %>%
+   # theme_vanilla() %>%
+   footnote (part = "header", i = 2, j = 2, value = as_paragraph("Any sub-samples in the regions less than 30 observations are dropped out"), ref_symbols = "1") %>%
+   footnote (part = "body", i = 8, j = 1, value = as_paragraph("Preliminary data - from the first three quarters of VHLSS 2024"), ref_symbols = "2,") %>%
+   footnote (part = "body", i = 8, j = 1, value = as_paragraph("This innovation was measured through a community-level questionnaire"), ref_symbols = "3") %>%
+   autofit()
> 
> ft
> 
> print(ft)
> 
> 
> save_as_html (ft,
+               path = "Output/Tab8.html",
+               page_size(orient = "landscape"))
> 
> 
> save_as_docx(ft, 
+              path = "Output/Tab8.docx",
+              pr_section = prop_section(page_size = page_size(orient = "landscape")))
> #Load packages
> library (this.path)
> library (tidyverse)
> library (ggplot2)
> library (gridExtra)
> library (fastDummies)
> library (ggpolypath)
> library (eulerr)
> library (readxl)
> library (stringr)
> library (flextable)
> library (sf)
> library (curl)
> library (survey)
> library (haven)
> library (readstata13)
> library (scales)
> # Reach of the CGIAR estimates ----
> 
> # a) Innovations in VHLSS 2023 ----
> df_23 <- read.csv("C:/Users/FKosmowski/SPIA Dropbox/SPIA General/SPIA 2019-2024/5. OBJ.3-Data collection/Country teams/Vietnam/Report 2024/Reproducible Scripts/Output/VH23_data.csv")
> 
> # ADD CS-MAPs. Later integrate the variable into df_23
> CS <- read.csv("C:/Users/FKosmowski/SPIA Dropbox/SPIA General/SPIA 2019-2024/5. OBJ.3-Data collection/Country teams/Vietnam/DATA/Non-genetics/CSMAPs.vars.22.23.csv")
> CS <- CS [CS$panel == 2023 ,] # From 406 to 267 in 2023
> 
> CS <- CS[order(CS$MATINH, CS$MAHUYEN, CS$MAXA, CS$MADIABAN, CS$HOSO, -CS$CSMAP_reach), ]
> CS <- CS[!duplicated(CS[c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO")]), ]
> 
> df_23 <- merge(df_23, CS, by = intersect(names(df_23), names(CS)), all.x = TRUE)
> df_23$CSMAP_reach <- as.integer(df_23$CSMAP_reach)
> 
> CG.reach.23 <- df_23 [, c(1:6,12,16,25,27,29,33,38,43,69,47:50)]
> 
> CG.reach.23$ThreeR <- ifelse(
+   CG.reach.23$d_1m5r_seed_120kg == 1 & CG.reach.23$lenient_2r == 1 & CG.reach.23$lenient_3r == 1, 1, 
+   ifelse(is.na(CG.reach.23$d_1m5r_seed_120kg) & is.na(CG.reach.23$lenient_2r) & is.na(CG.reach.23$lenient_3r), NA, 0 ))
> 
> 
> CG.reach.23$CG.reach.23 <- ifelse (CG.reach.23$CIAT.related == 1 | CG.reach.23$StrainB_edited == 1 | CG.reach.23$awd_1drydown == 1 | CG.reach.23$SWCP ==1  | CG.reach.23$ThreeR == 1 | CG.reach.23$CSMAP_reach == 1, 1, 0)
> CG.reach.23$CG.reach.23 [is.na (CG.reach.23$CG.reach.23)] <- 0
> table (CG.reach.23$CG.reach.23)

    0     1 
11023  3454 
> 
> CG.reach.23$sum_CG <- rowSums(
+   cbind(
+     ifelse(is.na(CG.reach.23$CIAT.related), 0, CG.reach.23$CIAT.related),
+     ifelse(is.na(CG.reach.23$StrainB_edited), 0, CG.reach.23$StrainB_edited),
+     ifelse(is.na(CG.reach.23$awd_1drydown), 0, CG.reach.23$awd_1drydown),
+     ifelse(is.na(CG.reach.23$SWCP), 0, CG.reach.23$SWCP),
+     ifelse(is.na(CG.reach.23$ThreeR), 0, CG.reach.23$ThreeR),
+     ifelse(is.na(CG.reach.23$CSMAP_reach), 0, CG.reach.23$CSMAP_reach)
+     
+   )
+ )
> 
> table (CG.reach.23$sum_CG) # Non adopters, single adopters and multi-adopters

    0     1     2     3 
11023  3293   159     2 
> 
> # What are the two innovations adopted together?
> subset_two_innovations <- CG.reach.23[CG.reach.23$sum_CG == 2, ]
> subset_two_innovations$two_innovations <- apply(
+   subset_two_innovations[, c("CIAT.related", "StrainB_edited", "awd_1drydown", "SWCP", "ThreeR", "CSMAP_reach", "mech_laser_level")],
+   1,
+   function(row) paste(names(row)[which(row == 1)], collapse = " & ")
+ )
Error in apply(subset_two_innovations[, c("CIAT.related", "StrainB_edited",  : 
  unused argument (function(row) paste(names(row)[which(row == 1)], collapse = " & "))
> # Add weights
> w <- read.dta13 ("C:/Users/FKosmowski/SPIA Dropbox/SPIA General/SPIA 2019-2024/5. OBJ.3-Data collection/Country teams/Vietnam/DATA/VHLSS_Household_2023/weight2023.dta")
> w$ID <- paste (w$tinh, w$huyen, w$xa, w$diaban, sep='-')
> CG.reach.23$ID <- paste (CG.reach.23$MATINH, CG.reach.23$MAHUYEN, CG.reach.23$MAXA, CG.reach.23$MADIABAN, sep='-')
> 
> CG.reach.23 <- merge (CG.reach.23, w, by="ID", all=TRUE)
> CG.reach.23 <- CG.reach.23 [!is.na (CG.reach.23$MATINH) ,]
> 
> CG.reach.23$weighted_frequency <- 0
> 
> 
> # Apply weights only where sum_CG is 1, using only wt35 weights for the calculation
> CG.reach.23$weighted_frequency[CG.reach.23$sum_CG == 1] <- rowSums(
+   cbind(
+     ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$CIAT.related), 
+            CG.reach.23$CIAT.related * CG.reach.23$weight_cass, 0),
+     
+     ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$StrainB_edited), 
+            CG.reach.23$StrainB_edited * CG.reach.23$wt45, 0),
+     
+     ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$awd_1drydown), 
+            CG.reach.23$awd_1drydown * CG.reach.23$wt45, 0),
+     
+     ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$SWCP), 
+            CG.reach.23$SWCP * CG.reach.23$wt45, 0),
+     
+     ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$ThreeR), 
+            CG.reach.23$ThreeR * CG.reach.23$wt45, 0),
+     
+     ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$CSMAP_reach), 
+            CG.reach.23$CSMAP_reach * CG.reach.23$wt45, 0)
+    )[CG.reach.23$sum_CG == 1, ],
+   na.rm = TRUE
+ )
> 
> 
> # Apply weights for sum_CG == 2 where CIAT.related is 1, using weight_cass
> CG.reach.23$weighted_frequency <- ifelse(
+   CG.reach.23$sum_CG == 2 & CG.reach.23$CIAT.related == 1 & !is.na(CG.reach.23$CIAT.related),
+   CG.reach.23$CIAT.related * CG.reach.23$weight_cass,
+   CG.reach.23$weighted_frequency  # Keep existing value if condition isn't met
+ )
> 
> # Apply weights for sum_CG == 2 where CIAT.related is 0, using wt45
> CG.reach.23$weighted_frequency <- ifelse(
+   CG.reach.23$sum_CG == 2 & CG.reach.23$CIAT.related == 0 & !is.na(CG.reach.23$CIAT.related),
+   rowSums(
+     cbind(
+       ifelse(!is.na(CG.reach.23$StrainB_edited), CG.reach.23$StrainB_edited * CG.reach.23$wt45, 0),
+       ifelse(!is.na(CG.reach.23$awd_1drydown), CG.reach.23$awd_1drydown * CG.reach.23$wt45, 0),
+       ifelse(!is.na(CG.reach.23$SWCP), CG.reach.23$SWCP * CG.reach.23$wt45, 0),
+       ifelse(!is.na(CG.reach.23$ThreeR), CG.reach.23$ThreeR * CG.reach.23$wt45, 0) # Covers all multi-adoption patterns in the data
+     ),
+     na.rm = TRUE
+   ),
+   CG.reach.23$weighted_frequency  # Keep existing value if condition isn't met
+ )
> 
> # Total reach in 2023
> sum(CG.reach.23$weighted_frequency) # Unique adopters = 1,763,580
[1] 1763580
> 
> # Substract the multi-adopters that adopted Cassava with wt45
> sum(CG.reach.23$wt45 [CG.reach.23$sum_CG == 2 & CG.reach.23$CIAT.related == 1], na.rm=TRUE) # 23,098
[1] 23098
> 
> # Substract the multi-adopters that adopted coffee with wt45
> sum(CG.reach.23$wt45 [CG.reach.23$sum_CG == 2 & CG.reach.23$SWCP == 1], na.rm=TRUE) # 18,269
[1] 18269
> 
> # Final estimates for reach in VH23 = 1,722,213
> Reach.VH23 <- sum(CG.reach.23$weighted_frequency) - (sum(CG.reach.23$wt45 [CG.reach.23$sum_CG == 2 & CG.reach.23$CIAT.related == 1], na.rm=TRUE) + sum(CG.reach.23$wt45 [CG.reach.23$sum_CG == 2 & CG.reach.23$SWCP == 1], na.rm=TRUE))
> 
> 
> 
> # 2. Innovation weighted frequency 
> total_weighted_frequency <- sum(CG.reach.23$weighted_frequency, na.rm = TRUE)
> 
> CIAT_related_contribution <- sum(
+   ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$CIAT.related),
+          CG.reach.23$CIAT.related * CG.reach.23$weight_cass, 0), na.rm = TRUE
+ ) + sum(
+   ifelse(CG.reach.23$sum_CG == 2 & CG.reach.23$CIAT.related == 1 & !is.na(CG.reach.23$CIAT.related),
+          CG.reach.23$CIAT.related * CG.reach.23$weight_cass, 0), na.rm = TRUE
+ )
> 
> StrainB_edited_contribution <- sum(
+   ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$StrainB_edited),
+          CG.reach.23$StrainB_edited * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> awd_1drydown_contribution <- sum(
+   ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$awd_1drydown),
+          CG.reach.23$awd_1drydown * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> SWCP_contribution <- sum(
+   ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$SWCP),
+          CG.reach.23$SWCP * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> ThreeR_contribution <- sum(
+   ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$ThreeR),
+          CG.reach.23$ThreeR * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> CSMAP_contribution <- sum(
+   ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$CSMAP_reach),
+          CG.reach.23$CSMAP_reach * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> LLL_contribution <- sum(
+   ifelse(CG.reach.23$sum_CG == 1 & !is.na(CG.reach.23$mech_laser_level),
+          CG.reach.23$mech_laser_level * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> 
> # For sum_CG == 2 & CIAT.related == 0 cases:
> StrainB_edited_contribution <- StrainB_edited_contribution + sum(
+   ifelse(CG.reach.23$sum_CG == 2 & CG.reach.23$CIAT.related == 0 & !is.na(CG.reach.23$StrainB_edited),
+          CG.reach.23$StrainB_edited * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> awd_1drydown_contribution <- awd_1drydown_contribution + sum(
+   ifelse(CG.reach.23$sum_CG == 2 & CG.reach.23$CIAT.related == 0 & !is.na(CG.reach.23$awd_1drydown),
+          CG.reach.23$awd_1drydown * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> SWCP_contribution <- SWCP_contribution + sum(
+   ifelse(CG.reach.23$sum_CG == 2 & CG.reach.23$CIAT.related == 0 & !is.na(CG.reach.23$SWCP),
+          CG.reach.23$SWCP * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> ThreeR_contribution <- ThreeR_contribution + sum(
+   ifelse(CG.reach.23$sum_CG == 2 & CG.reach.23$CIAT.related == 0 & !is.na(CG.reach.23$ThreeR),
+          CG.reach.23$ThreeR * CG.reach.23$wt45, 0), na.rm = TRUE
+ )
> 
> # Calculate the percentage contribution of each innovation
> contributions <- data.frame(
+   Innovation = c("CIAT.related", "StrainB_edited", "awd_1drydown", "SWCP", "ThreeR", "CSMAP_reach"),
+   Contribution = c(
+     CIAT_related_contribution,
+     StrainB_edited_contribution,
+     awd_1drydown_contribution,
+     SWCP_contribution,
+     ThreeR_contribution,
+     CSMAP_contribution
+   ),
+   Percentage = c(
+     CIAT_related_contribution,
+     StrainB_edited_contribution,
+     awd_1drydown_contribution,
+     SWCP_contribution,
+     ThreeR_contribution,
+     CSMAP_contribution
+   ) / total_weighted_frequency * 100
+ )
> 
> # Print results
> print(contributions) 
      Innovation Contribution Percentage
1   CIAT.related     270746.4  15.352086
2 StrainB_edited      25820.0   1.464067
3   awd_1drydown     277795.0  15.751763
4           SWCP     381010.0  21.604346
5         ThreeR     672533.0  38.134525
6    CSMAP_reach     135676.0   7.693213
> 
> 
> 
> 
> # b) Innovations integrated in VHLSS 2022 ----
> 
> df_22 <- read.csv("C:/Users/FKosmowski/SPIA Dropbox/SPIA General/SPIA 2019-2024/5. OBJ.3-Data collection/Country teams/Vietnam/Report 2024/Reproducible Scripts/Output/VH22_data.csv")
> CG.reach.22 <- df_22 [, c(1:6,33,17,39,40)]
> 
> names(df_22)
 [1] "MATINH"                       "IDHO"                         "MAHUYEN"                     
 [4] "MAXA"                         "MADIABAN"                     "HOSO"                        
 [7] "breeding_main_variety"        "Correct_name.DNA2"            "breeding_seed_origin"        
[10] "breeding_seed_duration"       "Decision"                     "Convergence.on.Year"         
[13] "IRRI_Parentage"               "Saltol"                       "qSub1"                       
[16] "breeding_certified_seed"      "IRRI_Parentage_edited"        "KYDIEUTRA"                   
[19] "mech_mini_combiner"           "mech_combine_harvester"       "mech_straw_baler"            
[22] "mech_row_seeder"              "mech_seed_blower"             "straw_burn"                  
[25] "straw_incorporated_soil"      "straw_mulching"               "straw_other"                 
[28] "straw_remove_completely"      "straw_remove_partly"          "straw_rm_compost"            
[31] "straw_rm_cooking"             "straw_rm_livestock"           "straw_rm_mushroom"           
[34] "straw_rm_other"               "straw_rm_sold"                "panel"                       
[37] "mean_csmap"                   "csmap_final"                  "weight_final_rice"           
[40] "weight_rice_DNA"              "THUNHAP"                      "TONGCHITIEU"                 
[43] "ethnic"                       "rls_head"                     "age"                         
[46] "edu_grade"                    "n_member"                     "male"                        
[49] "female"                       "internet"                     "land_area_sum"               
[52] "poor_commune"                 "dummy_main_str"               "dist_main_str"               
[55] "type_main_str"                "dummy_local_market_wholesale" "dist_market_wholesale"       
[58] "dummy_ext_center"             "dist_ext_center"              "farmer_participation_pct"    
[61] "dummy_protection_staff"       "main_str_asphalt"             "Quintiles"                   
[64] "Bottom_20"                    "Bottom_40"                    "Sub1"                        
> 
> w <- read.dta13 ("C:/Users/FKosmowski/SPIA Dropbox/SPIA General/SPIA 2019-2024/5. OBJ.3-Data collection/Country teams/Vietnam/DATA/VHLSS_Household_2022/datasets/Weights/wt2022_SPIA.dta")
> 
> w$ID <- paste (w$tinh, w$huyen, w$xa, w$diaban, sep='-')
> CG.reach.22$ID <- paste (CG.reach.22$MATINH, CG.reach.22$MAHUYEN, CG.reach.22$MAXA, CG.reach.22$MADIABAN, sep='-')
> CG.reach.22 <- merge (CG.reach.22, w, by="ID", all=TRUE)
> CG.reach.22 <- CG.reach.22 [!is.na (CG.reach.22$MATINH) ,]
> 
> # Since there is no multi-adopters, we sum-up both innovations
> sum(CG.reach.22$weight_rice_DNA [CG.reach.22$IRRI_Parentage_edited == 1], na.rm=TRUE) # 
[1] 1926902
> sum(CG.reach.22$wt45 [CG.reach.22$straw_rm_mushroom == 1], na.rm=TRUE) # 
[1] 4335
> 
> Reach.VH22 <- sum(CG.reach.22$weight_rice_DNA [CG.reach.22$IRRI_Parentage_edited == 1], na.rm=TRUE) + sum(CG.reach.22$wt45 [CG.reach.22$straw_rm_mushroom == 1], na.rm=TRUE) 
> 
> 
> 
> # Calculate CSMAp reach and LLL reach (additional hh not counted in other adoption figures). 
> sum(CG.reach.23$wt45 [CG.reach.23$sum_CG == 1 & CG.reach.23$mech_laser_level == 1], na.rm=TRUE) # 66,460 hhs 
[1] 66460
> sum(CG.reach.23$wt45 [CG.reach.23$sum_CG == 1 & CG.reach.23$CSMAP_reach == 1], na.rm=TRUE) # 135,676
[1] 135676
> 
> # Lower bound = 
> Reach.lower <- Reach.VH22 + Reach.VH23 + 227331 - 135676 # PFES from VH24 is added. CS_MAPs were already part so we substract it 
> 
> # Higher bound =
> Reach.higher <- Reach.VH22 + Reach.VH23 + 227331 + 66460 # PFES from VH24 is added + Lazer-land levelling
> 
> paste ('The reach CGIAR research is estimated to be between', round (Reach.lower,0), "and", round (Reach.higher,0), "households in Viet Nam")
[1] "The reach CGIAR research is estimated to be between 3745105 and 3947241 households in Viet Nam"
> 
> # Note that using population weights and taking multi=adoption into account leads to different estimates than Tab 7 of the report
> 
> 
> 
> 
> 
> # Fig 1. Number of households adopting each CGIAR-related innovation in Viet Nam in 2023, in millions  ----
> 
> options(scipen = 999)
> 
> data <- data.frame(
+   Innovation = c(
+     "Genetically Improved Farmed Tilapia (GIFT) derived strains", 
+     "Improved Rice Varieties", 
+     #"Salt-Tolerant Rice Varieties", 
+     #"Submergence-Tolerant Rice Varieties", 
+     "Improved Cassava Varieties", 
+     #"CMD-resistant cassava varieties", 
+     "Climate-Smart Mapping and Adaptation Planning (CS-MAP)", 
+     #"Agro-Climatic Bulletins (ABCs)", 
+     "Laser Land Leveling (LLL)", 
+     # "Drum Seeder", 
+     # "Combine Harvester", 
+     #"Mini-Combine Harvester", 
+     # "Straw Baler", 
+     "Off-field Straw Management practices", 
+     "3 Reductions 3 Gains (3R3G) and 1 Must do, 5 Reductions (1M5R)", 
+     "Alternate Wetting and Drying (AWD)", 
+     "Sustainable Water Use for Coffee production", 
+     "Payments for Forest Environmental Services (PFES)"
+   ),
+   Reach = c(27291, 1926902, 270869, 148000, 409359, 4949, 787775, 408689, 362010, 227331)
+ )
> 
> # Specify the innovations to be highlighted
> highlighted_innovations <- c(
+   "Genetically Improved Farmed Tilapia (GIFT) derived strains", 
+   "Improved Rice Varieties", 
+   "Improved Cassava Varieties", 
+   "Off-field Straw Management practices", 
+   "3 Reductions 3 Gains (3R3G) and 1 Must do, 5 Reductions (1M5R)", 
+   "Alternate Wetting and Drying (AWD)", 
+   "Sustainable Water Use for Coffee production", 
+   "Payments for Forest Environmental Services (PFES)"
+   #"Climate-Smart Mapping and Adaptation Planning (CS-MAP)"
+ )
> 
> 
> # Add a new column to indicate if an innovation should be highlighted
> data <- data %>%
+   mutate(Highlight = ifelse(Innovation %in% highlighted_innovations, "Highlighted", "Normal"))
> 
> # Remove rows with NA in Reach
> data <- na.omit(data)
> 
> # Order data by Reach in ascending order
> data <- data %>% arrange(Reach)
> 
> ggplot(data, aes(x = Reach / 1e6, y = reorder(Innovation, Reach, FUN = function(x) -x), fill = Highlight)) +
+   geom_bar(stat = "identity") +
+   scale_fill_manual(values = c("Highlighted" = "darkblue", "Normal" = "lightblue")) +
+   scale_x_continuous(labels = label_number(scale = 1),  # Format x-axis labels
+                      breaks = seq(0, 3, by = 1)) +  # Set breaks to end at 6
+   labs(#title = "Fig 1. Number of households adopting each CGIAR-related innovation in Vietnam in 2023, in millions",
+     x = " ",
+     y = "Innovation") +
+   theme_minimal() +
+   theme(legend.position = "none") +
+   theme(panel.grid.major = element_line(linetype = "dashed"),
+         panel.grid.minor = element_line(linetype = "dashed"))
> rm(list = ls()) #start clean
> 
> getwd() #get current working directory
[1] "C:/Users/FKosmowski/Downloads/Scripts_corrected"
> 
> setwd("YOUR_DESTINATION") #change working directory if needed
Error in setwd("YOUR_DESTINATION") : cannot change working directory
> library (haven)
> library (tidyverse)
> library (readxl)
> library (stringr)
> library (curl)
> library (httr)
> 
> # Function to format ID values
> 
> format_ID <- function(df, columns, widths, pad_char = "0") {
+   # Loop through each column and its corresponding width
+   for (i in seq_along(columns)) {
+     column <- columns[i]
+     width <- widths[i]
+     
+     # Pad the column with the specified width and character
+     df[[column]] <- str_pad(df[[column]], width = width, pad = pad_char)
+   }
+   return(df)
+ }
> 
> 
> # Define columns and widths
> columns <- c("MATINH", "MAHUYEN", "MAXA", "MADIABAN", "HOSO")
> widths <- c(2, 3, 5, 3, 3)
> 
> # Download data files from GitHub and save to your working directory----
> # Curl without token:
> curl_function <- function(url) {
+   url_pasted <- paste0("https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/", url)
+   
+   # Ensure the directory exists before saving the file
+   dir_path <- dirname(url)  # Extract the directory path from the URL
+   if (!dir.exists(dir_path)) {
+     dir.create(dir_path, recursive = TRUE)  # Create the directory structure if it doesn't exist
+   }
+   
+   # Use download.file to fetch the file without requiring a token
+   download.file(url_pasted, destfile = url, mode = "wb")
+ }
> 
> # 2. General calculations for both years  ----
> 
> ## 2.1. Structural changes ----
> 
> ### 2.1.1. Rice-growing hhs ----
> curl_function(url = "data/raw/Weight/structural_change.xlsx")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/structural_change.xlsx'
Content type 'application/octet-stream' length 69240 bytes (67 KB)
downloaded 67 KB

> 
> str_change <- read_excel("data/raw/Weight/structural_change.xlsx", sheet = "Sheet2") %>%
+   select ("Number of rice growing households" : "2020")
> 
> 
> colnames (str_change) <- c ("province", "n_rice_2016", "n_rice_2020")
> 
> n_rice_2016 <- str_change$n_rice_2016[1] #Number of rice-growing hh in 2016
> n_rice_2020 <- str_change$n_rice_2020[1] #Number of rice-growing hh in 2020
> 
> # decline_rate <- (n_rice_2020 / n_rice_2016)^(1/4) - 1  #for 4 years, N_rice declines at 6.3%
> decline_rate_annual <- (n_rice_2020 / n_rice_2016)^(1/4) - 1   
> 
> 
> pred_n_rice_2022 <- n_rice_2020 * (1 + decline_rate_annual)^2  #Predicted number of rice-growing hh in 2022
> pred_n_rice_2023 <- n_rice_2020 * (1 + decline_rate_annual)^3 #Predicted 2023
> 
> 
> 
> ### 2.1.2. Cassava DNA subsample ----
> 
> str_change_cass <- read_excel("data/raw/Weight/structural_change.xlsx",
+                               sheet = "Sheet3") %>%
+   select (c("...1", "cassava_2016", "cassava_2020"))
New names:
• `` -> `...1`
> 
> 
> n_cass_2016 <- str_change_cass$cassava_2016[1]
> n_cass_2020 <- str_change_cass$cassava_2020[1] #N cassava-growing hh in 2020
> decline_rate_annual_cass <- (n_cass_2020 / n_cass_2016) ^ (1/4) - 1 #decline rate of #cassava-growing hh
> pred_n_cass_2023 <- n_cass_2020 * (1 + decline_rate_annual_cass) ^ 3
> 
> 
> 
> 
> ### 2.1.3. Coffee ----
> 
> curl_function(url = "data/raw/Weight/coffee_household_clean.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/coffee_household_clean.csv'
Content type 'text/plain; charset=utf-8' length 589028 bytes (575 KB)
downloaded 575 KB

> 
> n_coffee_pop <- read.csv ("data/raw/Weight/coffee_household_clean.csv") 
> 
> n_coffee_2016 <- sum(n_coffee_pop$n_coffee_hh)
> 
> str_change_coffee <- read_excel("data/raw/Weight/structural_change.xlsx",
+                          sheet = "Sheet4") 
New names:
• `` -> `...1`
> 
> n_coffee_2020 <- str_change_coffee$Coffee[1]
> 
> increase_rate_annual_coffee <- (n_coffee_2020 / n_coffee_2016)^(1/4) - 1 # 1.9% increase annually
> 
> pred_n_coffee_2023 <- n_coffee_2020 * (1 + increase_rate_annual_coffee)^3 
> 
> ### 2.2. IN THE POPULATION----
> 
> #### 2.2.1. Number of rice-growing households IN THE POPULATION----
> curl_function (url = "data/raw/Weight/rice_household_clean.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/rice_household_clean.csv'
Content type 'text/plain; charset=utf-8' length 609161 bytes (594 KB)
downloaded 594 KB

> 
> n_rice_pop <- read.csv ("data/raw/Weight/rice_household_clean.csv") %>%
+   select (MATINH, MAXA, n_rice_hh) %>%
+   rename (n_rice_pop = n_rice_hh) #merge by Commune ID (MAXA) because of some administrative change
> 
> n_rice_pop <- format_ID(n_rice_pop, columns = c("MATINH", "MAXA"), widths = c(2, 5))
> 
> 
> #### 2.2.2. Number of general households IN THE POPULATION----
> curl_function (url = "data/raw/Weight/Census_household_communelevel_clean.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/Census_household_communelevel_clean.csv'
Content type 'text/plain; charset=utf-8' length 665472 bytes (649 KB)
downloaded 649 KB

> 
> 
> n_hh_pop <- read.csv ("data/raw/Weight/Census_household_communelevel_clean.csv") %>%
+   select (c(MATINH, MAXA, n_hh)) %>%
+   rename (n_hh_pop = n_hh) 
> #merge by Commune ID (MAXA) because of some administrative change 
> # (486 missing if merge by prov, dist, comm ID --> 470 missing if merge by prov and comm ID)
> 
> n_hh_pop <- format_ID(n_hh_pop, columns = c("MATINH", "MAXA"), widths = c(2, 5))
> 
> fraction_pop <- full_join (n_rice_pop, n_hh_pop) %>%
+   mutate (fraction_pop = n_rice_pop / n_hh_pop)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> 
> # 3. Recalculating weights 2023----
> 
> ## 3.1. Load original weights 2023----
> 
> curl_function(url = "data/raw/Weight/VHLSS_2023_weight.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/VHLSS_2023_weight.dta'
Content type 'application/octet-stream' length 46945 bytes (45 KB)
downloaded 45 KB

> 
> wt_2023 <- read_dta ("data/raw/Weight/VHLSS_2023_weight.dta")
> 
> colnames(wt_2023)[1:4] <- paste0 ("ma", colnames(wt_2023)[1:4]) 
> colnames(wt_2023)[1:4] <- toupper (colnames(wt_2023)[1:4])
> 
> 
> # Reformat IDs:
> wt_2023 <- format_ID(wt_2023, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> 
> 
> ## 3.2. Adjustment for under-representation----
> 
> ### 3.2.1. IN THE SAMPLE----
> 
> #### 3.2.1.1. Rice-growing households IN THE SAMPLE ----
> 
> curl_function(url = "data/raw/VHLSS_2023_Household/Final/Muc4B11.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Final/Muc4B11.csv'
Content type 'text/plain; charset=utf-8' length 1846335 bytes (1.8 MB)
downloaded 1.8 MB

> 
> rice_gso <- read.csv ("data/raw/VHLSS_2023_Household/Final/Muc4B11.csv") %>%
+   select (c(MATINH, MAXA, MADIABAN, IDHO, HOSO, KYDIEUTRA, wt45)) %>%
+   distinct ()
> 
> rice_gso <- rice_gso %>%
+   filter (!is.na(wt45))  #N = 13229
> 
> 
> rice_gso <- format_ID(rice_gso, columns = c("MATINH", "MAXA", "MADIABAN", "HOSO"), widths = c(2, 5, 3, 3))
> 
> rice_gso$IDHO <- paste0 (rice_gso$MAXA, rice_gso$MADIABAN, rice_gso$HOSO)
> 
> n_rice_sample <- rice_gso %>%
+   group_by (MATINH, MAXA, MADIABAN) %>%
+   summarise (n_rice_sample = n()) #Number of rice-growing households by commune
`summarise()` has grouped output by 'MATINH', 'MAXA'. You can override using the `.groups` argument.
> 
> 
> 
> 
> #### 3.2.1.2. Number of general households IN THE SAMPLE ---- 
> curl_function (url = "data/raw/VHLSS_2023_Correlates/Ho_ThongTinHo.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Correlates/Ho_ThongTinHo.dta'
Content type 'application/octet-stream' length 3634385 bytes (3.5 MB)
downloaded 3.5 MB

> 
> ho_thongtinho <- read_dta ("data/raw/VHLSS_2023_Correlates/Ho_ThongTinHo.dta") %>%
+   select (c(idho:hoso))
> 
> colnames(ho_thongtinho)[2:5] <- paste0 ("ma", colnames(ho_thongtinho)[2:5])
> 
> colnames(ho_thongtinho) <- toupper (colnames(ho_thongtinho))
> 
> 
> #Reformat IDs:
> ho_thongtinho <- format_ID(ho_thongtinho, columns = c("MATINH",  "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> 
> n_hh_sample <- ho_thongtinho %>%
+   group_by (MATINH, MAXA) %>%
+   summarise (n_hh_sample = n()) #Number of general households by commune
`summarise()` has grouped output by 'MATINH'. You can override using the `.groups` argument.
> 
> fraction_sample <- full_join (n_rice_sample, n_hh_sample) %>%
+   mutate (fraction_sample = n_rice_sample / n_hh_sample)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> 
> 
> ### 3.2.2. Adjustment for under-representation----
> 
> adj_underrep <- full_join (fraction_pop, fraction_sample) %>%
+   mutate (adj_underrep = fraction_pop / fraction_sample)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> 
> ## 3.3. Calculating new weights----
> 
> ### 3.3.1. Calculating k (adjustment for both str change and under-rep)----
> new_wt2023 <- rice_gso %>%
+   left_join (adj_underrep %>% select (c(MATINH, MAXA, MADIABAN, adj_underrep))) %>%
+   mutate (wt45_underrep = case_when (is.na(adj_underrep) ~ wt45, 
+                                      adj_underrep == 0 ~ wt45,
+                                      !is.na(adj_underrep) & adj_underrep != 0 ~ wt45 * adj_underrep)) %>%
+   mutate (k = pred_n_rice_2023 / sum (wt45_underrep, na.rm = TRUE)) %>%
+   mutate (weight_final_rice = k * wt45_underrep) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
> 
> #Sniff test to check if total new weights sum up to the predicted number of rice-growing households in 2023 
> sum(new_wt2023$weight_final_rice, na.rm = TRUE) == pred_n_rice_2023  
[1] TRUE
> 
> 
> #If they don't pass the sniff test, it is because of the floating-point decision issue (i.e: rounding error), check these:
> sum(new_wt2023$weight_final_rice, na.rm = TRUE)
[1] 7596334
> 
> pred_n_rice_2023  
[1] 7596334
> 
> all.equal(sum(new_wt2023$weight_final_rice, na.rm = TRUE), pred_n_rice_2023)
[1] TRUE
> 
> any(is.na(new_wt2023$weight_final_rice))  #no NA
[1] FALSE
> 
> 
> 
> 
> 
> 
> # 4. Recalculating weights 2022----
> 
> 
> ## 4.1. Load original weights 2022----
> 
> curl_function(url = "data/raw/VHLSS_2022_Household/datasets/Weights/wt2022_SPIA.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Weights/wt2022_SPIA.dta'
Content type 'application/octet-stream' length 87757 bytes (85 KB)
downloaded 85 KB

> 
> wt_2022 <- read_dta ("data/raw/VHLSS_2022_Household/datasets/Weights/wt2022_SPIA.dta") %>%
+   select (c(tinh:diaban, ky, wt45)) %>%
+   select (-huyen)
> 
> colnames(wt_2022)[1:3] <- paste0 ("ma", colnames(wt_2022)[1:3]) 
> 
> colnames(wt_2022)[1:3] <- toupper (colnames(wt_2022)[1:3])
> 
> colnames(wt_2022)[4] <- "KYDIEUTRA"
> 
> # Reformat IDs:
> wt_2022 <- format_ID(wt_2022, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> 
> 
> ## 4.2. Adjustment for under-representation----
> 
> ### 4.2.1. IN THE SAMPLE----
> 
> #### 4.2.1.1. Rice-growing households IN THE SAMPLE ----
> 
> curl_function (url = "data/raw/VHLSS_2022_Household/datasets/Ho_Muc4B11.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Household/datasets/Ho_Muc4B11.dta'
Content type 'application/octet-stream' length 1473727 bytes (1.4 MB)
downloaded 1.4 MB

> 
> rice_gso_22 <- read_dta ("data/raw/VHLSS_2022_Household/datasets/Ho_Muc4B11.dta") %>%
+   select (c(MATINH, MAXA, MADIABAN, IDHO, KYDIEUTRA)) %>%
+   distinct() 
> 
> 
> rice_gso_22 <- rice_gso_22 %>%
+   left_join (wt_2022)
Joining with `by = join_by(MATINH, MAXA, MADIABAN, KYDIEUTRA)`
> 
> 
> length(which(is.na(rice_gso_22$wt45))) #374 obs missing weights!!!!
[1] 374
> #These households were additionally collected for provincial purposes. They don't belong to VHLSS
> 
> 
> rice_gso_22 <- rice_gso_22 %>%
+   filter (!is.na(wt45))  #N = 13939
> 
> n_rice_sample_22 <- rice_gso_22 %>%
+   group_by (MATINH, MAXA, MADIABAN) %>%
+   summarise (n_rice_sample = n()) #Number of rice-growing households by commune
`summarise()` has grouped output by 'MATINH', 'MAXA'. You can override using the `.groups` argument.
> 
> 
> 
> 
> #### 4.2.1.2. Number of general households IN THE SAMPLE ---- 
> 
> curl_function (url = "data/raw/VHLSS_2022_Correlates/Ho_ThongTinHo.dta")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2022_Correlates/Ho_ThongTinHo.dta'
Content type 'application/octet-stream' length 2079245 bytes (2.0 MB)
downloaded 2.0 MB

> 
> ho_thongtinho_22 <- read_dta ("data/raw/VHLSS_2022_Correlates/Ho_ThongTinHo.dta") %>%
+   select (c(IDHO:HOSO))
> 
> 
> #Reformat IDs:
> ho_thongtinho_22 <- format_ID(ho_thongtinho_22, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> 
> 
> n_hh_sample_22 <- ho_thongtinho_22 %>%
+   group_by (MATINH, MAXA, MADIABAN) %>%
+   summarise (n_hh_sample = n()) #Number of general households by commune
`summarise()` has grouped output by 'MATINH', 'MAXA'. You can override using the `.groups` argument.
> 
> fraction_sample_22 <- full_join (n_rice_sample_22, n_hh_sample_22) %>%
+   mutate (fraction_sample = n_rice_sample / n_hh_sample)
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
> 
> 
> 
> ### 4.2.2. Adjustment for under-representation----
> 
> adj_underrep_22 <- full_join (fraction_pop, fraction_sample_22) %>%
+   mutate (adj_underrep = fraction_pop / fraction_sample)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> 
> 
> ## 4.3. Calculating new weights----
> 
> ### 4.3.1. Calculating k (adjustment for both str change and under-rep)----
> 
> new_wt2022 <- rice_gso_22 %>%
+   left_join (adj_underrep_22 %>% select (c(MATINH, MAXA, MADIABAN, adj_underrep))) %>%
+   mutate (wt45_underrep = case_when (is.na(adj_underrep) ~ wt45,
+                                      adj_underrep == 0 ~ wt45,
+                                      !is.na(adj_underrep) & adj_underrep != 0 ~ wt45 * adj_underrep)) %>%
+   mutate (k = pred_n_rice_2022 / sum (wt45_underrep, na.rm = TRUE)) %>%
+   mutate (weight_final_rice = k * wt45_underrep) %>%
+   mutate (panel = 2022)
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
> 
> 
> #Again, the sniff test to check if the new weights sum up to the predicted number of rice-growing households in 2022
> sum(new_wt2022$weight_final_rice, na.rm = TRUE) == pred_n_rice_2022  
[1] TRUE
> 
> # If not pass the sniff test, it is because of the floating-point precision issue (i.e: rounding error). Check these:
> sum(new_wt2022$weight_final_rice, na.rm = TRUE) #7725011
[1] 7720924
> 
> pred_n_rice_2022 #7725011
[1] 7720924
> 
> all.equal(sum(new_wt2022$weight_final_rice, na.rm = TRUE), pred_n_rice_2022)  #now it's true
[1] TRUE
> 
> any(is.na(new_wt2022$weight_final_rice)) # No NA values
[1] FALSE
> 
> 
> # Join two datasets for rice weights
> final_weight_rice <- new_wt2022 %>%
+   # select (c(MATINH, MAHUYEN, MAXA, MADIABAN, IDHO, weight_2022_final)) %>%
+   full_join (new_wt2023 ) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_final_rice, panel)) %>%
+   distinct() 
Joining with `by = join_by(MATINH, MAXA, MADIABAN, IDHO, KYDIEUTRA, wt45, adj_underrep, wt45_underrep, k,
weight_final_rice, panel)`
> 
> 
> 
> 
> # 4.4. Rice DNA subsample----
> 
> 
> #Load rice DNA dataset:
> curl_function (url = "data/processed/Rice.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/Rice.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 119588 bytes (116 KB)
downloaded 116 KB

> 
> df_rice_dna <- read.csv ("data/processed/Rice.vars.VH24.csv") %>%
+   select (c(MATINH, MAHUYEN, MAXA, MADIABAN, HOSO)) 
> 
> 
> 
> #Reformat IDs to match:
> df_rice_dna <- format_ID(df_rice_dna, columns = c("MATINH", "MAXA", "MADIABAN", "HOSO"), widths = c(2,5,3,3))
> 
> 
> n_rice_dna <- df_rice_dna %>%
+   group_by (MATINH, MAXA) %>%
+   summarise (n_rice_dna = n())  #Number of DNA rice-hh in a commune
`summarise()` has grouped output by 'MATINH'. You can override using the `.groups` argument.
> 
> fraction_dna <- n_rice_dna %>%
+   left_join (n_hh_sample) %>%
+   mutate (fraction_dna = n_rice_dna / n_hh_sample)
Joining with `by = join_by(MATINH, MAXA)`
> 
> #Normally, in each communes, there are 15 hh surveyed -> replace missing fraction in the sample with 15
> fraction_dna$fraction_dna[is.na(fraction_dna$fraction_dna)] <- fraction_dna$n_rice_dna[is.na(fraction_dna$fraction_dna)] / 15
> 
> 
> inflation_dna <- fraction_dna %>%
+   left_join (fraction_pop) %>%
+   mutate (inflation_dna = fraction_pop / fraction_dna) 
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> dna_weight_rice <- df_rice_dna %>%
+   left_join (wt_2022) %>%
+   left_join (inflation_dna) %>%
+   mutate (wt_underrep = wt45 * inflation_dna) %>%
+   mutate (adj_underrep = pred_n_rice_2022 / sum (wt_underrep)) %>%
+   mutate (weight_rice_DNA = adj_underrep * wt_underrep) %>%
+   mutate (panel = 2022) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_rice_DNA, panel))
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
Joining with `by = join_by(MATINH, MAXA)`
> 
> # Sniff test to check new weights of DNA rice sum up to predicted N_rice 2022:
> sum(dna_weight_rice$weight_rice_DNA, na.rm = TRUE) == pred_n_rice_2022 #passed
[1] TRUE
> 
> 
> final_weight_rice <- final_weight_rice %>%
+   full_join (dna_weight_rice)
Joining with `by = join_by(MATINH, MAXA, MADIABAN, panel)`
> 
> 
> 
> rm(list = setdiff(ls(), c("curl_function", "token", "final_weight_rice", "n_hh_sample", "n_hh_sample_22", "n_hh_pop", "wt_2023", "pred_n_cass_2023", 'format_ID', "pred_n_coffee_2023")))
> 
> 
> 
> 
> 
> 
> # 5. Cassava DNA subsample----
> 
> ### 5.1.1. In the sample ----
> 
> curl_function (url = "data/processed/Cass.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/Cass.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 219732 bytes (214 KB)
downloaded 214 KB

> 
> cassava <- read.csv ("data/processed/Cass.vars.VH24.csv") %>%
+   select (c(MATINH, MAXA, MADIABAN, HOSO, IDHO)) 
> 
> cassava <- format_ID(cassava, columns = c("MATINH", "MAXA", "MADIABAN", "HOSO"), widths = c(2,5,3,3))
> 
> cassava <- cassava %>%
+   left_join (wt_2023)
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
> 
> n_cass_sample <- cassava %>%
+   group_by(MATINH, MAXA, MADIABAN) %>%
+   summarise (n_cass_sample = n()) #Number of cassava households in the sample
`summarise()` has grouped output by 'MATINH', 'MAXA'. You can override using the `.groups` argument.
> 
> fraction_sample_cass <- n_cass_sample %>%
+   left_join (n_hh_sample) %>%
+   mutate (fraction_sample_cass = n_cass_sample / n_hh_sample)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> ### 5.1.2. In the population ----
> curl_function(url = "data/raw/Weight/cassava_household_clean.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/cassava_household_clean.csv'
Content type 'text/plain; charset=utf-8' length 594230 bytes (580 KB)
downloaded 580 KB

> 
> n_cass_pop <- read.csv ("data/raw/Weight/cassava_household_clean.csv") %>%
+   select (c(MATINH, MAXA, n_cassava_hh))  #GSO list: N_cassava by commune, merge by Commune ID
> 
> 
> 
> n_cass_pop <- format_ID(n_cass_pop, columns = c("MATINH", "MAXA"), widths = c(2,5))
> 
> 
> fraction_pop_cass <- n_cass_pop %>%
+   full_join (n_hh_pop) %>%
+   mutate (fraction_pop_cass = n_cassava_hh / n_hh_pop)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> adj_underrep_cass <- fraction_sample_cass %>%
+   left_join (fraction_pop_cass) %>%
+   mutate (adj_underrep_cass = fraction_pop_cass / fraction_sample_cass)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> ### 5.1.3. New weight----
> 
> cass_wt <- cassava %>%
+   left_join (adj_underrep_cass %>% select (c(MATINH, MAXA, MADIABAN, adj_underrep_cass))) %>%
+   mutate (wt45_underrep = case_when (is.na(adj_underrep_cass) ~ wt45,
+                                      adj_underrep_cass == 0 ~ wt45,
+                                      !is.na(adj_underrep_cass) & adj_underrep_cass != 0 ~ wt45 * adj_underrep_cass)) %>%
+   mutate (k = pred_n_cass_2023 / sum (wt45_underrep, na.rm = TRUE)) %>%
+   mutate (weight_cass = k * wt45_underrep) %>%
+   mutate (panel = 2023) %>%
+   select (c(MATINH:IDHO, weight_cass, panel)) 
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
> 
> 
> #Sniff test to check if new weights add up to the predicted number of cassava-growing households:
> sum(cass_wt$weight_cass, na.rm = TRUE) == pred_n_cass_2023  # passed the sniff test
[1] TRUE
> 
> 
> # If not pass the sniff test, it is because of the floating-point precision issue (i.e: rounding error). Check these:
> sum(cass_wt$weight_cass, na.rm = TRUE)  #508884.7
[1] 497960.7
> pred_n_cass_2023 #508884.7
[1] 497960.7
> 
> all.equal(sum(cass_wt$weight_cass, na.rm = TRUE), pred_n_cass_2023)  #TRUE
[1] TRUE
> 
> which(is.na(cass_wt$weight_cass))  #no NA values
 [1]  11  32  48  79  98 105 107 124 125 126 128 129 134 135 136 138 141 162 172 388 403 508 515 524 532
[26] 585 621 626 633 644 655 663 726 731 761 763
> 
> 
> 
> cassava_final_weight <- cass_wt %>%
+   select (c(MATINH:MADIABAN, panel, weight_cass))
> 
> 
> # 6. GIFT-derived tilapia strains ----
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") %>%
+   select (-hhiddistrict)
Error in `select()`:
! Can't subset columns that don't exist.
✖ Column `hhiddistrict` doesn't exist.
Backtrace:
 1. read.csv("data/processed/GIFT.vars.VH24.csv") %>% ...
 3. dplyr:::select.data.frame(., -hhiddistrict)
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") %>%
+   select (-hhiddistrict)
Error in `select()`:
! Can't subset columns that don't exist.
✖ Column `hhiddistrict` doesn't exist.
Backtrace:
 1. read.csv("data/processed/GIFT.vars.VH24.csv") %>% ...
 3. dplyr:::select.data.frame(., -hhiddistrict)
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") 
> gift[which(gift$I_Q5== "388893714"),]$HH_ID <- 1 #post-survey edit
Error in `$<-.data.frame`(`*tmp*`, HH_ID, value = 1) : 
  replacement has 1 row, data has 0
> gift <- gift %>%
+   rename (c("MATINH" = "hhidprovince",
+             # "MAHUYEN" = "hhiddistrict",
+             "MAXA" = "hhidcommune",
+             "HOSO" = "HH_ID"))
Error in `rename()`:
! Can't rename columns that don't exist.
✖ Column `hhidprovince` doesn't exist.
Backtrace:
 1. gift %>% ...
 3. dplyr:::rename.data.frame(., c(MATINH = "hhidprovince", MAXA = "hhidcommune", HOSO = "HH_ID"))
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <integer>.
ℹ `y$MATINH` is a <character>.
Backtrace:
 1. left_join(gift, gift_ea_unique) %>% mutate(panel = 2023)
 4. dplyr:::left_join.data.frame(gift, gift_ea_unique)
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <integer>.
ℹ `y$MATINH` is a <character>.
Backtrace:
 1. left_join(gift, gift_ea_unique) %>% mutate(panel = 2023)
 4. dplyr:::left_join.data.frame(gift, gift_ea_unique)
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
Error in `left_join()`:
! Can't join `x$MAXA` with `y$MAXA` due to incompatible types.
ℹ `x$MAXA` is a <integer>.
ℹ `y$MAXA` is a <character>.
Backtrace:
 1. left_join(gift, gift_ea_unique) %>% mutate(panel = 2023)
 4. dplyr:::left_join.data.frame(gift, gift_ea_unique)
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") %>%
+   select (-hhiddistrict)
Error in `select()`:
! Can't subset columns that don't exist.
✖ Column `hhiddistrict` doesn't exist.
Backtrace:
 1. read.csv("data/processed/GIFT.vars.VH24.csv") %>% ...
 3. dplyr:::select.data.frame(., -hhiddistrict)
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") #%>%
> #  select (-hhiddistrict)
> gift <- format_ID(gift, columns = c("hhidprovince", "hhidcommune", "HH_ID"), widths = c(2, 5, 3))
Error in `[[<-.data.frame`(`*tmp*`, column, value = character(0)) : 
  replacement has 0 rows, data has 204
Called from: `[[<-.data.frame`(`*tmp*`, column, value = character(0))
Browse[1]> 
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
Error in `left_join()`:
! Can't join `x$MAXA` with `y$MAXA` due to incompatible types.
ℹ `x$MAXA` is a <integer>.
ℹ `y$MAXA` is a <character>.
Backtrace:
 1. left_join(gift, gift_ea_unique) %>% mutate(panel = 2023)
 4. dplyr:::left_join.data.frame(gift, gift_ea_unique)
> class(gift_ea$MATINH)
[1] "integer"
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> gift$MATINH
  [1] 10 14 82 17  8  4 96  8 17 15 66 25 96 96  8 96  2  8 15 15 45  8  8 45  8 40 25 66 25  8 15 96 30
 [34]  8  8  6 15  8  8 96 96 96 96 96 96 96 96 96 96 96  4 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96
 [67] 96 96 96 96 96 66 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 15 96 96
[100] 96  8 96 31 96 96 96 96 96 96 96 96 96 96 96 96 60 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96
[133] 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96
[166] 96 96 67 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 96 67 96 20 96  8 15 25  8  8 15  8 15  8
[199]  8 96 38 60 96 19
> gift$MAXA
  [1]  2734  3673 28708  5320  2530  1666 32245  2275  5320  4714 24253  8179 32069 32077  2383 32074
 [17]   811  2275  4780  4747 19519  2383  2464 19402  2542 17554  7921 24253  8443  2422  4537 32245
 [33] 10876  2446  2554  1933  4768  2422  2383 32143 32092 32077 32069 32215 32053 32035 32124 32146
 [49] 32128 32143  1657 32086 32053 32125 32125 32086 32143 32077 32053 32215 32053 32086 32215 32059
 [65] 32035 32146 32048 32215 32221 32215 32053 24253 32221 32125 32053 32215 32125 32125 32245 32069
 [81] 32125 32128 32125 32236 32077 32125 32069 32143 32077 32053 32092 32143 32128 32059 32143 32077
 [97]  4765 32074 32086 32125  2542 32125 11587 32174 32077 32146 32035 32092 32092 32236 32053 32146
[113] 32053 32125 32143 23023 32086 32074 32074 32086 32035 32048 32086 32050 32050 32035 32069 32221
[129] 32128 32059 32125 32124 32215 32143 32146 32215 32128 32125 32092 32124 32074 32146 32035 32146
[145] 32053 32125 32048 32245 32077 32048 32125 32092 32174 32125 32053 32128 32048 32146 32125 32146
[161] 32124 32215 32146 32086 32143 32092 32215 24709 32074 32146 32053 32125 32146 32125 32146 32125
[177] 32050 32128 32050 32077 32035 32035 32128 32221 32128 24685 32215  6472 32035  2464  4768  8179
[193]  2209  2398  4714  2464  4747  2500  2446 32053 15136 23239 32128  5647
> class(gift$MAXA)
[1] "integer"
> class(gift_ea$MAXA) 
[1] "character"
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> class(gift_ea$MAXA)
[1] "character"
> # 6. GIFT-derived tilapia strains ----
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") #%>%
> #  select (-hhiddistrict)
> 
> gift[which(gift$I_Q5== "388893714"),]$HH_ID <- 1 #post-survey edit
Error in `$<-.data.frame`(`*tmp*`, HH_ID, value = 1) : 
  replacement has 1 row, data has 0
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") #%>%
> #  select (-hhiddistrict)
> 
> #gift[which(gift$I_Q5== "388893714"),]$HH_ID <- 1 #post-survey edit
> #gift[which(gift$I_Q5== "838834349"),]$hhidcommune <- 32221 #post-survey edit
> #gift[which(gift$I_Q5 == "395079742"),]$hhidcommune <- 4768 #post-survey
> 
> gift <- format_ID(gift, columns = c("hhidprovince", "hhidcommune", "HH_ID"), widths = c(2, 5, 3))
Error in `[[<-.data.frame`(`*tmp*`, column, value = character(0)) : 
  replacement has 0 rows, data has 204
Called from: `[[<-.data.frame`(`*tmp*`, column, value = character(0))
Browse[1]> 
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") #%>%
> gift <- format_ID(gift, columns = c("hhidprovince", "hhidcommune", "HH_ID"), widths = c(2, 5, 3))
Error in `[[<-.data.frame`(`*tmp*`, column, value = character(0)) : 
  replacement has 0 rows, data has 204
Called from: `[[<-.data.frame`(`*tmp*`, column, value = character(0))
Browse[1]> Q
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MAXA <- as.integer(gift_ea$MATINH); gift_ea$MADIABAN <- as.integer(gift_ea$MADIABAN); gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
> gift_df$MADIABAN[is.na(gift_df$MADIABAN)] <- "005"
> # gift_df$MAHUYEN[gift_df$MAXA == "32221"] <- "972"
> 
> 
> gift_final <- gift_df %>%
+   left_join (wt_2023) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <integer>.
ℹ `y$MATINH` is a <character>.
Backtrace:
 1. ... %>% distinct()
 7. dplyr:::left_join.data.frame(., wt_2023)
> class(gift_ea$MATINH)
[1] "integer"
> class(gift$MATINH)
[1] "integer"
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") #%>%
> #  select (-hhiddistrict)
> 
> #gift[which(gift$I_Q5== "388893714"),]$HH_ID <- 1 #post-survey edit
> #gift[which(gift$I_Q5== "838834349"),]$hhidcommune <- 32221 #post-survey edit
> #gift[which(gift$I_Q5 == "395079742"),]$hhidcommune <- 4768 #post-survey
> 
> #gift <- format_ID(gift, columns = c("hhidprovince", "hhidcommune", "HH_ID"), widths = c(2, 5, 3))
> 
> #gift <- gift %>%
> #  rename (c("MATINH" = "hhidprovince",
>             # "MAHUYEN" = "hhiddistrict",
> #            "MAXA" = "hhidcommune",
> #            "HOSO" = "HH_ID"))
> 
> 
> 
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MAXA <- as.integer(gift_ea$MAXA); gift_ea$MADIABAN <- as.integer(gift_ea$MADIABAN); gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> class(gift_ea$MATINH)
[1] "integer"
> class(gift$MATINH)
[1] "integer"
> 
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
> gift_df$MADIABAN[is.na(gift_df$MADIABAN)] <- "005"
> # gift_df$MAHUYEN[gift_df$MAXA == "32221"] <- "972"
> 
> 
> gift_final <- gift_df %>%
+   left_join (wt_2023) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <integer>.
ℹ `y$MATINH` is a <character>.
Backtrace:
 1. ... %>% distinct()
 7. dplyr:::left_join.data.frame(., wt_2023)
> wt_2023$MAXA <- as.integer(wt_2023$MAXA); wt_2023$MADIABAN <- as.integer(wt_2023$MADIABAN); wt_2023$MATINH <- as.integer(wt_2023$MATINH)
> 
> 
> gift_final <- gift_df %>%
+   left_join (wt_2023) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN)`
Error in `left_join()`:
! Can't join `x$MADIABAN` with `y$MADIABAN` due to incompatible types.
ℹ `x$MADIABAN` is a <character>.
ℹ `y$MADIABAN` is a <integer>.
Backtrace:
 1. ... %>% distinct()
 7. dplyr:::left_join.data.frame(., wt_2023)
> class(gift_df$MADIABAN)
[1] "character"
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") #%>%
> #  select (-hhiddistrict)
> 
> #gift[which(gift$I_Q5== "388893714"),]$HH_ID <- 1 #post-survey edit
> #gift[which(gift$I_Q5== "838834349"),]$hhidcommune <- 32221 #post-survey edit
> #gift[which(gift$I_Q5 == "395079742"),]$hhidcommune <- 4768 #post-survey
> 
> #gift <- format_ID(gift, columns = c("hhidprovince", "hhidcommune", "HH_ID"), widths = c(2, 5, 3))
> 
> #gift <- gift %>%
> #  rename (c("MATINH" = "hhidprovince",
>             # "MAHUYEN" = "hhiddistrict",
> #            "MAXA" = "hhidcommune",
> #            "HOSO" = "HH_ID"))
> 
> 
> 
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MAXA <- as.integer(gift_ea$MAXA); gift_ea$MADIABAN <- as.integer(gift_ea$MADIABAN); gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
> 
> gift_df$MADIABAN[is.na(gift_df$MADIABAN)] <- "005"
> class(gift_df$MADIABAN)
[1] "character"
> gift_final <- gift_df %>%
+   left_join (wt_2023) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN)`
Error in `left_join()`:
! Can't join `x$MADIABAN` with `y$MADIABAN` due to incompatible types.
ℹ `x$MADIABAN` is a <character>.
ℹ `y$MADIABAN` is a <integer>.
Backtrace:
 1. ... %>% distinct()
 7. dplyr:::left_join.data.frame(., wt_2023)
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") #%>%
> #  select (-hhiddistrict)
> 
> #gift[which(gift$I_Q5== "388893714"),]$HH_ID <- 1 #post-survey edit
> #gift[which(gift$I_Q5== "838834349"),]$hhidcommune <- 32221 #post-survey edit
> #gift[which(gift$I_Q5 == "395079742"),]$hhidcommune <- 4768 #post-survey
> 
> #gift <- format_ID(gift, columns = c("hhidprovince", "hhidcommune", "HH_ID"), widths = c(2, 5, 3))
> 
> #gift <- gift %>%
> #  rename (c("MATINH" = "hhidprovince",
>             # "MAHUYEN" = "hhiddistrict",
> #            "MAXA" = "hhidcommune",
> #            "HOSO" = "HH_ID"))
> 
> 
> 
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MAXA <- as.integer(gift_ea$MAXA); gift_ea$MADIABAN <- as.integer(gift_ea$MADIABAN); gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
> 
> gift_df$MADIABAN[is.na(gift_df$MADIABAN)] <- "005"
> # gift_df$MAHUYEN[gift_df$MAXA == "32221"] <- "972"
> 
> gift_df$MAXA <- as.integer(gift_df$MAXA); gift_df$MADIABAN <- as.integer(gift_df$MADIABAN); gift_df$MATINH <- as.integer(gift_df$MATINH)
> 
> gift_final <- gift_df %>%
+   left_join (wt_2023) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN)`
> # 7. Coffee----
> ## 7.1.1. In the sample----
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv'
Content type 'text/plain; charset=utf-8' length 80331 bytes (78 KB)
downloaded 78 KB

> 
> coffee <- read.csv("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv") %>%
+   select (c(MATINH, MAXA, MADIABAN, HOSO, IDHO)) 
> 
> coffee <- format_ID (coffee, c("MATINH", "MAXA", "MADIABAN", "HOSO"), c(2,5,3,2))
>   
> 
> coffee <- coffee %>%
+   left_join (wt_2023) %>%
+   select (-MAHUYEN)
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <character>.
ℹ `y$MATINH` is a <integer>.
Backtrace:
 1. coffee %>% left_join(wt_2023) %>% select(-MAHUYEN)
 4. dplyr:::left_join.data.frame(., wt_2023)
> # 7. Coffee----
> ## 7.1.1. In the sample----
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv'
Content type 'text/plain; charset=utf-8' length 80331 bytes (78 KB)
downloaded 78 KB

> 
> coffee <- read.csv("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv") %>%
+   select (c(MATINH, MAXA, MADIABAN, HOSO, IDHO)) 
> 
> coffee <- format_ID (coffee, c("MATINH", "MAXA", "MADIABAN", "HOSO"), c(2,5,3,2))
> gift_df$MAXA <- as.integer(gift_df$MAXA); gift_df$MADIABAN <- as.integer(gift_df$MADIABAN); gift_df$MATINH <- as.integer(gift_df$MATINH)
> 
>   
> coffee <- coffee %>%
+   left_join (wt_2023) %>%
+   select (-MAHUYEN)
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <character>.
ℹ `y$MATINH` is a <integer>.
Backtrace:
 1. coffee %>% left_join(wt_2023) %>% select(-MAHUYEN)
 4. dplyr:::left_join.data.frame(., wt_2023)
> # 7. Coffee----
> ## 7.1.1. In the sample----
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv'
Content type 'text/plain; charset=utf-8' length 80331 bytes (78 KB)
downloaded 78 KB

> 
> coffee <- read.csv("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv") %>%
+   select (c(MATINH, MAXA, MADIABAN, HOSO, IDHO)) 
> 
> coffee <- format_ID (coffee, c("MATINH", "MAXA", "MADIABAN", "HOSO"), c(2,5,3,2))
> coffee$MAXA <- as.integer(coffee$MAXA); coffee$MADIABAN <- as.integer(coffee$MADIABAN); coffee$MATINH <- as.integer(coffee$MATINH)
> 
>   
> coffee <- coffee %>%
+   left_join (wt_2023) %>%
+   select (-MAHUYEN)
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
> 
> 
> any(is.na(coffee$wt45))  #No problems with additional surveyed EAs
[1] FALSE
> 
> n_coffee_sample <- coffee %>%
+   group_by(MATINH, MAXA, MADIABAN) %>%
+   summarise (n_coffee_sample = n()) #Number of coffee households in the sample
`summarise()` has grouped output by 'MATINH', 'MAXA'. You can override using the `.groups` argument.
> 
> fraction_sample_coffee <- n_coffee_sample %>%
+   left_join (n_hh_sample) %>%
+   mutate (fraction_sample_coffee = n_coffee_sample / n_hh_sample)
Joining with `by = join_by(MATINH, MAXA)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <integer>.
ℹ `y$MATINH` is a <character>.
Backtrace:
 1. n_coffee_sample %>% left_join(n_hh_sample) %>% ...
 4. dplyr:::left_join.data.frame(., n_hh_sample)
> # 7. Coffee----
> ## 7.1.1. In the sample----
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv'
Content type 'text/plain; charset=utf-8' length 80331 bytes (78 KB)
downloaded 78 KB

> 
> coffee <- read.csv("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv") %>%
+   select (c(MATINH, MAXA, MADIABAN, HOSO, IDHO)) 
> 
> coffee <- format_ID (coffee, c("MATINH", "MAXA", "MADIABAN", "HOSO"), c(2,5,3,2))
> coffee <- coffee %>%
+   left_join (wt_2023) %>%
+   select (-MAHUYEN)
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <character>.
ℹ `y$MATINH` is a <integer>.
Backtrace:
 1. coffee %>% left_join(wt_2023) %>% select(-MAHUYEN)
 4. dplyr:::left_join.data.frame(., wt_2023)
> wt_2023$MAXA <- as.character(wt_2023$MAXA); wt_2023$MADIABAN <- as.character(wt_2023$MADIABAN); wt_2023$MATINH <- as.character(wt_2023$MATINH)
> coffee <- coffee %>%
+   left_join (wt_2023) %>%
+   select (-MAHUYEN)
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
> any(is.na(coffee$wt45))  #No problems with additional surveyed EAs
[1] TRUE
> 
> n_coffee_sample <- coffee %>%
+   group_by(MATINH, MAXA, MADIABAN) %>%
+   summarise (n_coffee_sample = n()) #Number of coffee households in the sample
`summarise()` has grouped output by 'MATINH', 'MAXA'. You can override using the `.groups` argument.
> 
> fraction_sample_coffee <- n_coffee_sample %>%
+   left_join (n_hh_sample) %>%
+   mutate (fraction_sample_coffee = n_coffee_sample / n_hh_sample)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> ## 7.1.2. In the population----
> curl_function (url = "data/raw/Weight/coffee_household_clean.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/Weight/coffee_household_clean.csv'
Content type 'text/plain; charset=utf-8' length 589028 bytes (575 KB)
downloaded 575 KB

> 
> 
> n_coffee_pop <- read.csv ("data/raw/Weight/coffee_household_clean.csv") %>%
+   select (-c(ends_with("name"), MAHUYEN)) 
> 
> n_coffee_pop <- format_ID (n_coffee_pop, columns = c("MATINH", "MAXA"), widths = c(2,5))
> 
> fraction_pop_coffee <- n_coffee_pop %>%
+   left_join (n_hh_pop) %>%
+   mutate (fraction_pop_coffee = n_coffee_hh / n_hh_pop)
Joining with `by = join_by(MATINH, MAXA)`
> 
> 
> 
> adj_underrep_coffee <- fraction_sample_coffee %>%
+   left_join (fraction_pop_coffee) %>%
+   mutate (adj_underrep_coffee = fraction_pop_coffee / fraction_sample_coffee)
Joining with `by = join_by(MATINH, MAXA)`
> ### 7.1.3. New weight----
> 
> coffee_wt <- coffee %>%
+   left_join (adj_underrep_coffee %>% select (c(MATINH, MAXA, MADIABAN, adj_underrep_coffee))) %>%
+   mutate (wt45_underrep = case_when (is.na(adj_underrep_coffee) ~ wt45,
+                                      adj_underrep_coffee == 0 ~ wt45,
+                                      !is.na(adj_underrep_coffee) & adj_underrep_coffee != 0 ~ wt45 * adj_underrep_coffee)) %>%
+   mutate (k = pred_n_coffee_2023 / sum (wt45_underrep, na.rm = TRUE)) %>%
+   mutate (weight_coffee = k * wt45_underrep) %>%
+   mutate (panel = 2023) %>%
+     select (c(MATINH:IDHO, weight_coffee, panel)) 
Joining with `by = join_by(MATINH, MAXA, MADIABAN)`
> 
> 
> sum(coffee_wt$weight_coffee) == pred_n_coffee_2023  #Test if new weights add up to predicted n_coffee households in 2023
[1] NA
> 
> 
> coffee_final_weight <- coffee_wt %>%
+   select (c(MATINH, MADIABAN, MAXA, weight_coffee, panel)) %>%
+   distinct()
> 
> final_weight <- final_weight_rice %>%
+   full_join (cassava_final_weight) %>%
+   full_join (gift_final) %>%
+   full_join (coffee_final_weight) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAXA, MADIABAN, panel)`
Joining with `by = join_by(MATINH, MAXA, MADIABAN, panel)`
Error in `full_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <character>.
ℹ `y$MATINH` is a <integer>.
Backtrace:
 1. ... %>% distinct()
 5. dplyr:::full_join.data.frame(., gift_final)
> final_weight <- final_weight_rice %>%
+   full_join (cassava_final_weight) %>%
+   full_join (gift_df) %>%
+   full_join (coffee_final_weight) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAXA, MADIABAN, panel)`
Joining with `by = join_by(MATINH, MAXA, MADIABAN, panel)`
Error in `full_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <character>.
ℹ `y$MATINH` is a <integer>.
Backtrace:
 1. ... %>% distinct()
 5. dplyr:::full_join.data.frame(., gift_df)
> class(gift_df$MATINH)
[1] "integer"
> class(gift_df$coffee_final_weight)
[1] "NULL"
> class(coffee_final_weight$MATINH)
[1] "character"
> gift_df$MAXA <- as.character(gift_df$MAXA); gift_df$MADIABAN <- as.character(gift_df$MADIABAN); gift_df$MATINH <- as.character(gift_df$MATINH)
> 
> final_weight <- final_weight_rice %>%
+   full_join (cassava_final_weight) %>%
+   full_join (gift_df) %>%
+   full_join (coffee_final_weight) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAXA, MADIABAN, panel)`
Joining with `by = join_by(MATINH, MAXA, MADIABAN, panel)`
Joining with `by = join_by(MATINH, MAXA, MADIABAN, panel)`
> # Save file:
> output_dir <- "Output"
> if (!dir.exists(output_dir)) {
+   dir.create(output_dir)  # Create the directory if it doesn't exist
+ }
> 
> 
> write.csv (final_weight, "Output/Report_weights.csv",
+            row.names = FALSE)
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MAXA <- as.integer(gift_ea$MAXA); gift_ea$MADIABAN <- as.integer(gift_ea$MADIABAN); gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
> 
> gift_df$MADIABAN[is.na(gift_df$MADIABAN)] <- "005"
> # gift_df$MAHUYEN[gift_df$MAXA == "32221"] <- "972"
> 
> gift_df$MAXA <- as.integer(gift_df$MAXA); gift_df$MADIABAN <- as.integer(gift_df$MADIABAN); gift_df$MATINH <- as.integer(gift_df$MATINH)
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MAXA <- as.integer(gift_ea$MAXA); gift_ea$MADIABAN <- as.integer(gift_ea$MADIABAN); gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
> 
> gift_df$MADIABAN[is.na(gift_df$MADIABAN)] <- "005"
> # gift_df$MAHUYEN[gift_df$MAXA == "32221"] <- "972"
> names(gift_df)
 [1] "SubmissionDate" "start"          "end"            "today"          "deviceid"       "Enumerator_ID" 
 [7] "MATINH"         "MAHUYEN"        "MAXA"           "HOSO"           "hhstatus"       "I_Q3"          
[13] "I_Q4"           "T_Q2_1"         "total_samples"  "total_similar"  "Strain"         "StrainB"       
[19] "Strain_present" "date"           "KYDIEUTRA"      "MADIABAN"       "panel"         
> gift_final <- gift_df %>%
+   left_join (gift_df) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(SubmissionDate, start, end, today, deviceid, Enumerator_ID, MATINH, MAHUYEN,
MAXA, HOSO, hhstatus, I_Q3, I_Q4, T_Q2_1, total_samples, total_similar, Strain, StrainB, Strain_present,
date, KYDIEUTRA, MADIABAN, panel)`
Error in `rename()`:
! Can't rename columns that don't exist.
✖ Column `wt45` doesn't exist.
Backtrace:
 1. ... %>% distinct()
 6. dplyr:::rename.data.frame(., weight_gift = wt45)
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MAXA <- as.integer(gift_ea$MAXA); gift_ea$MADIABAN <- as.integer(gift_ea$MADIABAN); gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
> 
> gift_df$MADIABAN[is.na(gift_df$MADIABAN)] <- "005"
> # gift_df$MAHUYEN[gift_df$MAXA == "32221"] <- "972"
> 
> gift_df$MAXA <- as.integer(gift_df$MAXA); gift_df$MADIABAN <- as.integer(gift_df$MADIABAN); gift_df$MATINH <- as.integer(gift_df$MATINH)
> 
> gift_final <- gift_df %>%
+   left_join (wt_2023) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <integer>.
ℹ `y$MATINH` is a <character>.
Backtrace:
 1. ... %>% distinct()
 7. dplyr:::left_join.data.frame(., wt_2023)
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") #%>%
> #  select (-hhiddistrict)
> 
> #gift[which(gift$I_Q5== "388893714"),]$HH_ID <- 1 #post-survey edit
> #gift[which(gift$I_Q5== "838834349"),]$hhidcommune <- 32221 #post-survey edit
> #gift[which(gift$I_Q5 == "395079742"),]$hhidcommune <- 4768 #post-survey
> 
> #gift <- format_ID(gift, columns = c("hhidprovince", "hhidcommune", "HH_ID"), widths = c(2, 5, 3))
> 
> #gift <- gift %>%
> #  rename (c("MATINH" = "hhidprovince",
>             # "MAHUYEN" = "hhiddistrict",
> #            "MAXA" = "hhidcommune",
> #            "HOSO" = "HH_ID"))
> 
> 
> 
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MAXA <- as.integer(gift_ea$MAXA); gift_ea$MADIABAN <- as.integer(gift_ea$MADIABAN); gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
> 
> gift_df$MADIABAN[is.na(gift_df$MADIABAN)] <- "005"
> # gift_df$MAHUYEN[gift_df$MAXA == "32221"] <- "972"
> 
> #gift_df$MAXA <- as.integer(gift_df$MAXA); gift_df$MADIABAN <- as.integer(gift_df$MADIABAN); gift_df$MATINH <- as.integer(gift_df$MATINH)
> 
> gift_final <- gift_df %>%
+   left_join (wt_2023) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN)`
Error in `left_join()`:
! Can't join `x$MATINH` with `y$MATINH` due to incompatible types.
ℹ `x$MATINH` is a <integer>.
ℹ `y$MATINH` is a <character>.
Backtrace:
 1. ... %>% distinct()
 7. dplyr:::left_join.data.frame(., wt_2023)
> gift_df$MAXA <- as.character(gift_df$MAXA); gift_df$MADIABAN <- as.character(gift_df$MADIABAN); gift_df$MATINH <- as.character(gift_df$MATINH)
> 
> gift_final <- gift_df %>%
+   left_join (wt_2023) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN)`
> View(gift_df)
> curl_function (url = "data/processed/GIFT.vars.VH24.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/processed/GIFT.vars.VH24.csv'
Content type 'text/plain; charset=utf-8' length 39821 bytes (38 KB)
downloaded 38 KB

> 
> gift <- read.csv ("data/processed/GIFT.vars.VH24.csv") #%>%
> #  select (-hhiddistrict)
> 
> #gift[which(gift$I_Q5== "388893714"),]$HH_ID <- 1 #post-survey edit
> #gift[which(gift$I_Q5== "838834349"),]$hhidcommune <- 32221 #post-survey edit
> #gift[which(gift$I_Q5 == "395079742"),]$hhidcommune <- 4768 #post-survey
> 
> #gift <- format_ID(gift, columns = c("hhidprovince", "hhidcommune", "HH_ID"), widths = c(2, 5, 3))
> 
> #gift <- gift %>%
> #  rename (c("MATINH" = "hhidprovince",
>             # "MAHUYEN" = "hhiddistrict",
> #            "MAXA" = "hhidcommune",
> #            "HOSO" = "HH_ID"))
> 
> 
> 
> # GSO dataset to get EA ID:
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv'
Content type 'text/plain; charset=utf-8' length 39306 bytes (38 KB)
downloaded 38 KB

> 
> gift_ea <- read.csv ("data/raw/VHLSS_2023_Household/Combined_modules/Ho_Muc4B51A.csv")%>%
+   select ("MATINH":"MADIABAN") %>%
+   select (-MAHUYEN)
> 
> gift_ea <- format_ID(gift_ea, columns = c("MATINH", "MAXA", "MADIABAN"), widths = c(2, 5, 3))
> gift_ea$MAXA <- as.integer(gift_ea$MAXA); gift_ea$MADIABAN <- as.integer(gift_ea$MADIABAN); gift_ea$MATINH <- as.integer(gift_ea$MATINH)
> 
> gift_ea_unique <- distinct(gift_ea) #to get EA ID
> 
> gift_df <- left_join (gift, gift_ea_unique) %>%
+   mutate (panel = 2023)
Joining with `by = join_by(MATINH, MAXA)`
> 
> gift_df$MADIABAN[is.na(gift_df$MADIABAN)] <- "005"
> # gift_df$MAHUYEN[gift_df$MAXA == "32221"] <- "972"
> 
> #gift_df$MAXA <- as.integer(gift_df$MAXA); gift_df$MADIABAN <- as.integer(gift_df$MADIABAN); gift_df$MATINH <- as.integer(gift_df$MATINH)
> 
> gift_df$MAXA <- as.character(gift_df$MAXA); gift_df$MADIABAN <- as.character(gift_df$MADIABAN); gift_df$MATINH <- as.character(gift_df$MATINH)
> 
> gift_final <- gift_df %>%
+   left_join (wt_2023) %>%
+   rename (weight_gift = wt45) %>%
+   select (c(MATINH, MAXA, MADIABAN, weight_gift)) %>%
+   mutate (panel = 2023) %>%
+   distinct()
Joining with `by = join_by(MATINH, MAHUYEN, MAXA, MADIABAN)`
> # 7. Coffee----
> ## 7.1.1. In the sample----
> curl_function (url = "data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv")
trying URL 'https://raw.githubusercontent.com/CGIAR-SPIA/Viet-Nam-report-2024/main/data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv'
Content type 'text/plain; charset=utf-8' length 80331 bytes (78 KB)
downloaded 78 KB

> 
> coffee <- read.csv("data/raw/VHLSS_2023_Household/Combined_modules/M4B13A.csv") %>%
+   select (c(MATINH, MAXA, MADIABAN, HOSO, IDHO)) 
> 
> coffee <- format_ID (coffee, c("MATINH", "MAXA", "MADIABAN", "HOSO"), c(2,5,3,2))
> gift_df$MAXA <- as.character(gift_df$MAXA); gift_df$MADIABAN <- as.character(gift_df$MADIABAN); gift_df$MATINH <- as.character(gift_df$MATINH)
> 
>   
> coffee <- coffee %>%
+   left_join (gift_df) %>%
+   select (-MAHUYEN)
Joining with `by = join_by(MATINH, MAXA, MADIABAN, HOSO)`
Error in `left_join()`:
! Can't join `x$HOSO` with `y$HOSO` due to incompatible types.
ℹ `x$HOSO` is a <character>.
ℹ `y$HOSO` is a <integer>.
Backtrace:
 1. coffee %>% left_join(gift_df) %>% select(-MAHUYEN)
 4. dplyr:::left_join.data.frame(., gift_df)

Restarting R session...

> 
